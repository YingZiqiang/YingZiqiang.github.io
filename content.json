{"meta":{"title":"yingzq's Blog","subtitle":"练习bug时长两年半的实习生","description":"交流NLP, 深度学习技术, 分享coding路上的风景","author":"应子强","url":"http://www.yingzq.com","root":"/"},"pages":[{"title":"about","date":"2019-09-22T13:14:01.000Z","updated":"2019-09-22T14:15:33.242Z","comments":true,"path":"about/index.html","permalink":"http://www.yingzq.com/about/index.html","excerpt":"","text":"Welcome to contact me at yingzq0116@163.com"},{"title":"","date":"2019-09-18T14:56:17.871Z","updated":"2019-09-18T14:56:17.871Z","comments":false,"path":"categories/index.html","permalink":"http://www.yingzq.com/categories/index.html","excerpt":"","text":""},{"title":"","date":"2019-09-18T14:56:25.333Z","updated":"2019-09-18T14:56:25.333Z","comments":false,"path":"tags/index.html","permalink":"http://www.yingzq.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"常见排序算法（归纳分析及java实现）","slug":"common-sorting-algorithms","date":"2019-12-25T11:55:56.000Z","updated":"2020-01-01T05:30:31.820Z","comments":true,"path":"2019/12/25/common-sorting-algorithms/","link":"","permalink":"http://www.yingzq.com/2019/12/25/common-sorting-algorithms/","excerpt":"排序算法是算法的入门知识，其经典思想可以用于很多算法当中。本文将介绍常见排序算法的原理、对它们的分析以及对应的Java代码实现。","text":"排序算法是算法的入门知识，其经典思想可以用于很多算法当中。本文将介绍常见排序算法的原理、对它们的分析以及对应的Java代码实现。 注：排序就是将一组对象按照某种逻辑顺序重新排列的过程，在商业数据处理和现代科学计算中有着重要的地位。 排序算法概述算法分类常见的排序算法可以分成两大类： 比较类排序：通过比较来决定元素间的相对次序，由于其时间复杂度不能突破$O(n\\log n)$，因此也称为非线性时间比较类排序。 非比较类排序：不通过比较来决定元素间的相对次序，它可以突破基于比较排序的时间下界，以线性时间运行，因此也称为线性时间非比较类排序。 图：排序算法分类 算法复杂度 排序方法 时间复杂度（平均） 时间复杂度（最好） 时间复杂度（最坏） 空间复杂度 稳定性 冒泡排序 $O(n^2)$ $O(n)$ $O(n^2)$ $O(1)$ 稳定 选择排序 $O(n^2)$ $O(n^2)$ $O(n^2)$ $O(1)$ 不稳定 插入排序 $O(n^2)$ $O(n)$ $O(n^2)$ $O(1)$ 稳定 希尔排序 $O(n^{1.3})$ $O(n)$ $O(n^2)$ $O(1)$ 不稳定 归并排序 $O(n\\log n)$ $O(n\\log n)$ $O(n\\log n)$ $O(n)$ 稳定 快速排序 $O(n\\log n)$ $O(n\\log n)$ $O(n^2)$ $O(1)$ 不稳定 堆排序 $O(n\\log n)$ $O(n\\log n)$ $O(n\\log n)$ $O(1)$ 不稳定 - - - - - - 计数排序 $O(n+k)$ $O(n+k)$ $O(n+k)$ $O(n+k)$ 稳定 桶排序 $O(n+n(\\log n - \\log m))$ $O(n)$ $O(n\\log n)$ $O(n+m)$ 稳定 基数排序 $O(k*n)$ $O(k*n)$ $O(k*n)$ $O(k+n)$ 稳定 冒泡排序（Bubble Sort）算法描述冒泡排序是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 具体流程如下： 比较相邻的元素。如果第一个比第二个大，就交换它们两个； 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样一趟比较交换下来排在最右的元素就会是最大的数； 除去最右的元素，我们对剩余的元素做同样的工作，如此重复下去，直到排序完成。 动态演示 冒泡排序演示 代码实现public class Bubble &#123; public static void sort(int[] arr) &#123; int len = arr.length; for (int i = 0; i &lt; len; i++) &#123; for (int j = 0; j &lt; len - 1 - i; j++) &#123; if (arr[j] &gt; arr[j + 1]) swap(arr, j, j + 1); &#125; &#125; &#125; private static void swap(int[] arr, int i, int j) &#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125;&#125; 算法分析 时间复杂度：平均、最好、最坏的时间复杂度均为$O(n^2)$，也就是运行时间和输入无关。因为总要进行n次遍历，每次遍历进行数据比较、交换的时间为$O(n)$；但是也可以通过添加一个标志位将最好的时间复杂度提升为$O(n)$。 空间复杂度：$O(1)$ 是否稳定：稳定；因为当相邻元素相等的时候算法并不会交换这两元素的位置，因此冒泡排序是稳定的。 选择排序（Selection Sort）算法描述选择排序是一种简单直观的排序算法。 具体流程如下： 找到数组中最小的那个元素； 将它和数组的第一个元素交换位置（如果第一个元素就是最小元素那么它就和自己交换）； 在剩下的元素中找到最小的元素，将它与数组的第二个元素交换位置； 如此往复，直到将整个数组排序。 因为这种算法会不断的选择剩余元素之中的最小值，所以叫做选择排序。 动态展示 选择排序展示 代码实现public class Selection &#123; public static void sort(int[] arr) &#123; int len = arr.length; for (int i = 0; i &lt; len; i++) &#123; int min = i; // 最小元素的引索 for (int j = i + 1; j &lt; len; j++) &#123; if (arr[j] &lt; arr[min]) min = j; &#125; swap(arr, i, min); &#125; &#125; private static void swap(int[] arr, int i, int j) &#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125;&#125; 算法分析 时间复杂度：平均、最好、最坏的时间复杂度均为$O(n^2)$，也就是运行时间和输入无关。因为总要进行n次遍历，每次遍历需要花费$O(n)$的时间来找到最小元素的引索。 空间复杂度：$O(1)$ 是否稳定：不稳定；由于选择元素之后会发生交换操作，所以有可能把前面的元素交换到后面，例如数组 [3, 3, 2] ，第一次交换时改变了两个3的位置顺序，所以不是稳定的排序。 插入排序（Insertion Sort）算法描述通常人们整理桥牌的方法是一张一张的来，将每一张牌插入到其他已经有序的牌中的适当位置。在计算机的实现中，为了给要插入的元素腾出空间，我们需要将其余所有元素在插入之前都向右移动一位，这种算法叫做插入排序。 具体流程如下： 从数组第2个元素开始抽取元素； 把它与左边第一个元素比较，如果左边第一个元素比它大，则继续与左边第二个元素比较下去，直到遇到不比它大的元素，然后插到这个元素的右边； 继续选取第3，4，…n个元素,重复步骤 2 ，选择适当的位置插入。 动态演示 插入排序展示 代码实现public class Insertion &#123; public static void sort(int[] arr) &#123; int len = arr.length; for (int i = 1; i &lt; len; i++) &#123; // 将arr[i]插入到arr[i-1]、arr[i-2]、arr[i-3]...之中 for (int j = i; j &gt; 0 &amp;&amp; arr[j] &lt; arr[j - 1]; j--) &#123; swap(arr, j, j - 1); &#125; &#125; &#125; private static void swap(int[] arr, int i, int j) &#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125;&#125; 算法分析 时间复杂度：平均时间复杂度和最坏时间复杂度均为$O(n^2)$，但是最好时间复杂度为$O(n)$，也就是插入排序算法所需的时间取决于输入中元素的初始顺序。当排序一个随机顺序或者逆序的数组时，需要插入n-1次，每次插入平均的比较、交换次数为$O(n)$，所以平均和最坏的时间复杂度为$O(n^2)$；当排序一个已经排好序的数组时，每次插入仅需要$O(1)$次比较，因此最好时间复杂度为$O(n)$；对于接近有序的数组插入排序算法也是极快的。 空间复杂度：$O(1)$ 是否稳定：稳定；因为在比较元素大小的时候，如果两个元素相等则不会进行交换。 希尔排序（Shell Sort）算法描述希尔排序是插入排序的一种更高效的改进版本，也叫缩小增量排序。相较于插入排序一点一点的移动元素，希尔排序实现了快速移动一大步。 具体流程如下： 先设定一个合适的h，例如数组总长度的1/3； 利用插入排序，使得数组中任意间隔为h的元素都是有序的，这样的数组也被称为h有序数组； 按一定的规律减小h的值，重复第2步，直到h=1。 算法图示 图：希尔排序图示 注：较为复杂的算法动态图反而不好看清，所以使用图示来展示。 代码实现public class Shell &#123; public static void sort(int[] arr) &#123; int len = arr.length; int h = 1; while (h &lt; len / 3) h = 3 * h + 1; // 1, 4, 13, 40, 121, 364, ... while (h &gt;= 1) &#123; // 将数组变为h有序 for (int i = h; i &lt; len; i++) &#123; // 将arr[i]插入到arr[i-h], arr[i-2*h], arr[i-3*h]... 之中 for (int j = i; j &gt;= h &amp;&amp; arr[j] &lt; arr[j - h]; j -= h) &#123; swap(arr, j, j - h); &#125; &#125; h = h / 3; &#125; &#125; private static void swap(int[] arr, int i, int j) &#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125;&#125; 注：递增序列的选择也是一大难题，代码使用的是$h_{t+1} = 3 * h_t + 1$ 算法分析 时间复杂度：希尔排序的时间复杂度无法准确量化，平均复杂度介于$O(n\\log n)$和$O(n^2)$之间，并且和递增序列的选择也有着很大关系；有人用大量实验说明对于一个已知的非常好的递增序列，平均时间复杂度大约为$O(n^{1.3})$。 空间复杂度：$O(1)$ 是否稳定：不稳定；虽然插入排序是稳定的，但是希尔排序在插入的时候是跳跃性插入的，有可能破坏稳定性。 归并排序（Merge Sort）算法描述什么是归并呢？归并操作即将两个有序的数组归并成一个更大的有序数组。 很快人们就基于归并操作发明了一种简单的递归排序算法：归并排序。要将一个数组排序，可以先（递归地）将它分成两半分别排序，然后将结果归并起来。 归并排序是一种分治的排序方法。 算法图示 图：归并排序图示 代码实现归并排序有两种实现方式： 自顶向下的递归实现 自底向上的迭代实现 自顶向下（Top-down）public class Merge &#123; private static int[] aux; //归并所需的辅助数组 public static void sort(int[] arr) &#123; aux = new int[arr.length]; sort(arr, 0, arr.length - 1); &#125; private static void sort(int[] arr, int lo, int hi) &#123; // 将数组arr[lo..hi]排序 if (hi &lt;= lo) return; int mid = lo + (hi - lo) / 2; sort(arr, lo, mid); sort(arr, mid + 1, hi); merge(arr, lo, mid, hi); &#125; private static void merge(int[] arr, int lo, int mid, int hi) &#123; // 将arr[lo..mid]和arr[mid+1..hi]归并 int i = lo, j = mid + 1; // 将arr[lo..hi]复制到aux[lo..hi] for (int k = lo; k &lt;= hi; k++) aux[k] = arr[k]; for (int k = lo; k &lt;= hi; k++) &#123; if (i &gt; mid) arr[k] = aux[j++]; else if (j &gt; hi) arr[k] = aux[i++]; else if (aux[j] &lt; aux[i]) arr[k] = aux[j++]; else arr[k] = aux[i++]; // 包含两元素相等的情况，取较前位置的元素来保证算法稳定性 &#125; &#125;&#125; 自底向上（Bottom-up）public class MergeBU &#123; private static int[] aux; //归并所需的辅助数组 public static void sort(int[] arr) &#123; int len = arr.length; aux = new int[len]; for (int sz = 1; sz &lt; len; sz *= 2) // sz:子数组大小 for (int lo = 0; lo &lt; len - sz; lo += sz + sz) // lo:子数组引索 merge(arr, lo, lo + sz - 1, Math.min(lo + sz + sz - 1, len - 1)); &#125; private static void merge(int[] arr, int lo, int mid, int hi) &#123; // 将arr[lo..mid]和arr[mid+1..hi]归并 int i = lo, j = mid + 1; // 将arr[lo..hi]复制到aux[lo..hi] for (int k = lo; k &lt;= hi; k++) aux[k] = arr[k]; for (int k = lo; k &lt;= hi; k++) &#123; if (i &gt; mid) arr[k] = aux[j++]; else if (j &gt; hi) arr[k] = aux[i++]; else if (aux[j] &lt; aux[i]) arr[k] = aux[j++]; else arr[k] = aux[i++]; // 包含两元素相等的情况，取较前位置的元素来保证算法稳定性 &#125; &#125;&#125; 算法分析 时间复杂度：平均、最好、最坏的时间复杂度均为$O(n\\log n)$；因为归并排序递归的深度为$\\log n$，每一层进行比较并移动的次数均为$O(n)$。 空间复杂度：$O(n)$；递归深度为$O(\\log n)$，辅助数组的空间为$O(n)$，所以整体的空间复杂度为$O(n)$。 是否稳定：稳定；因为在合并的时候，如果两个元素相等则选择前面的元素到辅助数组，所以相等元素的相对顺序未发生改变。 快速排序（Quick Sort）算法描述快速排序可能是应用最广泛的排序算法了。 快速排序引人注目的特点包括它是原地排序（只需要一个很小的辅助栈），且将长度为n的数组排序所需的时间和$n\\log n$成正比，前面讲到的排序算法均无法将这两个优点同时结合起来。另外，快速排序的内循环比大多数排序算法都要短小，这意味着它无论是在理论上还是实际中都要更快。 快速排序的基本思想：通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。 和归并排序一样，快速排序也是一种分治的排序方法。 算法图示 注：快速排序的关键是递归进行切分（partition）操作，所以我们主要展示如何进行一次partition操作。 图：快速排序partition示意图 图：快速排序partition详细轨迹 代码实现import java.util.Random;public class Quick &#123; private static Random random = new Random(); public static void sort(int[] arr) &#123; shuffle(arr); // 打乱数组，保证快排算法性能 sort(arr, 0, arr.length - 1); &#125; private static void sort(int[] arr, int lo, int hi) &#123; if (hi &lt;= lo) return; int j = partition(arr, lo, hi); sort(arr, lo, j - 1); sort(arr, j + 1, hi); &#125; private static int partition(int[] arr, int lo, int hi) &#123; // 将数组切分为arr[lo..i-1], arr[i], arr[i+1..hi] int i = lo, j = hi + 1; // 左右扫描指针 int v = arr[lo]; // 切分元素，即pivot while (true) &#123; // 扫描左右，检查扫描是否结束并交换元素 while (arr[++i] &lt; v) if (i == hi) break; while (arr[--j] &gt; v) if (j == lo) break; if (i &gt;= j) break; swap(arr, i, j); &#125; swap(arr, lo, j); // 将切分元素v放入正确的位置 return j; // arr[lo..j-1] &lt;= arr[j] &lt;= arr[j+1..hi]达成 &#125; private static void swap(int[] arr, int i, int j) &#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125; private static void shuffle(int[] arr) &#123; int n = arr.length; for (int i = 0; i &lt; n; i++) &#123; int r = i + random.nextInt(n - i); // i &lt;= r &lt;= n-1 swap(arr, i, r); &#125; &#125;&#125; 注：当输入的数组有序或者基本有序时，快排的时间复杂度为$O(n^2)$。这里有两种解决方法，一种是在最开始随机打乱数组，另一种是进行partition操作时随机选择切分元素。 算法分析 时间复杂度：平均、最好的时间复杂度为$O(n\\log n)$，最坏的时间复杂度为$O(n^2)$。 空间复杂度：$O(1)$ 是否稳定：不稳定；因为随机打乱数组或随机选择切分元素均会使得相等元素的相对顺序发生改变。 堆排序（Heap Sort）算法描述堆排序是指利用堆这种数据结构所设计的一种排序算法。 堆是具有以下性质的完全二叉树：每个结点的值都大于或等于其左右孩子结点的值，称为大顶堆；或者每个结点的值都小于或等于其左右孩子结点的值，称为小顶堆。 具体流程如下： 大顶堆的构造，将原始数组重新组织安排进一个堆中； 将堆首（最大值）和堆尾互换； 将堆的尺寸减1，利用下沉操作（代码中的sink()函数）将新的堆首元素调整到相应位置； 重复步骤二和步骤三，直到堆的尺寸为 1。 注：关于堆的具体内容可以看 二叉堆是什么鬼？ 和 算法4 优先队列等相关资料 ，这里就不再详细讲解了。 算法图示上面的步骤其实主要分成两步： 第一步是堆的构建，这里不使用优先队列中的不断插入新元素来构建堆（时间复杂度$O(n\\log n$)），而是从右到左用下沉操作（sink()函数）构建子堆，可以证明这里只需要$O(n)$的时间复杂度即可构建堆； 第二步是下沉排序，从堆中按递减顺序取出所有元素并得到排序结果。 图：堆的构造（左）和下沉排序（右） 代码实现public class Heap &#123; public static void sort(int[] a) &#123; int N = a.length; // 1. 建立大顶堆 for (int k = N / 2; k &gt;= 1; k--) &#123; sink(a, k, N); &#125; // 2. 下沉排序 while (N &gt; 1) &#123; swap(a, 1, N--); sink(a, 1, N); &#125; &#125; private static void sink(int[] a, int k, int n) &#123; while (2 * k &lt;= n) &#123; int j = 2 * k; if (j &lt; n &amp;&amp; less(a, j, j + 1)) j++; if (!less(a, k, j)) break; swap(a, k, j); k = j; &#125; &#125; private static boolean less(int[] a, int i, int j) &#123; return a[i - 1] &lt; a[j - 1]; &#125; private static void swap(int[] a, int i, int j) &#123; int temp = a[i - 1]; a[i - 1] = a[j - 1]; a[j - 1] = temp; &#125;&#125; 注：注意代码中的less()函数和swap()函数引索都减去了 1，这是由于堆顶元素从 引索 1 开始更方便。 算法分析 时间复杂度：平均、最好、最坏的时间复杂度均为$O(n\\log n)$。 空间复杂度：$O(1)$，堆排序是原地排序。 是否稳定：不稳定。 计数排序（Counting Sort）算法描述计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围、最大值和最小值的差值不是很大的整数。 动态演示 计数排序展示 代码实现public class Counting &#123; public static void sort(int[] arr) &#123; // 1. 得到数列的最大值和最小值 int max = max(arr); int min = min(arr); // 2. 计算出现频率 int[] count = new int[max - min + 1]; for (int num : arr) &#123; count[num - min] += 1; &#125; // 3. 遍历频率数组，生成结果 int idx = 0; for (int i = 0; i &lt; count.length; i++) &#123; while (count[i]-- &gt; 0) &#123; arr[idx++] = i + min; &#125; &#125; &#125; private static int max(int[] arr) &#123; int max = Integer.MIN_VALUE; for (int num : arr) max = Math.max(max, num); return max; &#125; private static int min(int[] arr) &#123; int min = Integer.MAX_VALUE; for (int num : arr) min = Math.min(min, num); return min; &#125;&#125; 如果只是单纯的给整数排序，上面的代码就已经可以解决问题了。但是如果我们希望对数组进行的是稳定排序，也就是相同元素的相对顺序不改变呢？可以使用如下代码： public class Counting &#123; public static void sort(int[] arr) &#123; // 1. 得到数列的最大值和最小值 int max = max(arr); int min = min(arr); int R = max - min + 1; // 2. 计算出现频率 int[] count = new int[R + 1]; for (int num : arr) &#123; count[num - min + 1]++; &#125; // 3. 将频率转为引索 for (int i = 0; i &lt; R; i++) &#123; count[i+1] += count[i]; &#125; // 4. 将元素分类 int N = arr.length; int[] aux = new int[N]; for(int i = 0;i &lt; N;i++)&#123; aux[count[arr[i] - min]++] = arr[i]; &#125; // 5. 回写 for (int i = 0; i &lt; N; i++) &#123; arr[i] = aux[i]; &#125; &#125; private static int max(int[] arr) &#123; int max = Integer.MIN_VALUE; for (int num : arr) max = Math.max(max, num); return max; &#125; private static int min(int[] arr) &#123; int min = Integer.MAX_VALUE; for (int num : arr) min = Math.min(min, num); return min; &#125;&#125; 算法分析 时间复杂度：当输入的元素是 n 个 m 到 m+k 之间的整数时，很容易算出平均、最好、最坏的时间复杂度均为$O(n+k)$。所以当 k 的值远小于 n 或者和 n 差不多大时，算法可以达到$O(n)$级别的线性时间复杂度。 空间复杂度：$O(n+k)$；因为需要一个 k 大小的用来计数的数组，还需要一个 n 大小的辅助数组。 是否稳定：稳定。 桶排序（Bucket Sort）算法描述桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。 桶排序的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排）。 具体流程如下： 根据待排序集合中最大元素和最小元素的差值范围和映射规则，确定申请的桶个数； 遍历待排序集合，将每一个元素移动到对应的桶中； 将每个不为空的桶进行排序； 拼接不为空的桶中的数据，得到结果。 算法图示 图：桶排序展示 代码实现import java.util.ArrayList;import java.util.Collections;import java.util.List;public class Bucket &#123; public static void sort(double[] arr) &#123; // 1. 得到数列的最大值和最小值 double max = arr[0], min = arr[0]; for (int i = 1; i &lt; arr.length; i++) &#123; max = Math.max(max, arr[i]); min = Math.min(min, arr[i]); &#125; // 2. 初始化桶 int bucketNum = arr.length; // 桶个数，这里取数组长度 List&lt;List&lt;Double&gt;&gt; bucketList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; bucketNum; i++) &#123; bucketList.add(new ArrayList&lt;&gt;()); &#125; // 3. 遍历原始数组，将每个元素放入桶中 double gap = max - min; for (double num : arr) &#123; int idx = (int) ((num - min) * (bucketNum - 1) / gap); // 定义的映射规则 bucketList.get(idx).add(num); &#125; // 4. 对每个桶内部进行排序 for (List&lt;Double&gt; bucket : bucketList) &#123; Collections.sort(bucket); // JDK底层采用了归并排序的优化版本 &#125; // 5. 拼接不为空的桶中的数据，得到结果 int idx = 0; for (List&lt;Double&gt; bucket : bucketList) &#123; for (double num : bucket) &#123; arr[idx++] = num; &#125; &#125; &#125;&#125; 注1：这里使用double类型是因为刚好计数排序是无法完成double类型数组的排序的。另外，桶个数以及映射规则是根据实际情况调整的，代码中只是起了“举例”的作用。注2：代码第二步中的初始化桶其实用数组而不是ArrayList更好，但是java不支持泛型数组，因此使用ArratList写起来更加简洁清晰。 算法分析 时间复杂度：这里用 m 来表示桶的个数，平均时间复杂度为$O(n) + O(m\\frac{n}{m} \\log \\frac{n}{m})=O(n+n(\\log n - \\log m))$，当每个桶刚好有1个元素时有最好的时间复杂度$O(n)$，当一个桶有 n 个元素而其他桶都为空时有最坏时间复杂度$O(n\\log n)$。 空间复杂度：$O(n+m)$ 是否稳定：稳定；要注意的是桶内使用的排序算法需要是稳定排序。 基数排序（Radix Sort）算法描述基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。 基数排序的方式可以采用LSD（Least significant digital）或MSD（Most significant digital），LSD的排序方式由键值的最右边开始，而MSD则相反，由键值的最左边开始。 算法图示 图：基数排序LSD展示 代码实现public class LSD &#123; public static void sort(int[] arr) &#123; int N = arr.length; int R = 10; // 一位整数的范围在0~9之间，所以设置基数R=10 int[] aux = new int[N]; // 辅助数组 // 求最大值 int max = arr[0]; for (int i = 1; i &lt; N; i++) &#123; max = Math.max(max, arr[i]); &#125; // 计算最大值位数 int maxLen = 1; while (max / 10 &gt; 0) &#123; maxLen++; max = max / 10; &#125; for (int k = 1; k &lt;= maxLen; k++) &#123; // 对每一位使用计数排序 int[] count = new int[R + 1]; // 计算出现频率 for (int num : arr) &#123; int idx = (num / (int) Math.pow(10, k - 1)) % 10; count[idx + 1]++; &#125; for (int i = 0; i &lt; R; i++) &#123; // 将频率转换为引索 count[i + 1] += count[i]; &#125; for (int num : arr) &#123; // 将元素分类 int idx = (num / (int) Math.pow(10, k - 1)) % 10; aux[count[idx]++] = num; &#125; for (int i = 0; i &lt; N; i++) &#123; // 回写 arr[i] = aux[i]; &#125; &#125; &#125;&#125; 算法分析 时间复杂度：用 k 来表示所需排序元素的平均位数，则基数排序平均、最好、最坏的时间复杂度均为$O(k*n)$；因为一共进行了 k 次循环，每次循环是一次计数排序（O(n)时间复杂度）。 空间复杂度：$O(n+k)$ 是否稳定：稳定。 参考资料 Algorithms 4th edition, sorting 必学十大经典排序算法，看这篇就够了 十大经典排序算法（动图演示）","categories":[],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://www.yingzq.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"Java","slug":"Java","permalink":"http://www.yingzq.com/tags/Java/"}]},{"title":"Transformer代码实现","slug":"the-annotated-transformer","date":"2019-11-18T07:22:27.000Z","updated":"2019-12-24T11:06:59.005Z","comments":true,"path":"2019/11/18/the-annotated-transformer/","link":"","permalink":"http://www.yingzq.com/2019/11/18/the-annotated-transformer/","excerpt":"Transformer是如今几乎所有的预训练模型的基本结构。也许我们平时更多的是关注如何更好的利用已经训练好的GPT、BERT等模型进行fine-tune，但是同样重要的是，我们需要了解这些强力的模型具体是如何构建的。所以本文我们主要研究如何在PyTorch框架下用代码实现 “Attention is All You Need” 论文中原始Transformer的结构。","text":"Transformer是如今几乎所有的预训练模型的基本结构。也许我们平时更多的是关注如何更好的利用已经训练好的GPT、BERT等模型进行fine-tune，但是同样重要的是，我们需要了解这些强力的模型具体是如何构建的。所以本文我们主要研究如何在PyTorch框架下用代码实现 “Attention is All You Need” 论文中原始Transformer的结构。 本文内容参考了 The Annotated Transformer ，对应的代码则针对PyTorch 1.3环境做了一定更新，完整代码可见 Transformer Code 。 准备工作本文的测试环境是Python 3.6+和PyTorch 1.3，如果版本不对可能代码需要略微的调整。 在这里先导入所有需要用到的库，方便检测是否有缺失： import numpy as npimport torchimport torch.nn as nnimport torch.nn.functional as Fimport math, copy, time 背景介绍当谈及序列模型(sequence modeling)，我们首先想到的就是RNN及其变种，但是RNN模型的缺点也非常明显：需要顺序计算，从而很难并行。因此出现了Extended Neural GPU、ByteNet和ConvS2S等网络模型。这些模型都是以CNN为基础，这比较容易并行，但是和RNN相比，它较难学习到长距离的依赖关系。 本文的Transformer使用了Self-Attention机制，它在编码每一词的时候都能够注意(attend to)整个句子，从而可以解决长距离依赖的问题，同时计算Self-Attention可以用矩阵乘法一次计算所有的时刻，因此可以充分利用计算资源。 模型结构目前的主流神经序列转换(neural sequence transduction)模型都是基于Encoder-Decoder结构的。所谓的序列转换模型就是把一个输入序列转换成另外一个输出序列，它们的长度很可能是不同的。比如基于神经网络的机器翻译，输入是法语句子，输出是英语句子，这就是一个序列转换模型。类似的包括文本摘要、对话等问题都可以看成序列转换问题。我们这里主要关注机器翻译，但是任何输入是一个序列输出是另外一个序列的问题都可以考虑使用Encoder-Decoder结构。 Encoder将输入序列$(x_1,\\ldots,x_n)$编码成一个连续的序列$\\boldsymbol{z}=(z_1,\\ldots,z_n)$。而Decoder根据$\\boldsymbol{z}$来解码得到输出序列$(y_1,\\ldots,y_m)$。Decoder是自回归的(auto-regressive)，它会把前一个时刻的输出作为当前时刻的输入。Encoder-Decoder结构对应的代码如下： class EncoderDecoder(nn.Module): \"\"\" A standard Encoder-Decoder architecture. Base for this and many other models. \"\"\" def __init__(self, encoder, decoder, src_embed, tgt_embed, generator): super(EncoderDecoder, self).__init__() self.encoder = encoder self.decoder = decoder self.src_embed = src_embed self.tgt_embed = tgt_embed self.generator = generator def forward(self, src, tgt, src_mask, tgt_mask): \"\"\" Take in and process masked src and target sequences. \"\"\" return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask) def encode(self, src, src_mask): return self.encoder(self.src_embed(src), src_mask) def decode(self, memory, src_mask, tgt, tgt_mask): return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask) class Generator(nn.Module): \"\"\" Define standard linear + softmax generation step. \"\"\" def __init__(self, d_model, vocab): super(Generator, self).__init__() self.proj = nn.Linear(d_model, vocab) def forward(self, x): return F.log_softmax(self.proj(x), dim=-1) EncoderDecoder定义了一种通用的Encoder-Decoder架构，具体的encoder、decoder、src_embed、target_embed和generator都是构造函数传入的参数。这样我们做实验更换不同的组件就会更加方便。 解释一下各种参数的意义：encoder、encoder分别代表编码器和解码器；src_embed、tgt_embed分别代表将源语言、目标语言的ID序列编码为词向量(embedding)的方法；generator则是根据解码器当前时刻的隐状态输出当前时刻的词，上面已给出具体的实现方法(即Generator类)。 Transformer模型也遵循着Encoder-Decoder的架构。它的Encoder是由$N=6$个相同的EncoderLayer组成，每个EncoderLayer包含一个Self-Attention Sublayer层和一个Feed-Forward Sublayer层；而它的Decoder也是由$N=6$个相同的DecoderLayer组成，每个DecoderLayer包含一个Self-Attention Sublayer层、一个Encoder-Decoder-Attention Sublayer层和一个Feed-Forward Sublayer层。 注：Feed-Forward层其实就是全连接层的意思。 下图清晰的展示了Transformer整体架构： 图：Transformer整体架构 Encoder and Decoder StacksEncoder前面提到Encoder是由$N=6$个相同结构的EncoderLayer堆叠而成，所以我们定义Encoder的代码如下： def clones(module, N): \"\"\" Produce N identical layers. \"\"\" return nn.ModuleList([copy.deepcopy(module) for _ in range(N)]) class Encoder(nn.Module): \"\"\" Core encoder is a stack of N layers. \"\"\" def __init__(self, layer, N): super(Encoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, mask): \"\"\" Pass the input (and mask) through each layer in turn. \"\"\" for layer in self.layers: x = layer(x, mask) return self.norm(x) 也就是Encoder会把传入的layer深拷贝N次，然后让传入的Tensor依次通过这N个layer，最后再通过一层 Layer Normalization。 class LayerNorm(nn.Module): \"\"\" Construct a layernorm module, see https://arxiv.org/abs/1607.06450 for details. \"\"\" def __init__(self, features, eps=1e-6): super(LayerNorm, self).__init__() self.a_2 = nn.Parameter(torch.ones(features)) self.b_2 = nn.Parameter(torch.zeros(features)) self.eps = eps def forward(self, x): mean = x.mean(-1, keepdim=True) std = x.std(-1, keepdim=True) return self.a_2 * (x - mean) / (std + self.eps) + self.b_2 按照原论文，每一个EncoderLayer的每一个子层(sub-layer)的输出应该是$LayerNorm(x+Sublayer(x))$，其中的Sublayer(x)是对子层结构实现的抽象函数。这里稍微做了一些修改，首先在每一个子层的输出之后加了一个 Dropout层 ，另外一个不同就是把LayerNorm层放到前面了。也就是现在每一个子层实际的输出是： $$x+Dropout(Sublayer(LayerNorm(x)))$$ 注：原论文的LayerNorm放在最后，这里把它放在前面并且在Encoder的最后一层再加上了一个LayerNorm。这里的实现和论文的实现基本是一致的，只是给最底层的输入x多做了一个LayerNorm。 为了加快残差连接的速度，模型中所有的子层(sub-layer)，包括Embedding层，将它们的输出维度均设置为$d_{model}=512$。所以我们有如下代码： class SublayerConnection(nn.Module): \"\"\" A residual connection followed by a layer norm. Note for code simplicity the norm is first as opposed to last. \"\"\" def __init__(self, size, dropout): super(SublayerConnection, self).__init__() self.norm = LayerNorm(size) self.dropout = nn.Dropout(dropout) def forward(self, x, sublayer): \"\"\" Apply residual connection to any sublayer with the same size. \"\"\" return x + self.dropout(sublayer(self.norm(x))) 上面提到EncoderLayer是由Self-Attention、Feed-Forward这两个子层构成，所以有： class EncoderLayer(nn.Module): \"\"\" Encoder is made up of self-attn and feed forward. \"\"\" def __init__(self, size, self_attn, feed_forward, dropout): super(EncoderLayer, self).__init__() self.self_attn = self_attn self.feed_forward = feed_forward self.sublayer = clones(SublayerConnection(size, dropout), 2) self.size = size def forward(self, x, mask): x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) return self.sublayer[1](x, self.feed_forward) DecoderDecoder也是由$N=6$个相同结构的DecoderLayer堆叠而成。 class Decoder(nn.Module): \"\"\" Generic N layer decoder with masking. \"\"\" def __init__(self, layer, N): super(Decoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, memory, src_mask, tgt_mask): for layer in self.layers: x = layer(x, memory, src_mask, tgt_mask) return self.norm(x) 前面讲到，一个DecoderLayer除了有和EncoderLayer一样的两个子层，还多了一个Encoder-Decoder-Attention子层，这个子层会让模型在解码时会考虑最后一层Encoder所有时刻的输出。 class DecoderLayer(nn.Module): \"\"\" Decoder is made of self-attn, src-attn, and feed forward. \"\"\" def __init__(self, size, self_attn, src_attn, feed_forward, dropout): super(DecoderLayer, self).__init__() self.size = size self.self_attn = self_attn self.src_attn = src_attn self.feed_forward = feed_forward self.sublayer = clones(SublayerConnection(size, dropout), 3) def forward(self, x, memory, src_mask, tgt_mask): m = memory x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) return self.sublayer[2](x, self.feed_forward) 注：多出来的这一层Attention子层(代码中是src_attn)实现和Self-Attention是一样的，只不过src_attn的Query来自于前层Decoder的输出，但是Key和Value来自于Encoder最后一层的输出(代码中是memory)；而Self-Attention的Q、K、V则均来自前层的输出。 Decoder和Encoder还有一个关键的不同：Decoder在解码第t个时刻的时候只能使用小于t时刻的输入，而不能使用t+1时刻及其之后的输入。因此我们需要一个函数来产生一个Mask矩阵： def subsequent_mask(size): \"\"\" Mask out subsequent positions. \"\"\" attn_shape = (1, size, size) subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8') return torch.from_numpy(subsequent_mask) == 0 上面代码的意思是先用triu函数产生一个上三角矩阵，再利用matrix == 0得到所需要的下三角矩阵。 AttentionMulti-Head AttentionAttention(包括Self-Attention和普通的Attention)可以看成一个函数，它的输入是Query,Key和Value，输出是一个Tensor。其中输出是Value的加权平均，而权重则来自Query和Key的计算。 论文中首先提到了Scaled Dot-Product Attention，如下图所示： 图：Scaled Dot-Product Attention Scaled Dot-Product Attention需要保证Query和Key的维度是相同的，记为$d_k$，Value的维度记为$d_v$。 具体计算是先将一组query和所有的keys作点乘运算，然后除以$\\sqrt{d_k}$保证后续梯度的稳定性，然后将这些分数进行softmax归一化，作为query和Keys的相似程度，也就是values加权平均的权重，最后将所有values作加权平均作为输出。这里用矩阵直接表示： $$Attention(Q,K,V)=softmax(\\frac{Q K^T}{\\sqrt{d_k}})V$$ def attention(query, key, value, mask=None, dropout=None): \"\"\" Compute 'Scaled Dot Product Attention' \"\"\" d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) p_attn = F.softmax(scores, dim=-1) if dropout is not None: p_attn = dropout(p_attn) return torch.matmul(p_attn, value), p_attn 论文中非常重要的Multi-Head Attention便是基于Scaled Dot-Product Attention。其实很简单，前面定义的一组Q、K和V可以让一个词attend to相关的词，我们可以定义多组Q、K和V，它们分别可以关注不同的上下文： 图：Multi-Head Attention 由上图我们可以得到如下计算公式： $$MultiHead(Q,K,V)=Concat(head_1,\\ldots,head_h)W^O \\\\where \\ head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)$$ 论文中使用了$h=8$个Head，所以此时$d_k=d_v=d_{model}/h=64$。虽然此时Head数扩大了$h=8$由于每一个Head的维度缩小了$h=8$倍，所以总体的计算成本是基本不变的。 根据上述分析，我们可以写出Multi-Head Attention的代码了。 class MultiHeadedAttention(nn.Module): \"\"\" Implements 'Multi-Head Attention' proposed in the paper. \"\"\" def __init__(self, h, d_model, dropout=0.1): \"\"\" Take in model size and number of heads. \"\"\" super(MultiHeadedAttention, self).__init__() assert d_model % h == 0 # We assume d_v always equals d_k self.d_k = d_model // h self.h = h self.linears = clones(nn.Linear(d_model, d_model), 4) self.attn = None self.dropout = nn.Dropout(p=dropout) def forward(self, query, key, value, mask=None): if mask is not None: # Same mask applied to all h heads. mask = mask.unsqueeze(1) nbatches = query.size(0) # 1) Do all the linear projections in batch from d_model =&gt; h x d_k query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))] # 2) Apply attention on all the projected vectors in batch. x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout) # 3) \"Concat\" using a view and apply a final linear. x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k) return self.linears[-1](x) Attention在模型中的应用在Transformer里，有3个地方用到了Multi-Head Attention： 1) Decoder的Encoder-Decoder-Attention层。其中query来自于前一层Decoder的输出，而key和value则来自于是Encoder最后一层的输出，这个Attention层使得Decoder在解码时会考虑最后一层Encoder所有时刻的输出，是一种在Encoder-Decoder架构中常用的注意力机制。2) Encoder的Self-Attention层。query，key和value均来自于相同的地方，也就是前层Encoder的输出。3) Decoder的Self-Attention层。query，key和value均来自于相同的地方，也就是前层Decoder的输出，但是Mask使得它不能访问未来时刻的输出。 Feed-Forward除了Attention子层，Encoder和Decoder的每一层还包括一个Feed-Forward子层，也就是全连接层。每个时刻的全连接层是可以独立并行计算的(当然参数是共享的)。全连接层由两个线性变换以及它们之间的ReLU激活组成： $$FFN(x)=max(0,xW_1+b_1)W_2+b_2$$ 全连接层的输入和输出都是$d_{model}=512$维的，中间隐单元的个数是$d_{ff}=2048$。代码实现非常简单： class PositionwiseFeedForward(nn.Module): \"\"\" Implements FFN equation. \"\"\" def __init__(self, d_model, d_ff, dropout=0.1): super(PositionwiseFeedForward, self).__init__() self.w_1 = nn.Linear(d_model, d_ff) self.w_2 = nn.Linear(d_ff, d_model) self.dropout = nn.Dropout(dropout) def forward(self, x): return self.w_2(self.dropout(F.relu(self.w_1(x)))) Embeddings和大部分NLP任务一样，输入的词序列都是ID序列，所以需要有个Embeddings层。 class Embeddings(nn.Module): def __init__(self, d_model, vocab): super(Embeddings, self).__init__() # lut =&gt; lookup table self.lut = nn.Embedding(vocab, d_model) self.d_model = d_model def forward(self, x): return self.lut(x) * math.sqrt(self.d_model) 需要注意的是，在Embeddings层，所有的权重都扩大了$\\sqrt{d_{model}}$倍。 Positional Encoding其实Transformer是没有考虑词的顺序(位置)关系的。为了解决这个问题引入位置编码(Positional Encoding)，论文中使用的公式如下： $$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}}) \\\\PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$$ 其中pos代表位置而i代表维度。例如输入的ID序列长度为10，那么经过Embeddings层后Tensor的尺寸就是(10,512)，此时上式中的pos的范围就是0~9；对于不同维度，这里范围是0~511，偶数维使用sin函数，而奇数维使用cos函数。 这种位置编码的好处是：$PE_{pos+k}$可以表示成$PE_{pos}$的线性函数，这样网络就能容易的学到相对位置的关系。 我们来简单验证下，这里$10000^{2i/d_{model}}$是一个常数，我们记为$W_{i}$。 $$PE_{(pos+k,2i)}=sin(\\frac{pos+k}{W_{i}})=sin(\\frac{pos}{W_{i}})cos(\\frac{k}{W_{i}})+cos(\\frac{pos}{W_{i}})sin(\\frac{k}{W_{i}}) \\\\=PE_{(pos,2i)}cos(\\frac{k}{W_{i}})+PE_{(pos,2i+1)}sin(\\frac{k}{W_{i}})$$ $$PE_{(pos+k,2i+1)}=cos(\\frac{pos+k}{W_{i}})=cos(\\frac{pos}{W_{i}})cos(\\frac{k}{W_{i}})-sin(\\frac{pos}{W_{i}})sin(\\frac{k}{W_{i}}) \\\\=PE_{(pos,2i+1)}cos(\\frac{k}{W_{i}})-PE_{(pos,2i)}sin(\\frac{k}{W_{i}})$$ 可以看到$PE_{pos+k}$的确是$PE_{pos}$的线性函数。 位置编码的代码如下： class PositionalEncoding(nn.Module): \"\"\" Implement the PE function. \"\"\" def __init__(self, d_model, dropout, max_len=5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) # Compute the positional encodings once in log space. pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) self.register_buffer('pe', pe) def forward(self, x): x = x + self.pe[:, :x.size(1)] return self.dropout(x) 完整模型这里我们定义一个函数，输入是超参，输出是根据超参构建的模型： def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1): \"\"\" Helper: Construct a model from hyperparameters. \"\"\" c = copy.deepcopy attn = MultiHeadedAttention(h, d_model) ff = PositionwiseFeedForward(d_model, d_ff, dropout) position = PositionalEncoding(d_model, dropout) model = EncoderDecoder( Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N), Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N), nn.Sequential(Embeddings(d_model, src_vocab), c(position)), nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)), Generator(d_model, tgt_vocab)) # This was important from their code. # Initialize parameters with Glorot / fan_avg. for p in model.parameters(): if p.dim() &gt; 1: nn.init.xavier_uniform_(p) return model 模型训练首先我们需要一个Batch类，用于提供批次数据，并且构造所需要的掩码： class Batch(object): \"\"\" Object for holding a batch of data with mask during training. \"\"\" def __init__(self, src, trg=None, pad=0): self.src = src self.src_mask = (src != pad).unsqueeze(-2) if trg is not None: self.trg = trg[:, :-1] self.trg_y = trg[:, 1:] self.trg_mask = self.make_std_mask(self.trg, pad) self.ntokens = (self.trg_y != pad).sum().item() @staticmethod def make_std_mask(tgt, pad): \"\"\" Create a mask to hide padding and future words. \"\"\" tgt_mask = (tgt != pad).unsqueeze(-2) tgt_mask = tgt_mask &amp; subsequent_mask(tgt.size(-1)) return tgt_mask 值得注意的是解码阶段的Mask(代码中是trg_mask)需要将未来时刻的输出掩盖掉，这在前面已经实现了相应的函数(即subsequent_mask函数)。 接下来再写出运行一个epoch的训练代码，非常的简单： def run_epoch(data_iter, model, loss_compute): \"\"\" Standard Training and Logging Function \"\"\" start = time.time() total_tokens = 0 total_loss = 0 tokens = 0 for i, batch in enumerate(data_iter): out = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask) loss = loss_compute(out, batch.trg_y, batch.ntokens) total_loss += loss total_tokens += batch.ntokens tokens += batch.ntokens if i % 50 == 1: elapsed = time.time() - start print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" % (i, loss / batch.ntokens, tokens / elapsed)) start = time.time() tokens = 0 return total_loss / total_tokens 对于优化器(optimizer)，论文选用了常见的 Adam optimizer ，相应的优化器参数是$\\beta_1=0.9,\\beta_2=0.98,\\epsilon=10^{-9}$。特别的，对于比较重要的学习率参数，是随着训练的进行动态变化的，具体公式如下： $$lrate=d_{model}^{-0.5} \\cdot min(step\\_num^{−0.5},step\\_num \\cdot warmup\\_steps^{−1.5})$$ 也就是在最开始的$warmup\\_steps$步，学习率线性增加；然后再慢慢的非线性降低。论文中$warmup\\_steps=4000$。 class NoamOpt(object): \"\"\" Optim wrapper that implements rate. \"\"\" def __init__(self, model_size, factor, warmup, optimizer): self.optimizer = optimizer self._step = 0 self.warmup = warmup self.factor = factor self.model_size = model_size self._rate = 0 def step(self): \"\"\" Update parameters and rate. \"\"\" self._step += 1 rate = self.rate() for p in self.optimizer.param_groups: p['lr'] = rate self._rate = rate self.optimizer.step() def rate(self, step=None): if step is None: step = self._step return self.factor * (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))def get_std_opt(model): return NoamOpt(model.src_embed[0].d_model, 2, 4000, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)) 论文中使用到了3种Regularization，一种是Dropout，一种是残差连接，这两种前面已经做出讲解。最后一种是Label Smoothing，虽然Label Smoothing增加了模型训练的困惑度，但是的确使得最终的指标上升了，具体实现如下： class LabelSmoothing(nn.Module): \"\"\" Implement label smoothing. \"\"\" def __init__(self, size, padding_idx, smoothing=0.0): super(LabelSmoothing, self).__init__() self.criterion = nn.KLDivLoss(reduction='sum') self.padding_idx = padding_idx self.confidence = 1.0 - smoothing self.smoothing = smoothing self.size = size self.true_dist = None def forward(self, x, target): assert x.size(1) == self.size true_dist = x.clone() true_dist.fill_(self.smoothing / (self.size - 2)) true_dist.scatter_(1, target.unsqueeze(1), self.confidence) true_dist[:, self.padding_idx] = 0 mask = torch.nonzero(target == self.padding_idx) if mask.size(0) &gt; 0: true_dist.index_fill_(0, mask.squeeze(), 0.0) self.true_dist = true_dist return self.criterion(x, true_dist) A First Example论文中要完成的是一个机器翻译任务，但是那可能有点麻烦，所以我们就来完成一个简单的复制任务来检验我们的模型，也就是给定来自一个小型词汇表的token序列，我们的目标是通过Encoder-Decoder结构生成相同的token序列，例如输入是[1,2,3,4,5]，那么生成的序列也应该是[1,2,3,4,5]。 任务数据生成代码如下，让src=trg即可。 def data_gen(V, batch, nbatches): \"\"\" Generate random data for a src-tgt copy task. \"\"\" for i in range(nbatches): data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10))) data[:, 0] = 1 yield Batch(src=data, trg=data, pad=0) 然后是一个计算loss的方法： class SimpleLossCompute(object): \"\"\" A simple loss compute and train function. \"\"\" def __init__(self, generator, criterion, opt=None): self.generator = generator self.criterion = criterion self.opt = opt def __call__(self, x, y, norm): x = self.generator(x) loss = self.criterion(x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)) / norm loss.backward() if self.opt is not None: self.opt.step() self.opt.optimizer.zero_grad() return loss.item() * norm 在预测阶段是一个自回归模型，为了简单我们直接使用Greedy Search(一般情况下是使用Beam Search)，也就是每一个时刻都取概率最大的词作为输出。 def greedy_decode(model, src, src_mask, max_len, start_symbol): memory = model.encode(src, src_mask) ys = torch.ones(1, 1, dtype=torch.long).fill_(start_symbol) for i in range(max_len - 1): out = model.decode(memory, src_mask, ys, subsequent_mask(ys.size(1))) prob = model.generator(out[:, -1]) _, next_word = torch.max(prob, dim=1) next_word = next_word.item() ys = torch.cat([ys, torch.ones(1, 1, dtype=torch.long).fill_(next_word)], dim=1) return ys 最后，将这个例子运行起来，我们便可以看到，几分钟的时间内Transformer已经能够完美的完成这个复制任务！ # Train the simple copy task.V = 11criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)model = make_model(V, V, N=2)model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))for epoch in range(15): model.train() run_epoch(data_gen(V, 30, 20), model, SimpleLossCompute(model.generator, criterion, model_opt)) model.eval() print(run_epoch(data_gen(V, 30, 5), model, SimpleLossCompute(model.generator, criterion, None)))# This code predicts a translation using greedy decoding for simplicity.print()print(\"&#123;&#125;predict&#123;&#125;\".format('*' * 10, '*' * 10))model.eval()src = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])src_mask = torch.ones(1, 1, 10)print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1)) 结语本文相对于 原博客 有所扩充也有所删减。扩充主要在对代码的讲解部分；删减则主要是并没有完全复现机器翻译的任务，但是其实机器翻译的任务与上节中的复制任务是非常类似的，所以也没有继续在这上面花费精力，若对机器翻译非常感兴趣的童鞋倒是可以阅读 原博客 了解更多。 总体来说我们完完全全复现了 “Attention is All You Need” 中的结构，论文的思想真的的非常棒的！代码也在PyTroch 1.3的环境中测试通过，许多组件具有很好复用性，完整代码可见 Transformer Code ，可以方便以后对铺天盖地的基于Transformer的预训练模型的学习。","categories":[],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://www.yingzq.com/tags/NLP/"},{"name":"Transformer","slug":"Transformer","permalink":"http://www.yingzq.com/tags/Transformer/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://www.yingzq.com/tags/Deep-Learning/"},{"name":"PyTorch","slug":"PyTorch","permalink":"http://www.yingzq.com/tags/PyTorch/"},{"name":"BERT","slug":"BERT","permalink":"http://www.yingzq.com/tags/BERT/"}]},{"title":"Transformer图解","slug":"the-illustrated-transformer","date":"2019-10-29T16:23:45.000Z","updated":"2020-02-01T12:51:11.221Z","comments":true,"path":"2019/10/30/the-illustrated-transformer/","link":"","permalink":"http://www.yingzq.com/2019/10/30/the-illustrated-transformer/","excerpt":"Transformer模型来自论文 Attention Is All You Need 。这个模型最初是为了提高机器翻译的效率，它的Self-Attention机制和Position Encoding可以替代RNN。因为RNN是顺序执行的，t时刻没有完成就不能处理t+1时刻，因此很难并行。但是后来发现Self-Attention效果很好，在很多其它的地方也可以使用Transformer模型。这包括著名的GPT和BERT模型，都是以Transformer为基础的。 本文我们将通过图解的方式来直观的理解Transformer模型的基本原理，内容主要参考了文章 The Illustrated Transformer 。","text":"Transformer模型来自论文 Attention Is All You Need 。这个模型最初是为了提高机器翻译的效率，它的Self-Attention机制和Position Encoding可以替代RNN。因为RNN是顺序执行的，t时刻没有完成就不能处理t+1时刻，因此很难并行。但是后来发现Self-Attention效果很好，在很多其它的地方也可以使用Transformer模型。这包括著名的GPT和BERT模型，都是以Transformer为基础的。 本文我们将通过图解的方式来直观的理解Transformer模型的基本原理，内容主要参考了文章 The Illustrated Transformer 。 模型概述我们首先把模型看成一个黑盒子，如下图所示，对于机器翻译来说，它的输入是源语言(法语)的句子，输出是目标语言(英语)的句子。 图：Transformer的输入和输出 把黑盒子稍微打开一点，我们可以看到Transformer(或者任何的NMT系统)由2个部分组成：Encoders和Decoders，如下图所示： 图：典型的Encoder-Decoder结构 注：这里Encoder/Decoder使用复数形式是为了强调编码/解码阶段可能由多个组件堆叠而成，下文会讲到 其中Encoders是由多个(论文中是6个)Encoder堆叠而成，Decoders也同样是由多个(论文中是6个)Decoder堆叠而成，如下图所示： 图：Stacked Encoder and Decoder 对于Encoders中的每一个Encoder，它们结构都是相同的，但是并不会共享权值。每个Encoder由两个子网络层组成，分别是一个Self-Attention层和一个Feed-Forward层(全连接层)，如下图所示： 图：Transformer的一个Encoder层 对于Decoders中的每一个Decoder，它们的结构也都是相同的并且权值不共享。相比于Encoder，每一个Decoder除了Self-Attention层和全连接层之外还多了一个普通的Attention层，这个Attention层使得Decoder在解码时会考虑最后一层Encoder所有时刻的输出。 它的结构如下图所示： 图：Transformer的一个Decoder层 画出Tensor进行分析现在我们已经了解了模型的主要组件，让我们开始研究对应的Tensor如何在这些组件之间流动。 由于输入的句子是一个词的序列，利用NLP的常规做法，我们先通过Embedding把它变成一个连续稠密的向量，如下图所示： 图：Emebdding层 注：每一个词都编码成了512维度的向量，为了便于展示我们将用上面这些简单的小方块来表示这些向量 虽然Emebdding层只发生在最底层的Encoder中，但是对于所有的Encoder层，你都可以按照同一种思维模式来理解，那就是它们都接收一个512维度的向量列表作为输入，只不过最底层的Encoder接收的是单词嵌入，其它Encoder则接收的是前一层Encoder的输出。 Embedding之后的序列会输入到最底层的Encoder，首先经过Self-Attention层然后再经过全连接层，如下图所示： 图：Transformer Encoder层 开始编码接下来，我们将以一个较短的句子为例，看看在编码器中每个子层中具体都发生了什么，首先最底层的Encoder结构如下图所示： 图：Transformer Encoder层 可以看出这张图与上一张图内容基本相同，但是其实本张图展示了更多的细节：图中Self-Attention层是一个大的方框，表示它的输入是所有的$x_1,x_2,\\ldots,x_n$，输出是$z_1,z_2,\\ldots,z_n$，也就是说在计算$z_i$时需要依赖所有时刻的输入$x_1,x_2,\\ldots,x_n$，不过我们可以用矩阵运算一下子把所有的$z_i$计算出来(后面介绍)；而全连接层是每个时刻是一个方框(但不同时刻的参数是共享的)，表示计算$r_i$只需要$z_i$，因此全连接网络的计算是完全是独立的，很容易并行计算；此外，前一层的输出$r_1,r_2,\\ldots,r_n$直接输入到下一层。 Self-Attention概述再把黑盒子打开一点，我们来了解一下Tensor如何在Self-Attention层中流动。 Self-Attention是Trandformer中最重要的组件，我们先来提炼一下它是如何工作的。假如我们想要翻译下面的句子： &quot;The animal didn&#39;t cross the street because it was too tired&quot; 句子的意思大概是“这个动物无法穿越马路，因为它太累了”，这里的it到底指代什么呢，是animal还是street？要知道具体的指代，我们需要在理解it的时候同时关注所有的单词，重点是animal、street和tired，然后根据知识(常识)我们知道只有animal才能tired，而street是不能tired的。Self-Attention用Encoder在编码一个词的时候会考虑句子中所有其它的词，从而确定怎么编码当前词。 但是如果把tired换成narrow，那么it就指代的是street了。 而LSTM(即使是双向的)是无法实现上面的逻辑的。 为什么呢？比如前向的LSTM，我们在编码it的时候根本没有看到后面是tired还是narrow，所有它无法把it编码成哪个词。而后向的LSTM呢？当然它看到了tired，但是到it的时候它还没有看到animal和street这两个单词，当然就更无法编码it的内容了。 当然多层的LSTM理论上是可以编码这个语义的，它需要下层的LSTM同时编码了animal和street以及tired三个词的语义，然后由更高层的LSTM来把it编码成animal的语义。但是这样模型更加复杂。 下图是模型的最上一层(下标0是第一层，5是第六层)Encoder的Attention可视化图，这是 Tensor2Tensor notebook 输出的内容： 图：Self-Attention的可视化 我们可以看到，在编码it这个单词的时候，Self-Attention让模型更多地关注到“The animal”，因此编码后的it有了Animal的语义。 Self-Attention详细介绍下面我们详细的介绍Self-Attention是怎么计算的，首先介绍向量的形式逐个时刻计算，这便于理解，接下来我们把它写出矩阵的形式一次计算所有时刻的结果。 对于输入的每一个向量(第一层是词的Embedding，其它层是前一层的输出)，我们首先需要生成3个新的向量Q、K和V，分别代表Query向量、Key向量和Value向量。Query向量表示为了编码当前词，需要去注意(attend to)其它(其实也包括它自己)的词；而Key向量可以认为是这个词用于被检索的关键信息；Value向量则是真正的内容。 我们对比一下普通的Attention(Luong 2015)，使用内积计算energy的情况。如下图所示，在这里，Key和Value向量都是它本身，而Query向量是当前隐状态$h_t$。计算energy $e_{tj}$的时候我们计算Q($h_t$)和K($\\overline{h}_j$)的内积，然后用softmax进行归一化，最后把所有的$\\overline{h}_j$加权平均得到context向量。 图：普通的Attention机制 而Self-Attention里的Query不是隐状态，并且来自当前输入向量本身，因此叫作Self-Attention。 另外Key和Value都不是输入向量，而是输入向量做了一下线性变换，这样做的好处是模型可以根据数据从输入向量中提取最适合作为Key和Value的部分。类似的，Query也是对输入向量做一下线性变换，它让系统可以根据任务学习出最适合的Query，从而可以注意到特定的内容。 K、V和Q的具体的计算过程如下如所示： 图：K、V和Q的计算过程 图中输入的是两个词“thinking”和“machines”，我们对它们进行Embedding(这是第一层，如果是后面的层，直接输入就是向量了)，得到向量$x_1$,$x_2$。接着我们用3个矩阵分别对它们进行变换，得到向量$q_1,k_1,v_1$和$q_2,k_2,v_2$。比如$q_1=x_1W^Q$，图中$x_1$的shape是1x4，$W^Q$是4x3，得到的$q_1$便是1x3。其它的计算也是类似的。 值得注意的是，为了能够使得Key和Query可以内积，我们要求$W^K$和$W^Q$的shape是一样的，但是并不要求$W^V$和它们一定一样(虽然实际论文实现是一样的)。 每个时刻t都计算出$q_t,k_t,v_t$后，我们就可以来计算Self-Attention了。以第一个时刻为例，我们首先计算$q_1$和$k_1,k_2$的内积，得到score，过程如下图所示： 图：Self-Attention的向量计算步骤一 接下来使用softmax把score归一化，注意这里把score除以$\\sqrt{d_k}$($d_k$表示Key向量的维度)之后再计算的softmax，根据论文的说法，这样计算梯度时会更加稳定(stable)。计算过程如下图所示： 图：Self-Attention的向量计算步骤二 简单阐述下为什么这里要除以$\\sqrt{d_k}$。假设q、k向量中的每一个元素是独立的随机变量并且它们的均值是0方差是1，此时将q、k向量做点乘运算$q \\cdot k=\\sum_{i=1}^{d_k} q_i k_i$，点乘的结果是一个均值为0，方差为$d_k$的随机变量。 接下来用softmax得到的归一化分数对所有时刻的V求加权平均，这样就可以认为得到的向量在Self-Attention的帮助下综合考虑了所有时刻的输入信息，计算过程如下图所示： 图：Self-Attention的向量计算步骤三 这里只是演示了计算第一个时刻的过程，计算其它时刻的过程是完全一样的。 Self-Attention的矩阵计算前面介绍的方法需要一个循环遍历所有的时刻t计算得到$z_t$，我们可以把上面的向量计算变成矩阵的形式，从而一次计算出所有时刻的输出，这样的矩阵运算可以充分利用硬件资源(包括一些软件的优化)，从而效率更高。 首先还是计算Q、K和V，不过不是计算某个时刻的$q_t,k_t,v_t$了，而是一次计算所有时刻的Q、K和V。计算过程如下图所示。这里的输入是一个矩阵，矩阵的第i行表示第i个时刻的输入$x_i$。 图：Self-Attention的矩阵计算步骤一 接下来就是计算Q和K得到score，然后除以$\\sqrt{d_k}$，然后再softmax，最后加权平均得到输出。全过程如下图所示： 图：Self-Attention的矩阵计算步骤二 Multi-Head Attention这篇论文还提出了Multi-Head Attention的概念。其实很简单，前面定义的一组Q、K和V可以让一个词attend to相关的词，我们可以定义多组Q、K和V，它们分别可以关注不同的上下文。计算Q、K和V的过程还是一样，这不过现在变换矩阵从一组$(W_Q,W_K,W_V)$变成了多组$(W_0^Q,W_0^K,W_0^V)$ ，$(W_1^Q,W_1^K,W_1^V)$，…。如下图所示： 图：Multi-Head计算多组Q、K和V 论文中使用了8组不同的Q、K和V。对于输入矩阵X，每一组Q、K和V都可以得到一个输出矩阵Z，如下图所示： 图：Multi-Head计算输出多个Z 但是后面的全连接网络需要的输入是一个矩阵而不是多个矩阵，因此我们可以把每个head输出的$Z_i$拼接起来，然后再经过一个线性变换(矩阵$W^O$)得到最终的输出矩阵Z。这个过程如下图所示： 图：Multi-Head生成最终的矩阵Z 上面的步骤涉及很多步骤和矩阵运算，我们用一张大图把整个过程表示出来，如下图所示： 图：Multi-Head计算完整过程 让我们回顾一下之前的例子，当我们在例句“The animal didn’t cross the street because it was too tired”中编码单词“it”时，不同的head把焦点放到了哪里： 图：Multi-Head Attention的焦点 可以看到，有的head(橘黄色部分)计算的结果认为其与“the animal”关系比较密切，而另一个head(绿色部分)则认为和“tired”关系更近。换句话说，使用Multi-Head Attention对“it”进行编码时，可以同时注意到“animal”和“tired”，这使得对“it”的编码更加全面而准确。 位置编码(Positional Encoding) 注意：这是Transformer原始论文使用的位置编码方法，而在BERT模型里，使用的是简单的可以学习的Embedding，和Word Embedding一样，只不过输入是位置而不是词而已。 我们的目的是用Self-Attention替代RNN，RNN能够记住过去的信息，这可以通过Self-Attention“实时”的注意相关的任何词来实现等价(甚至更好)的效果。RNN还有一个特点就是能考虑词的顺序(位置)关系，一个句子即使词完全是相同的但是语义可能完全不同，比如“北京到上海的机票”与“上海到北京的机票”，它们的语义就有很大的差别。我们上面的介绍的Self-Attention是不考虑词的顺序的，如果模型参数固定了，上面两个句子的北京都会被编码成相同的向量。但是实际上我们可以期望这两个北京编码的结果不同，前者可能需要编码出发城市的语义，而后者需要包含目的城市的语义。而RNN是可以(至少是可能)学到这一点的。当然RNN为了实现这一点的代价就是顺序处理，很难并行。 为了解决这个问题，我们需要引入位置编码，也就是t时刻的输入，除了Embedding之外(这是与位置无关的)，我们还引入一个向量，这个向量是与t有关的，我们把Embedding和位置编码向量加起来作为模型的输入。这样的话如果两个词在不同的位置出现了，虽然它们的Embedding是相同的，但是由于位置编码不同，最终得到的向量也是不同的。 位置编码有很多方法，其中需要考虑的一个重要因素就是需要它编码的是相对位置的关系。比如两个句子：“北京到上海的机票”和“你好，我们要一张北京到上海的机票”。显然加入位置编码之后，两个北京的向量是不同的了，两个上海的向量也是不同的了，但是我们期望$Q_{北京1} K_{上海1}=Q_{北京2} K_{上海2}$。具体的位置编码算法不是这里关注的重点，我们以后再介绍。位置编码加入后的模型如下图所示： 图：位置编码 举一个简单的例子，假设Embedding的维度是4，那么实际的位置编码应该是这样的： 图：位置编码例子 残差连接在继续之前，我们需要说说编码器架构中的一个细节：每个Encoder中的每个子层(Self-Attention层和全连接层)在其周围都有一个残差连接，然后紧接着是一个 Layer Normalization 层。如下图所示： 图：残差和Layer Normalization 下图则展示了更多细节：输入$x_1,x_2$经Self-Attention层之后变成$z_1,z_2$，然后和残差连接的输入$x_1,x_2$加起来，然后经过Layer Normalization层输出给全连接层；全连接层正如上文提到的也是有一个残差连接和一个Layer Normalization层，最后再输出给上一层。 图：残差和Layer Normalization细节 Decoder和Encoder是类似的，区别在于它多了一个Encoder-Decoder Attention层，这个层的输入除了来自Self-Attention之外还有Encoder最后一层的所有时刻的输出。Encoder-Decoder Attention层的Query来自下一层，而Key和Value则来自Encoder的输出。 如果我们仅考虑一个由两个编码器和两个解码器组成的Transformer，它看起来是这样的: 图：两层Encoder、Decoder堆叠的Transformer 省略部分到这里，Transformer的主体部分已经全部介绍完，关于机器翻译任务解码的细节、最后输出层的设计、损失函数的选取等并不是本文关注的重点，我们会在 Transformer代码实现 中完完全全的实现一遍，在这里便不做多余的介绍。","categories":[],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://www.yingzq.com/tags/NLP/"},{"name":"Transformer","slug":"Transformer","permalink":"http://www.yingzq.com/tags/Transformer/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://www.yingzq.com/tags/Deep-Learning/"},{"name":"BERT","slug":"BERT","permalink":"http://www.yingzq.com/tags/BERT/"}]},{"title":"贪心算法","slug":"greedy-algorithm","date":"2019-10-22T14:04:30.000Z","updated":"2019-12-25T12:39:25.730Z","comments":true,"path":"2019/10/22/greedy-algorithm/","link":"","permalink":"http://www.yingzq.com/2019/10/22/greedy-algorithm/","excerpt":"","text":"简介贪心算法，又名贪婪算法，是在寻找最优解问题时的常用方法。贪心算法分阶段地工作，在每一个阶段，选取当前状态下最好或最优的的选择，而不考虑将来的后果。通常，这意味着选择的是某个局部最优。这种 “眼下能够拿到的就拿” 的策略也正是贪心算法名称的由来，就好像一个贪婪的人，他事事都想要眼前看到最好的那个，看不到长远的东西，也不为最终的结果和将来着想，贪图眼前局部的利益最大化，有点走一步看一步的感觉。 当算法终止时，我们希望局部最优等于全局最优。如果是这样的话，那么算法就是正确的；否则，算法得到的就是一个次最优解（suboptimal solution）。如果不要求绝对最佳答案，有时候也可以使用简单的贪心算法生成近似的答案，而无需使用产生准确答案所需要的复杂算法。 注：本文我们主要考虑第一种情况，也就是贪心算法能得到最优解的情况 贪心算法的应用实例也非常广泛，例如最短路径中的Dijkstra算法，最小生成树中的Prim算法和Kruskal算法。所以贪心算法是值得我们深入了解的。 基本思路首先要明确一个问题可以使用贪心算法的两个基本要素： 贪心选择性质：所求问题的整体最优解可以通过一系列局部最优的选择，即贪心选择来达到 最优子结构性质：一个问题的最优解包含其子问题的最优解 贪心算法的基本过程如下： 建立数学模型来描述问题 把求解的问题分成若干个子问题 对每一子问题求解，得到子问题的局部最优解 把子问题的解局部最优解合成原来解问题的一个解 其实上面的两段话大致有个印象就好，具体的做法我们接下来通过一些例子学习。 例题分析分发饼干（LeetCode455 easy）题目：假设你是一位很棒的家长，想要给你的孩子们一些小饼干。但是，每个孩子最多只能给一块饼干。对每个孩子 i ，都有一个胃口值 gi ，这是能让孩子们满足胃口的饼干的最小尺寸；并且每块饼干 j ，都有一个尺寸 sj 。如果 sj &gt;= gi ，我们可以将这个饼干 j 分配给孩子 i ，这个孩子会得到满足。你的目标是尽可能满足越多数量的孩子，并输出这个最大数值。 注意： 你可以假设胃口值为正。 一个小朋友最多只能拥有一块饼干。 示例： 输入: [1,2,3], [1,1]输出: 1解释:你有三个孩子和两块小饼干，3个孩子的胃口值分别是：1,2,3。虽然你有两块小饼干，由于他们的尺寸都是1，你只能让胃口值是1的孩子满足。所以你应该输出1。 分析咋一看似乎有点麻烦，回溯、动态规划什么的已经开始酝酿了！但是再一想，本题的目标是尽可能满足多数量的孩子，那么很显然有如下结论：无论饼干情况如何，肯定是先满足胃口小的孩子是最优的；反之对于某个特定孩子而言，饼干肯定要选择最小的但是能满足该孩子的是最优的。 所以这题可以使用贪心算法，优先使用最小的饼干满足最小的胃口，如果不可以，则丢弃这个饼干，采用次小的，依次类推，直到没有饼干或者没有小朋友。具体代码思路如下： 对g和s升序排序 初始化两个指针i和j分别指向g和s初始位置 对比g[i]和s[j] g[i] &lt;= s[j]： 饼干满足胃口，把能满足的孩子数量加 1，并移动指针i = i + 1，j = j + 1 g[i] &gt; s[j]：无法满足胃口，j右移，继续查看下一块饼干是否可以满足胃口 Java代码 class Solution &#123; public int findContentChildren(int[] g, int[] s) &#123; Arrays.sort(g); Arrays.sort(s); int i = 0; for(int j=0;i&lt;g.length &amp;&amp; j&lt;s.length;j++) &#123; if(g[i]&lt;=s[j]) i++; &#125; return i; &#125;&#125; 柠檬水找零（LeetCode 860 easy）题目在柠檬水摊上，每一杯柠檬水的售价为 5 美元。顾客排队购买你的产品，按账单 bills 支付的顺序一次购买一杯。每位顾客只买一杯柠檬水，然后向你付 5 美元、10 美元或 20 美元。你必须给每个顾客正确找零，也就是说净交易是每位顾客向你支付 5 美元。 注意，一开始你手头没有任何零钱。如果你能给每位顾客正确找零，返回 true ，否则返回 false 。 示例 输入：[5,5,5,10,20]输出：true解释：前 3 位顾客那里，我们按顺序收取 3 张 5 美元的钞票。第 4 位顾客那里，我们收取一张 10 美元的钞票，并返还 5 美元。第 5 位顾客那里，我们找还一张 10 美元的钞票和一张 5 美元的钞票。由于所有客户都得到了正确的找零，所以我们输出 true。 分析顾客付款有三种可能：5、10、20。 如果收到5美元，直接把5元放进口袋，什么也不用做 如果收到的10美元，则需要翻一翻自己的口袋里是否有5美元的零钱，没有则交易失败，可以关门回家了（返回false） 如果收到20美元，这时可以找零10美元+5美元或者三张5美元，这个时候如何抉择呢？我们使用贪心策略，尽可能地多留5元在手上，防止后面付款10元的顾客无法找零，也就是优先使用10美元+5美元，如果没有则使用5美元+5美元+5美元的组合，都不成立的话则交易失败，可以关门回家了（返回false） 可以很显然的看出，当算法终止时局部最优是等于全局最优的，这一题的贪心策略是完全正确的。 Java代码 class Solution &#123; public boolean lemonadeChange(int[] bills) &#123; int five = 0, ten = 0; for (int i : bills) &#123; if (i == 5) five++; else if (i == 10) &#123;five--; ten++;&#125; else if (ten &gt; 0) &#123;ten--; five--;&#125; else five -= 3; if (five &lt; 0) return false; &#125; return true; &#125;&#125; 跳跃游戏（LeetCode 55 medium）给定一个非负整数数组，你最初位于数组的第一个位置。数组中的每个元素代表你在该位置可以跳跃的最大长度。判断你是否能够到达最后一个位置。 示例 输入: [2,3,1,1,4]输出: true解释: 我们可以先跳 1 步，从位置 0 到达 位置 1, 然后再从位置 1 跳 3 步到达最后一个位置。输入: [3,2,1,0,4]输出: false解释: 无论怎样，你总会到达索引为 3 的位置。但该位置的最大跳跃长度是 0 ， 所以你永远不可能到达最后一个位置。 分析这题粗略一看是可以用动态规划完成的，但是仔细研读后，其实我们很容易有这种想法： 如果某一个作为起跳点的格子可以跳跃的距离是3，那么表示后面3个格子都可以作为起跳点 那么我们选择这3个起跳点中的哪一个呢？其实在这里，我们是希望选择的新的起跳点可以让后续的跳跃尽可能的远，这样的局部最优很显然是全局最优。所以可以对每一个能作为起跳点的格子都尝试跳一次，把能跳到的最远距离不断更新 如果可以一直跳到最后就成功了，反之则失败 那什么是能跳到的最远距离呢？在代码中便是i+nums[i]，也就是此时数组的下标加上该位置数组元素对应的数值。 Java代码 class Solution &#123; public boolean canJump(int[] nums) &#123; int max = 0; for(int i = 0;i &lt; nums.length;i++) &#123; if(max &lt; i) return false; max = Math.max(i + nums[i], max); &#125; return true; &#125;&#125; 参考文献 小白带你学—贪心算法 LeetCode Algorithms","categories":[],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://www.yingzq.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"Java","slug":"Java","permalink":"http://www.yingzq.com/tags/Java/"},{"name":"算法设计技巧","slug":"算法设计技巧","permalink":"http://www.yingzq.com/tags/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E6%8A%80%E5%B7%A7/"},{"name":"LeetCode","slug":"LeetCode","permalink":"http://www.yingzq.com/tags/LeetCode/"}]},{"title":"Git的杀手锏：分支模型（下）","slug":"git-branching-part-two","date":"2019-10-16T05:12:25.000Z","updated":"2019-10-22T15:50:39.871Z","comments":true,"path":"2019/10/16/git-branching-part-two/","link":"","permalink":"http://www.yingzq.com/2019/10/16/git-branching-part-two/","excerpt":"在 Git的杀手锏：分支模型（上）中，我们已经初步了解了Git分支的基本原理和优良特征，同时也学会了Git分支新建和合并的基本操作。在本章我们继续来学习Git分支的工作流、远程分支和变基等内容。","text":"在 Git的杀手锏：分支模型（上）中，我们已经初步了解了Git分支的基本原理和优良特征，同时也学会了Git分支新建和合并的基本操作。在本章我们继续来学习Git分支的工作流、远程分支和变基等内容。 Git分支开发工作流现在我们已经学会新建和合并分支，那么可以或者应该用它来做些什么呢？本节会介绍一些常见的利用分支进行开发的工作流程。而正是由于分支管理的便捷，才衍生出这些典型的工作模式。 长期分支因为Git使用简单的三方合并，所以就算在一段较长的时间内，反复把一个分支合并入另一个分支，也不是什么难事。也就是说，在整个项目开发周期的不同阶段，你可以同时拥有多个开放的分支；你可以定期地把某些特性分支合并入其他分支中。 许多使用Git的开发者都喜欢使用这种方式来工作，比如只在master分支上保留完全稳定的代码——有可能仅仅是已经发布或即将发布的代码。他们还有一些名为develop或者next的平行分支，被用来做后续开发或者测试稳定性——这些分支不必保持绝对稳定，但是一旦达到稳定状态，它们就可以被合并入master分支了。这样，在确保这些已完成的特性分支（短期分支，比如之前的iss53分支）能够通过所有测试，并且不会引入更多bug之后，就可以合并入主干分支中，等待下一次的发布。 事实上我们刚才讨论的，是随着你的提交而不断右移的指针。稳定分支的指针总是在提交历史中落后一大截，而前沿分支的指针往往比较靠前。 图1：渐进稳定分支的线性图 通常把他们想象成流水线（work silos）可能更好理解一点，那些经过测试考验的提交会被遴选到更加稳定的流水线上去。 图2：渐进稳定分支的流水线视图 你可以用这种方法维护不同层次的稳定性。一些大型项目还有一个proposed（建议）或 pu: proposed updates（建议更新）分支，它可能因包含一些不成熟的内容而不能进入next或者master分支。这么做的目的是使你的分支具有不同级别的稳定性；当它们具有一定程度的稳定性后，再把它们合并入具有更高级别稳定性的分支中。再次强调一下，使用多个长期分支的方法并非必要，但是这么做通常很有帮助，尤其是当你在一个非常庞大或者复杂的项目中工作时。 特性分支特性分支对任何规模的项目都适用。特性分支是一种短期分支，它被用来实现单一特性或其相关工作。也许你从来没有在其他的版本控制系统上这么做过，因为在那些版本控制系统中创建和合并分支通常很费劲。然而，在Git中一天之内多次创建、使用、合并、删除分支都很常见。 你已经在前文中创建的iss53和hotfix特性分支中看到过这种用法。你在特性分支（iss53和hotfix分支）中提交了一些更新，并且在它们合并入主干分支之后，你又删除了它们。这项技术能使你快速并且完整地进行上下文切换（context-switch）——因为你的工作被分散到不同的流水线中，在不同的流水线中每个分支都仅与其目标特性相关，因此，在做代码审查之类的工作的时候就能更加容易地看出你做了哪些改动。 你可以把做出的改动在特性分支中保留几分钟、几天甚至几个月，等它们成熟之后再合并，而不用在乎它们建立的顺序或工作进度。 考虑这样一个例子，你在master分支上工作到C1，这时为了解决一个问题而新建iss91分支，在iss91分支上工作到C4，然而对于那个问题你又有了新的想法，于是你再新建一个iss91v2分支试图用另一种方法解决那个问题，接着你回到master分支工作了一会儿，你又冒出了一个不太确定的想法，你便在C10的时候新建一个dumbidea分支，并在上面做些实验。你的提交历史看起来像下面这个样子： 图3：拥有多个特性分支的提交历史 现在，我们假设两件事情：你决定使用第二个方案来解决那个问题，即使用在iss91v2分支中方案；另外，你将dumbidea分支拿给你的同事看过之后，结果发现这是个惊人之举。这时你可以抛弃iss91分支（即丢弃C5和C6提交），然后把另外两个分支合并入主干分支。最终你的提交历史看起来像下面这个样子： 图4：合并了dumbidea和iss91v2分支之后的提交历史 更多有关分支工作流的细节可以参考 Distributed Workflows 最后请牢记，当你做这么多操作的时候，这些分支全部都存于本地。当你新建和合并分支的时候，所有这一切都只发生在你本地的Git版本库中，没有与服务器发生交互。 Git远程分支远程引用是对远程仓库的引用（指针），包括分支、标签等等。你可以通过git ls-remote [remote]来显式地获得远程引用的完整列表，或者通过git remote show [remote]获得远程分支的更多信息。然而，一个更常见的做法是利用远程跟踪分支。 远程跟踪分支是远程分支状态的引用。它们是你不能移动的本地引用，当你做任何网络通信操作时，它们会自动移动。远程跟踪分支像是你上次连接到远程仓库时，那些分支所处状态的书签。 它们以&lt;remote&gt;/&lt;branch&gt;形式命名。例如，如果你想要看你最后一次与远程仓库origin通信时master分支的状态，你可以查看origin/master分支。你与同事合作解决一个问题并且他们推送了一个iss53分支，你可能有自己的本地iss53分支；但是在服务器上的分支会指向origin/iss53的提交。 这可能有一点儿难以理解，让我们来看一个例子。假设你的网络里有一个在git.ourcompany.com的Git服务器。如果你从这里克隆，Git的clone命令会为你自动将其命名为origin，拉取它的所有数据，创建一个指向它的master分支的指针，并且在本地将其命名为origin/master。Git也会给你一个与origin的master分支在指向同一个地方的本地master分支，这样你就有工作的基础。 注意：“origin” 并无特殊含义远程仓库名字 “origin” 与分支名字 “master” 一样，在Git中并没有任何特别的含义一样。同时 “master” 是当你运行git init时默认的起始分支名字，原因仅仅是它的广泛使用，“origin” 是当你运行git clone时默认的远程仓库名字。如果你运行git clone -o booyah，那么你默认的远程分支名字将会是booyah/master。 图5：克隆之后的服务器与本地仓库 如果你在本地的master分支做了一些工作，与此同时，其他人推送提交到git.ourcompany.com并更新了它的master分支，那么你的提交历史将向不同的方向前进。另外，只要你不与origin服务器连接，你的origin/master指针就不会移动。 图6：本地与远程的工作可以分叉 如果要同步你的工作，运行git fetch &lt;remote&gt;命令（在本例中是git fetch origin）。这个命令查找 “origin” 是哪一个服务器（在本例中是git.ourcompany.com），从中抓取本地没有的数据，并且更新本地数据库，移动origin/master指针指向更新后的位置。 图7：git fetch更新远程仓库引用 为了演示有多个远程仓库与远程分支的情况，我们假定你有另一个内部Git服务器，仅用于你小组的开发工作。这个服务器位于git.team1.ourcompany.com。你可以运行git remote add命令添加一个新的远程仓库引用到当前的项目。将这个远程仓库命名为teamone，将其作为整个URL的缩写。 图8：添加另一个远程仓库 现在，可以运行git fetch teamone来抓取远程仓库teamone有而本地没有的数据。因为那台服务器上现有的数据是origin服务器上的一个子集，所以Git并不会抓取数据而是会设置远程跟踪分支teamone/master指向teamone的master分支。 图9：远程跟踪分支teamone/master 推送当你想要公开分享一个分支时，需要将其推送到有写入权限的远程仓库上。本地的分支并不会自动与远程仓库同步——你必须显式地推送想要分享的分支。这样，你就可以把不愿意分享的内容放到私人分支上，而将需要和别人协作的内容推送到公开分支。 如果希望和别人一起在名为serverfix的分支上工作，你可以像推送第一个分支那样推送它。运行git push &lt;remote&gt; &lt;branch&gt;: $ git push origin serverfixCounting objects: 24, done.Delta compression using up to 8 threads.Compressing objects: 100% (15&#x2F;15), done.Writing objects: 100% (24&#x2F;24), 1.91 KiB | 0 bytes&#x2F;s, done.Total 24 (delta 2), reused 0 (delta 0)To https:&#x2F;&#x2F;github.com&#x2F;schacon&#x2F;simplegit * [new branch] serverfix -&gt; serverfix 这里有些工作被简化了。Git自动将serverfix分支名字展开为refs/heads/serverfix:refs/heads/serverfix，那意味着，“推送本地的serverfix分支来更新远程仓库上的serverfix分支。” 你也可以运行git push origin serverfix:serverfix，它会做同样的事——也就是说 “推送本地的serverfix分支，将其作为远程仓库的serverfix分支”。你也可以通过这种格式来推送本地分支到一个命名不相同的远程分支，例如你并不想让远程仓库上的分支叫做serverfix，可以运行git push origin serverfix:awesomebranch来将本地的serverfix分支推送到远程仓库上的awesomebranch分支。 下一次其他协作者从服务器上抓取数据时，他们会在本地生成一个远程分支origin/serverfix，指向服务器的serverfix分支的引用： $ git fetch originremote: Counting objects: 7, done.remote: Compressing objects: 100% (2&#x2F;2), done.remote: Total 3 (delta 0), reused 3 (delta 0)Unpacking objects: 100% (3&#x2F;3), done.From https:&#x2F;&#x2F;github.com&#x2F;schacon&#x2F;simplegit * [new branch] serverfix -&gt; origin&#x2F;serverfix 要特别注意的一点是当抓取到新的远程跟踪分支时，本地不会自动生成一份可编辑的副本。换句话说，这种情况下，不会有一个新的serverfix分支，只有一个不可以修改的origin/serverfix指针。 可以运行git merge origin/serverfix将这些工作合并到当前所在的分支。如果想要在自己的serverfix分支上工作，可以将其建立在远程跟踪分支之上： $ git checkout -b serverfix origin&#x2F;serverfixBranch serverfix set up to track remote branch serverfix from origin.Switched to a new branch &#39;serverfix&#39; 这会给你一个用于工作的本地分支，并且起点位于origin/serverfix。 跟踪分支从一个远程跟踪分支检出一个本地分支会自动创建所谓的“跟踪分支”（它跟踪的分支叫做“上游分支”）。跟踪分支是与远程分支有直接关系的本地分支。如果在一个跟踪分支上输入git pull，Git能自动地识别去哪个服务器上抓取、合并到哪个分支。 当克隆一个仓库时，它通常会自动地创建一个跟踪origin/master的master分支。然而，如果你愿意的话可以设置其他的跟踪分支，或是一个在其他远程仓库上的跟踪分支，又或者不跟踪master分支。最简单的实例就是像之前看到的那样，运行git checkout -b &lt;branch&gt; &lt;remote&gt;/&lt;branch&gt;。这是一个十分常用的操作所以Git提供了--track快捷方式： $ git checkout --track origin&#x2F;serverfixBranch serverfix set up to track remote branch serverfix from origin.Switched to a new branch &#39;serverfix&#39; 事实上，这个命令实在是太常用了，所以有一个快捷方式中的快捷方式。如果您试图检出的分支满足(a)名称不存在(b)有且仅有一个远程分支名称和它完全匹配，Git将为您创建跟踪分支： $ git checkout serverfixBranch serverfix set up to track remote branch serverfix from origin.Switched to a new branch &#39;serverfix&#39; 如果想要将本地分支与远程分支设置为不同名字，你可以轻松地使用原始版本的命令增加一个不同名字的本地分支： $ git checkout -b sf origin&#x2F;serverfixBranch sf set up to track remote branch serverfix from origin.Switched to a new branch &#39;sf&#39; 现在，本地分支sf会自动从origin/serverfix拉取。 设置已有的本地分支跟踪一个刚刚拉取下来的远程分支，或者想要修改正在跟踪的上游分支，你可以在任意时间使用-u或--set-upstream-to选项运行git branch来显式地设置。 $ git branch -u origin&#x2F;serverfixBranch serverfix set up to track remote branch serverfix from origin. 注意：上游快捷方式当设置好跟踪分支后，可以通过@{upstream}或@{u}快捷方式来引用它。所以在master分支时并且它正在跟踪origin/master时，如果愿意的话可以使用git merge @{u}来取代git merge origin/master。 如果想要查看设置的所有跟踪分支，可以使用git branch的-vv选项。这会将所有的本地分支列出来并且包含更多的信息，如每一个分支正在跟踪哪个远程分支与本地分支是否是领先、落后或是都有。 $ git branch -vv iss53 7e424c3 [origin&#x2F;iss53: ahead 2] forgot the brackets master 1ae2a45 [origin&#x2F;master] deploying index fix* serverfix f8674d9 [teamone&#x2F;server-fix-good: ahead 3, behind 1] this should do it testing 5ea463a trying something new 这里可以看到iss53分支正在跟踪origin/iss53并且 “ahead” 是2，意味着本地有两个提交还没有推送到服务器上。也能看到master分支正在跟踪origin/master分支并且是最新的。接下来可以看到serverfix分支正在跟踪teamone服务器上的server-fix-good分支并且领先3落后1，意味着服务器上有一次提交还没有合并入同时本地有三次提交还没有推送。最后看到testing分支并没有跟踪任何远程分支。 需要重点注意的一点是这些数字的值来自于你从每个服务器上最后一次抓取的数据。这个命令并没有连接服务器，它只会告诉你关于本地缓存的服务器数据。如果想要统计最新的领先与落后数字，需要在运行此命令前抓取所有的远程仓库。可以像这样做： $ git fetch --all; git branch -vv 拉取当git fetch命令从服务器上抓取本地没有的数据时，它并不会修改工作目录中的内容。它只会获取数据然后让你自己合并。然而，有一个命令叫作git pull在大多数情况下它的含义是一个git fetch紧接着一个git merge命令。如果有一个像之前章节中演示的设置好的跟踪分支，不管它是显式地设置还是通过clone或checkout命令为你创建的，git pull都会查找当前分支所跟踪的服务器与分支，从服务器上抓取数据然后尝试合并入那个远程分支。 由于git pull的魔法经常令人困惑所以通常单独显式地使用fetch与merge命令会更好一些。 删除远程分支假设你已经通过远程分支做完所有的工作了，也就是说你和你的协作者已经完成了一个特性并且将其合并到了远程仓库的master分支（或任何其他稳定代码分支）。可以运行带有--delete选项的git push命令来删除一个远程分支。例如想要从服务器上删除serverfix分支，运行下面的命令： $ git push origin --delete serverfixTo https:&#x2F;&#x2F;github.com&#x2F;schacon&#x2F;simplegit - [deleted] serverfix 基本上这个命令做的只是从服务器上移除这个指针。Git服务器通常会保留数据一段时间直到垃圾回收运行，所以如果不小心删除掉了，通常是很容易恢复的。 变基在Git中整合来自不同分支的修改主要有两种方法：merge以及rebase。在本节中我们将学习什么是“变基”，怎样使用“变基”，并将展示该操作的惊艳之处，以及指出在何种情况下你应避免使用它。 变基的基本操作考虑之前遇到的情况，开发任务分叉到了两个不同分支，又各自提交了更新。 图10：分叉的提交历史 之前介绍过，整合分支最容易的方法是merge命令。它会把两个分支的最新快照（C3和C4）以及二者最近的共同祖先（C2）进行三方合并，合并的结果是生成一个新的快照（并提交），如下图所示： 图11：通过合并操作来整合分叉了的历史 其实，还有一种方法：你可以提取在C4中引入的补丁和修改，然后在C3的基础上应用一次。在Git中，这种操作就叫做 变基。 你可以使用rebase命令将提交到某一分支上的所有修改都移至另一分支上，就好像“重新播放”一样。 在上面这个例子中，运行： $ git checkout experiment$ git rebase masterFirst, rewinding head to replay your work on top of it...Applying: added staged command 它的原理是首先找到这两个分支（即当前分支experiment、变基操作的目标基底分支master）的最近共同祖先C2，然后对比当前分支相对于该祖先的历次提交，提取相应的修改并存为临时文件，然后将当前分支指向目标基底C3，最后以此将之前另存为临时文件的修改依序应用。 图12：将C4中的修改变基到C3上 现在回到master分支，进行一次快进合并。 $ git checkout master$ git merge experiment 图13：master分支的快进合并 此时，C4&#39;指向的快照就和上面使用merge命令的例子中C5指向的快照一模一样了。这两种整合方法的最终结果没有任何区别，但是变基使得提交历史更加整洁。你在查看一个经过变基的分支的历史记录时会发现，尽管实际的开发工作是并行的，但它们看上去就像是串行的一样，提交历史是一条直线没有分叉。 一般我们这样做的目的是为了确保在向远程分支推送时能保持提交历史的整洁，例如向某个其他人维护的项目贡献代码时。在这种情况下，你首先在自己的分支里进行开发，当开发完成时你需要先将你的代码变基到origin/master上，然后再向主项目提交修改。这样的话，该项目的维护者就不再需要进行整合工作，只需要快进合并便可。 请注意，无论是通过变基，还是通过三方合并，整合的最终结果所指向的快照始终是一样的，只不过提交历史不同罢了。变基是将一系列提交按照原有次序依次应用到另一分支上，而合并是把最终结果合在一起。 更有趣的变基例子在对两个分支进行变基时，所生成的“重放”并不一定要在目标分支上应用，你也可以指定另外的一个分支进行应用。 就像下图的例子： 图14：从一个特性分支里再分出一个特性分支的提交历史 你创建了一个特性分支server，为服务端添加了一些功能，提交了C3和C4。然后从C3上创建了特性分支client，为客户端添加了一些功能，提交了C8和C9。最后，你回到server分支，又提交了C10。 假设你希望将client中的修改合并到主分支并发布，但暂时并不想合并server中的修改，因为它们还需要经过更全面的测试。这时，你就可以使用git rebase命令的--onto选项，选中在client分支里但不在server分支里的修改（即C8和C9），将它们在master分支上重放： $ git rebase --onto master server client 以上命令的意思是：“取出client分支，找出处于client分支和server分支的共同祖先之后的修改，然后把它们在master分支上重放一遍”。这理解起来有一点复杂，不过效果非常酷。 图15：截取特性分支上的另一个特性分支，然后变基到其他分支 现在可以快进合并master分支了。 $ git checkout master$ git merge client 图16：快进合并master分支，使之包含来自client分支的修改 接下来你决定将server分支中的修改也整合进来。使用git rebase &lt;basebranch&gt; &lt;topicbranch&gt;命令可以直接将特性分支（即本例中的server）变基到目标分支（即master）上。这样做能省去你先切换到server分支，再对其执行变基命令的多个步骤： $ git rebase master server 这样，server中的代码便被“续”到了master后面，如下图所示： 图17：将server中的修改变基到master上 然后就可以快进合并主分支master了： $ git checkout master$ git merge server 至此，client和server分支中的修改都已经整合到主分支里了，你可以删除这两个分支 $ git branch -d client$ git branch -d server 最终提交历史会变成下图中的样子： 图18：最终的提交历史 变基的风险奇妙的变基也并非完美无缺，要用它得遵守一条准则： 不要对在你的仓库外有副本的分支执行变基。 变基操作的实质是丢弃一些现有的提交，然后相应地新建一些内容一样但实际上不同的提交。如果你已经将提交推送至某个仓库，而其他人也已经从该仓库拉取提交并进行了后续工作，此时，如果你用git rebase命令重新整理了提交并再次推送，你的同伴因此将不得不再次将他们手头的工作与你的提交进行整合，如果接下来你还要拉取并整合他们修改过的提交，事情就会变得一团糟。 变基 vs. 合并至此，你已在实战中学习了变基和合并的用法，你一定会想问，到底哪种方式更好。在回答这个问题之前，让我们退后一步，想讨论一下提交历史到底意味着什么。 有一种观点认为，仓库的提交历史即是 记录实际发生过什么。它是针对历史的文档，本身就有价值，不能乱改。从这个角度看来，改变提交历史是一种亵渎，你使用 谎言 掩盖了实际发生过的事情。如果由合并产生的提交历史是一团糟怎么办？既然事实就是如此，那么这些痕迹就应该被保留下来，让后人能够查阅。 另一种观点则正好相反，他们认为提交历史是 项目过程中发生的事。没人会出版一本书的第一版草稿，软件维护手册也是需要反复修订才能方便使用。持这一观点的人会使用rebase及filter-branch等工具来编写故事，怎么方便后来的读者就怎么写。 现在，让我们回到之前的问题上来，到底合并还是变基好？希望你能明白，这并没有一个简单的答案。Git是一个非常强大的工具，它允许你对提交历史做许多事情，但每个团队、每个项目对此的需求并不相同。既然你已经分别学习了两者的用法，相信你能够根据实际情况作出明智的选择。 总的原则是，只对尚未推送或分享给别人的本地修改执行变基操作清理历史，从不对已推送至别处的提交执行变基操作，这样，你才能享受到两种方式带来的便利。 总结我们已经讲完了Git分支与合并的基础知识。你现在应该能自如地创建并切换至新分支、在不同分支之间切换以及合并本地分支。你现在应该也能通过推送你的分支至共享服务以分享它们、使用共享分支与他人协作以及在共享之前使用变基操作合并你的分支。同时通过这些例子，我们也能真真切切的感受到Git分支模型的强大，说分支模型是Git的杀手锏一点也不为过！ 参考文献 Pro Git Book","categories":[],"tags":[{"name":"Git","slug":"Git","permalink":"http://www.yingzq.com/tags/Git/"}]},{"title":"Git的杀手锏：分支模型（上）","slug":"git-branching","date":"2019-10-13T04:11:55.000Z","updated":"2019-10-22T15:50:22.636Z","comments":true,"path":"2019/10/13/git-branching/","link":"","permalink":"http://www.yingzq.com/2019/10/13/git-branching/","excerpt":"几乎所有的版本控制系统都以某种形式支持分支。使用分支意味着你可以把你的工作从开发主线上分离开来，以免影响开发主线。Git处理分支的方式可谓是难以置信的轻量，创建新分支这一操作几乎能在瞬间完成，并且在不同分支之间的切换操作也是一样便捷。与许多其它版本控制系统不同，Git鼓励在工作流程中频繁地使用分支与合并，哪怕一天之内进行许多次。理解和精通这一特性，你便会意识到Git是如此的强大而又独特，并且从此真正改变你的开发方式。","text":"几乎所有的版本控制系统都以某种形式支持分支。使用分支意味着你可以把你的工作从开发主线上分离开来，以免影响开发主线。Git处理分支的方式可谓是难以置信的轻量，创建新分支这一操作几乎能在瞬间完成，并且在不同分支之间的切换操作也是一样便捷。与许多其它版本控制系统不同，Git鼓励在工作流程中频繁地使用分支与合并，哪怕一天之内进行许多次。理解和精通这一特性，你便会意识到Git是如此的强大而又独特，并且从此真正改变你的开发方式。 正如文章题目，这也是Git能在众多版本控制系统脱颖而出的“杀手锏” 初识Git分支Git分支简介在 What is Git 中，我们了解到Git保存的不是文件的变化或者差异，而是一系列不同时刻的文件快照，这和其他版本控制系统很是不同，那么为什么Git要这样做呢？ 在进行提交操作时，Git会保存一个提交对象（commit object）。知道了Git保存数据的方式，我们可以很自然的想到该提交对象会包含一个指向暂存内容快照的指针。但不仅仅是这样，该提交对象还包含了作者的姓名和邮箱、提交时输入的信息以及指向它的父对象的指针。 首次提交产生的提交对象没有父对象，普通提交操作产生的提交对象有一个父对象，而由多个分支合并产生的提交对象有多个父对象 为了更加形象地说明，我们假设现在有一个工作目录，里面包含了三个将要被暂存和提交的文件。暂存操作会为每一个文件计算校验和（使用SHA-1哈希算法），然后会把当前版本的文件快照保存到Git仓库中（Git使用blob对象来保存它们），最终将校验和加入到暂存区域等待提交： $ git add README test.rb LICENSE$ git commit -m &#39;The initial commit of my project&#39; 当使用git commit进行提交操作时，Git会先计算每一个子目录（本例中只有项目根目录）的校验和，然后在Git仓库中这些校验和保存为树对象。随后，Git便会创建一个提交对象，它除了包含上面提到的那些信息外，还包含指向这个树对象（项目根目录）的指针。如此一来，Git就可以在需要的时候重现此次保存的快照。 现在，Git仓库中有五个对象：三个blob对象（保存着文件快照）、一个树对象（记录着目录结构和blob对象索引）以及一个提交对象（包含着指向前述树对象的指针和所有提交信息）。 图1：首次提交对象及其树结构 如果做些修改后再次提交，那么这次产生的提交对象会包含一个指向上次提交对象（父对象）的指针。 图2：提交对象及其父对象 Git的分支，其实本质上仅仅是指向提交对象的轻量级可移动指针。Git的默认分支名字是master。在多次提交操作之后，你其实已经有一个指向最后那个提交对象的master分支。它会在每次的提交操作中自动向前移动。 Git的master分支并不是一个特殊分支。它就跟其它分支完全没有区别。之所以几乎每一个仓库都有master分支，是因为git init命令默认创建它，并且大多数人都懒得去改动它。 图3：分支及其提交历史 Git分支创建Git创建新分支很简单，因为Git只是为你创建了一个可以移动的新的指针。例如创建一个testing分支，你需要使用git branch命令： $ git branch testing 这会在当前所在的提交对象上创建一个指针。 图4：两个指向相同提交历史的分支 那么，Git又是怎么知道当前在哪一个分支上呢？也很简单，它有一个名为HEAD的特殊指针。请注意它和许多其它版本控制系统（如Subversion或CVS）里的HEAD概念完全不同。在Git中，它是一个指针，指向当前所在的本地分支。在本例中，你仍然在master分支上。 因为git branch命令仅仅创建一个新分支，并不会自动切换到新分支中去。 图5：HEAD指向当前所在的分支 你可以简单地使用git log命令查看各个分支当前所指的对象。提供这一功能的参数是--decorate。 $ git log --oneline --decoratef30ab (HEAD -&gt; master, testing) add feature #32 - ability to add new34ac2 fixed bug #1328 - stack overflow under certain conditions98ca9 initial commit of my project 可以看到，当前 “master” 和 “testing” 分支均指向校验和以f30ab开头的提交对象。 Git分支切换要切换到一个已存在的分支，你需要使用git checkout命令。我们现在切换到新创建的testing分支去： $ git checkout testing 这样HEAD就指向testing分支了。 图6：HEAD指向当前所在的分支 那么，这样的实现方式会给我们带来什么好处呢？现在不妨再提交一次： $ vim test.rb$ git commit -a -m &#39;made a change&#39; 图7：HEAD分支随着提交操作自动向前移动 这就有意思了，你的testing分支向前移动了，但是master分支却没有，它仍然指向运行git checkout时所指的对象。现在我们切换回 master 分支看看： $ git checkout master 图8：检出时HEAD随之移动 这条命令做了两件事。一是使HEAD指回master分支，二是将工作目录恢复成master分支所指向的快照内容。也就是说，你现在做修改的话，项目将始于一个较旧的版本。本质上来讲，这就是忽略testing分支所做的修改，以便于向另一个方向进行开发。 注意：分支切换会改变你工作目录中的文件在切换分支时，一定要注意你工作目录里的文件会被改变。如果是切换到一个较旧的分支，你的工作目录会恢复到该分支最后一次提交时的样子。如果Git不能干净利落地完成这个任务，它将禁止切换分支。 我们不妨再稍微做些修改并提交： $ vim test.rb$ git commit -a -m &#39;made other changes&#39; 现在，这个项目的提交历史已经产生了分叉。因为刚才你创建了一个新分支，并切换过去进行了一些工作，随后又切换回master分支进行了另外一些工作。上述两次改动针对的是不同分支：你可以在不同分支间不断地来回切换和工作，并在时机成熟时将它们合并起来。而所有这些工作，你需要的命令只有branch、checkout和commit。 图9：项目分叉历史 你可以简单地使用git log命令查看分叉历史。运行git log --oneline --decorate --graph --all，它会输出你的提交历史、各个分支的指向以及项目的分支分叉情况。 $ git log --oneline --decorate --graph --all* c2b9e (HEAD -&gt; master) made other changes| * 87ab2 (testing) made a change|&#x2F;* f30ab add feature #32 - ability to add new formats to the* 34ac2 fixed bug #1328 - stack overflow under certain conditions* 98ca9 initial commit of my project 由于Git的分支实质上仅是包含所指对象校验和（长度为40的SHA-1值字符串）的文件，所以它的创建和销毁都异常高效。创建一个新分支就相当于往一个文件中写入41个字节（40个字符和1个换行符），如此的简单能不快吗？ 这与过去大多数版本控制系统形成了鲜明的对比，它们在创建分支时，将所有的项目文件都复制一遍，并保存到一个特定的目录。完成这样繁琐的过程通常需要好几秒钟，有时甚至需要好几分钟。所需时间的长短，完全取决于项目的规模。而在Git中，任何规模的项目都能在瞬间创建新分支。 同时，由于每次提交都会记录父对象，所以寻找恰当的合并基础（即共同祖先）也是同样的简单和高效。这些高效的特性使得Git鼓励开发人员频繁地创建和使用分支。 接下来，让我们看看你为什么应该这样做。 注意：Git可以创建分支并同时切换到该分支创建一个分支并希望同时切换到该分支是很常见的，这可以通过git checkout -b &lt;newbranchname&gt;一条命令即可完成！ Git分支新建与合并让我们来看一个简单的Git分支新建与合并的例子，实际工作中你可能会用到类似的工作流。你将经历如下步骤： 开发某个网站。 为实现某个新的需求，创建一个分支。 在这个分支上开展工作。 正在此时，你突然接到一个电话说有个很严重的问题需要紧急修补。你将按照如下方式来处理： 切换到你的线上分支（production branch）。 为这个紧急任务新建一个分支，并在其中修复它。 在测试通过之后，切换回线上分支，然后合并这个修补分支，最后将改动推送到线上分支。 切换回你最初工作的分支上，继续工作。 分支新建首先，我们假设你正在你的项目上工作，并且已经有一些提交。 图10：一个简单提交历史 现在，你已经决定要解决你的公司使用的问题追踪系统中的 #53 问题。想要新建一个分支并同时切换到那个分支上，你可以运行一个带有-b参数的git checkou命令： $ git checkout -b iss53Switched to a new branch &quot;iss53&quot; 它是下面两条命令的简写： $ git branch iss53$ git checkout iss53 图11：创建一个新分支指针 你继续在 #53 问题上工作，并且做了一些提交。在此过程中，iss53分支在不断的向前推进，因为你已经检出到该分支（也就是说，你的HEAD指针指向了iss53分支） $ vim index.html$ git commit -a -m &#39;added a new footer [issue 53]&#39; 图12：iss53分支随着工作的进展向前推进 现在你接到那个电话，有个紧急问题等待你来解决。有了Git的帮助，你不必把这个紧急问题和iss53的修改混在一起，你也不需要花大力气来还原关于 53# 问题的修改，然后再添加关于这个紧急问题的修改，最后将这个修改提交到线上分支。你所要做的仅仅是切换回master分支！ 但是，在你这么做之前，要留意你的工作目录和暂存区里那些还没有被提交的修改，它可能会和你即将检出的分支产生冲突从而阻止Git切换到该分支。最好的方法是，在你切换分支之前，保持好一个干净的状态。有一些方法可以绕过这个问题，即保存进度（stashing）和修补提交（commit amending）。 现在，我们假设你已经把你的修改全部提交了，这时你可以切换回master分支了： $ git checkout masterSwitched to branch &#39;master&#39; 这个时候，你的工作目录和你在开始 #53 问题之前一模一样，现在你可以专心修复紧急问题了。请牢记：当你切换分支的时候，Git会重置你的工作目录，使其看起来像回到了你在那个分支上最后一次提交的样子。Git会自动添加、删除、修改文件以确保此时你的工作目录和这个分支最后一次提交时的样子一模一样。 接下来，你要修复这个紧急问题。让我们建立一个针对该紧急问题的分支（hotfix branch），在该分支上工作直到问题解决： $ git checkout -b hotfixSwitched to a new branch &#39;hotfix&#39;$ vim index.html$ git commit -a -m &#39;fixed the broken email address&#39;[hotfix 1fb7853] fixed the broken email address 1 file changed, 2 insertions(+) 图13：基于master分支的紧急问题分支hotfix branch 你可以运行你的测试，确保你的修改是正确的，然后将其合并回你的master分支来部署到线上。你可以使用git merge命令来达到上述目的： $ git checkout master$ git merge hotfixUpdating f42c576..3a0874cFast-forward index.html | 2 ++ 1 file changed, 2 insertions(+) 在合并的时候，你应该注意到了“快进（fast-forward）”这个词。由于当前master分支所指向的提交是你当前提交（有关hotfix的提交）的直接上游，所以Git只是简单的将指针向前移动。换句话说，当你试图合并两个分支时，如果顺着一个分支走下去能够到达另一个分支，那么Git在合并两者的时候，只会简单的将指针向前推进（指针右移），因为这种情况下的合并操作没有需要解决的分歧——这就叫做“快进（fast-forward）”。 图14：master被快进到hotfix 关于这个紧急问题的解决方案发布之后，你准备回到被打断之前时的工作中。然而，你应该先删除hotfix分支，因为master分支已经指向了同一个位置，所以你已经不再需要它了。你可以使用带-d选项的git branch命令来删除分支： $ git branch -d hotfixDeleted branch hotfix (3a0874c). 现在你可以切换回你正在工作的分支继续你的工作，也就是针对 #53 问题的那个分支（iss53分支）。 $ git checkout iss53Switched to branch &quot;iss53&quot;$ vim index.html$ git commit -a -m &#39;finished the new footer [issue 53]&#39;[iss53 ad82d7a] finished the new footer [issue 53]1 file changed, 1 insertion(+) 图15：继续在iss53分支上的工作 注意到你在hotfix分支上所做的工作并没有包含到iss53分支中。如果你需要拉取hotfix所做的修改，你可以使用git merge master命令将master分支合并入iss53分支，或者你也可以等到iss53分支完成其使命，再将其合并回master分支。 分支合并假设你已经修正了 #53 问题，并且打算将你的工作合并入master分支。为此，你需要合并iss53分支到master分支，这和之前你合并hotfix分支所做的工作差不多。你只需要检出到你想合并入的分支，然后运行git merge命令： $ git checkout masterSwitched to branch &#39;master&#39;$ git merge iss53Merge made by the &#39;recursive&#39; strategy.index.html | 1 +1 file changed, 1 insertion(+) 这和你之前合并hotfix分支的时候看起来有一点不一样。在这种情况下，你的开发历史从一个更早的地方开始分叉开来（diverged）。因为master分支所在提交并不是iss53分支所在提交的直接祖先，Git不得不做一些额外的工作。出现这种情况的时候，Git会使用两个分支的末端所指的快照（C4和C5）以及这两个分支的工作祖先（C2），做一个简单的三方合并。 图16：一次典型合并中所用到的三个快照 和之前将分支指针向前推进所不同的是，Git将此次三方合并的结果做了一个新的快照并且自动创建一个新的提交指向它。这个被称作一次合并提交，它的特别之处在于他有不止一个父提交。 图17：一个合并提交 需要指出的是，Git会自行决定选取哪一个提交作为最优的共同祖先，并以此作为合并的基础；这和更加古老的CVS系统或者Subversion（1.5 版本之前）不同，在这些古老的版本管理系统中，用户需要自己选择最佳的合并基础。Git的这个优势使其在合并操作上比其他系统要简单很多。 既然你的修改已经合并进来了，你已经不再需要iss53分支了。现在你可以在任务追踪系统中关闭此项任务，并删除这个分支。 $ git branch -d iss53 遇到冲突时的分支合并有时候合并操作不会如此顺利。如果你在两个不同的分支中，对同一个文件的同一个部分进行了不同的修改，Git就没法干净的合并它们。如果你对 #53 问题的修改和有关hotfix的修改都涉及到同一个文件的同一处，在合并它们的时候就会产生合并冲突： $ git merge iss53Auto-merging index.htmlCONFLICT (content): Merge conflict in index.htmlAutomatic merge failed; fix conflicts and then commit the result. 此时Git做了合并，但是没有自动地创建一个新的合并提交。Git会暂停下来，等待你去解决合并产生的冲突。你可以在合并冲突后的任意时刻使用git status命令来查看那些因包含合并冲突而处于未合并（unmerged）状态的文件： $ git statusOn branch masterYou have unmerged paths. (fix conflicts and run &quot;git commit&quot;)Unmerged paths: (use &quot;git add &lt;file&gt;...&quot; to mark resolution) both modified: index.htmlno changes added to commit (use &quot;git add&quot; and&#x2F;or &quot;git commit -a&quot;) 任何因包含合并冲突而有待解决的文件，都会以未合并状态标识出来。Git会在有冲突的文件中加入标准的冲突解决标记，这样你可以打开这些包含冲突的文件然后手动解决冲突。出现冲突的文件会包含一些特殊区段，看起来像下面这个样子： &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:index.html&lt;div id&#x3D;&quot;footer&quot;&gt;contact : email.support@github.com&lt;&#x2F;div&gt;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&lt;div id&#x3D;&quot;footer&quot;&gt; please contact us at support@github.com&lt;&#x2F;div&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; iss53:index.html 这表示HEAD所指示的版本（也就是你的master分支所在的位置，因为你在运行merge命令的时候已经检出到了这个分支）在这个区段的上半部分（=======的上半部分），而iss53分支所指示的版本在=======的下半部分。为了解决冲突，你必须选择使用由=======分割的两部分中的一个，或者你也可以自行合并这些内容。例如，你可以通过把这段内容换成下面的样子来解决冲突： &lt;div id&#x3D;&quot;footer&quot;&gt;please contact us at email.support@github.com&lt;&#x2F;div&gt; 上述的冲突解决方案仅保留了其中一个分支的修改，并且&lt;&lt;&lt;&lt;&lt;&lt;&lt;，=======和&gt;&gt;&gt;&gt;&gt;&gt;&gt;这些行被完全删除了。在你解决了所有文件里的冲突之后，对每个文件使用git add命令来将其标记为冲突已解决。一旦暂存这些原本有冲突的文件，Git就会将它们标记为冲突已解决。 如果你想使用图形化工具来解决冲突，你可以运行git mergetool，该命令会为你启动一个合适的可视化合并工具，并带领你一步一步解决这些冲突： $ git mergetoolThis message is displayed because &#39;merge.tool&#39; is not configured.See &#39;git mergetool --tool-help&#39; or &#39;git help config&#39; for more details.&#39;git mergetool&#39; will now attempt to use one of the following tools:tortoisemerge emerge vimdiffMerging:READMENormal merge conflict for &#39;README&#39;: &#123;local&#125;: modified file &#123;remote&#125;: modified fileHit return to start merge resolution tool (vimdiff): 如果你想使用除默认工具（在这里Git使用vimdiff做为默认的合并工具）外的其他合并工具，你可以在“下列工具中（one of the following tools）”这句后面看到所有支持的合并工具。然后输入你喜欢的工具名字就可以了。 如果你需要更加高级的工具来解决复杂的合并冲突，可以参考 Git Tools - Advanced Merging 等你退出合并工具之后，Git会询问刚才的合并是否成功。如果你回答是，Git会暂存那些文件以表明冲突已解决：你可以再次运行git status来确认所有的合并冲突都已被解决： $ git statusOn branch masterAll conflicts fixed but you are still merging. (use &quot;git commit&quot; to conclude merge)Changes to be committed: modified: index.html 如果你对结果感到满意，并且确定之前有冲突的的文件都已经暂存了，这时你可以输入git commit来完成合并提交。默认情况下提交信息看起来像下面这个样子： Merge branch &#39;iss53&#39;Conflicts: index.html## It looks like you may be committing a merge.# If this is not correct, please remove the file# .git&#x2F;MERGE_HEAD# and try again.# Please enter the commit message for your changes. Lines starting# with &#39;#&#39; will be ignored, and an empty message aborts the commit.# On branch master# All conflicts fixed but you are still merging.## Changes to be committed:# modified: index.html# 如果你觉得上述的信息不够充分，不能完全体现分支合并的过程，你可以修改上述信息，添加一些细节给未来检视这个合并的读者一些帮助，告诉他们你是如何解决合并冲突的，以及理由是什么。 Git分支管理现在已经创建、合并、删除了一些分支，让我们看看一些常用的分支管理工具。 git branch命令不只是可以创建与删除分支。如果不加任何参数运行它，会得到当前所有分支的一个列表： $ git branch iss53* master testing 注意master分支前的*字符：它代表现在检出的那一个分支（也就是说，当前HEAD指针所指向的分支）。这意味着如果在这时候提交，master分支将会随着新的工作向前移动。如果需要查看每一个分支的最后一次提交，可以运行git branch -v命令： $ git branch -v iss53 93b412c fix javascript issue* master 7a98805 Merge branch &#39;iss53&#39; testing 782fd34 add scott to the author list in the readmes --merged与--no-merged这两个有用的选项可以过滤这个列表中已经合并或尚未合并到当前分支的分支。如果要查看哪些分支已经合并到当前分支，可以运行git branch --merged： $ git branch --merged iss53* master 因为之前已经合并了iss53分支，所以现在看到它在列表中。在这个列表中分支名字前没有*号的分支通常可以使用git branch -d删除掉；你已经将它们的工作整合到了另一个分支，所以并不会失去任何东西。 查看所有包含未合并工作的分支，可以运行git branch --no-merged： $ git branch --no-merged testing 这里显示了其他分支。因为它包含了还未合并的工作，尝试使用git branch -d命令删除它时会失败： $ git branch -d testingerror: The branch &#39;testing&#39; is not fully merged.If you are sure you want to delete it, run &#39;git branch -D testing&#39;. 如果真的想要删除分支并丢掉那些工作，如同帮助信息里所指出的，可以使用-D选项强制删除它。 总结到这里我们已经初步了解了Git分支的基本原理和优良特征，同时也学会了Git分支新建和合并的基本操作，难以置信的轻量分支模型也正是Git在众多版本控制系统中脱颖而出的杀手锏！ 但是至此关于Git分支的内容还未结束，我们还没充分了解到Git分支的工作流、Git远程分支等内容，限于篇幅将在 Git的杀手锏：分支模型（下）继续讲解。 参考文献 Pro Git Book","categories":[],"tags":[{"name":"Git","slug":"Git","permalink":"http://www.yingzq.com/tags/Git/"}]},{"title":"请务必掌握的Git基础","slug":"git-basics","date":"2019-10-01T05:03:38.000Z","updated":"2019-12-25T12:37:11.913Z","comments":true,"path":"2019/10/01/git-basics/","link":"","permalink":"http://www.yingzq.com/2019/10/01/git-basics/","excerpt":"Git是目前最流行的分布式版本控制系统，值得我们去学会使用并深入了解。本文将会介绍几个最基本的，也是最常用的命令，以后绝大多数时间里用到的可能也就是这几个命令，主要涉及Git的配置、Git仓库的获取、如何记录每次更新到Git仓库、提交历史的查看、Git的撤销操作、Git标签和Git别名等基础知识点，这些是你务必掌握的Git基础！","text":"Git是目前最流行的分布式版本控制系统，值得我们去学会使用并深入了解。本文将会介绍几个最基本的，也是最常用的命令，以后绝大多数时间里用到的可能也就是这几个命令，主要涉及Git的配置、Git仓库的获取、如何记录每次更新到Git仓库、提交历史的查看、Git的撤销操作、Git标签和Git别名等基础知识点，这些是你务必掌握的Git基础！ 前奏在开始阅读前，有几个需要注意的点： 如果你没听说过Git或者还不了解Git是什么，请参阅文章 What is Git 。 Git有多种使用方式，包括原生的和GUI模式，在这里推荐先使用并熟悉命令行模式，因为如果你学会了在命令行下如何操作，那么你在操作GUI软件时应该也不会遇到什么困难，但是，反之则不成立。 Git的安装在不同平台上Git安装的方式有一定差异，在这里不详细介绍。值得注意的是，虽然Git具有很好的向后兼容性，但是还是建议大家最好将它升级到最新的版本。 例如在Ubuntu系统中，可通过如下指令来安装最新稳定版本的Git： $ sudo apt-get install git 在mac系统中则可通过Homebrew来安装最新版本的Git： $ brew install git 更多的安装指导可查看 Installing Git 和 官方下载界面 。 安装完成后，输入指令git --version，返回正常的版本信息则证明已成功安装Git： $ git --versiongit version 2.21.0 注：因为Git的安装是非常方便快捷的，官网也有清晰的指导，所以这一章节讲的非常简略。 初次使用Git前的配置在第一次使用一个新编辑器的时候，你会倾向于把字体大小、界面还有需要的插件等统一配置一下，这样在你以后使用这个编辑器的时候，编辑器便会默认载入这些配置，非常的方便。 Git中也有类似的机制。Git自带一个git config的工具来帮助设置控制Git外观和行为的配置变量，这些变量存储在三个不同的位置： etc/gitconfig文件: 包含系统上每一个用户及他们仓库的通用配置。如果使用带有--system选项的git config时，它会从此文件读写配置变量。 ~/.gitconfig或~/.config/git/config文件：只针对当前用户。可以传递--global选项让Git读写此文件。 当前使用仓库的Git目录中的config文件（也就是.git/config）：针对该仓库。可以传递--local选项让Git读写此文件，但是其实这个选项是默认的，因此可以省略。 每一个级别覆盖上一级别的配置，例如.git/config的配置变量会覆盖/etc/gitconfig中的配置变量。 在Windows系统中，Git会查找$HOME目录下（一般情况下是 C:\\Users\\$USER）的.gitconfig文件。Git同样也会寻找/etc/gitconfig文件，但只限于MSys的根目录下，即安装Git时所选的目标位置。 用户信息首先你需要设置自己的用户名称和邮箱，这样做很重要，因为每一个Git的提交都会使用这些信息，并且它会写入到你的每一次提交中，不可更改（换句话说，不设置用户信息无法进行git commit操作）： $ git config --global user.name &quot;YingZiqiang&quot;$ git config --global user.email yingzq0116@163.com 再次强调，如果使用了--global选项，那么该命令只需要运行一次，因为之后无论你在该系统上做任何事情，Git都会使用那些信息。当你想针对特定项目使用不同的用户名称与邮件地址时，可以在那个项目目录下运行没有--global选项的命令来配置。 文本编辑器当Git需要你输入信息时会调用默认的文本编辑器。如果未配置，Git会使用操作系统默认的文本编辑器，通常是Vim。如果你想使用不同的文本编辑器，例如Emacs，可以这样做： $ git config --global core.editor emacs 检查配置信息如果想要检查你的配置，可以使用git config --list命令来列出所有Git当时能找到的配置。 $ git config --listuser.name&#x3D;YingZiqianguser.email&#x3D;88629850@qq.compush.default&#x3D;simple... 有时候会看到重复的变量名，因为Git可能会从多个配置文件中读取同一个配置变量。这种情况下，Git会使用它找到的每一个变量的最后一个配置。 可以通过输入git config &lt;key&gt;来检查Git的某一项配置： $ git config user.nameYingZiqiang 如果你想确认某配置变量的最终决定权的来源，可以输入git config --show-origin &lt;key&gt;查询： $ git config --show-origin user.namefile:&#x2F;Users&#x2F;yingzq&#x2F;.gitconfig YingZiqiang Git帮助若你使用Git时需要获取帮助，有三种方法可以找到Git命令的使用手册： $ git help &lt;verb&gt;$ git &lt;verb&gt; --help$ man git-&lt;verb&gt; 例如，要想获得config命令的手册，执行 $ git help config 另外，如果你不想查看一个Git命令完整的使用手册，仅仅是想要快速查看某些参数的用法，可以通过-h选项来获得一个更加简洁的帮助界面，例如查看git add命令的参数信息： $ git add -h 注：你可以随时随地可以使用这些命令而无需联网 获取Git仓库有两种取得Git项目仓库的方法。第一种是在现有项目或目录下导入所有文件到Git中；第二种是从一个服务器克隆一个现有的Git仓库。 在现有目录中初始化仓库如果你打算使用Git来对现有的项目进行管理，你只需要进入该项目目录并输入： $ git init 该命令将创建一个名为.git的子目录，这个子目录含有你初始化的Git仓库中所有的必须文件，这些文件是Git仓库的骨干。但是，在这个时候，我们仅仅是做了一个初始化的操作，你的项目里的文件还没有被跟踪。 如果你是在一个已经存在文件的文件夹（而不是空文件夹）中初始化Git仓库来进行版本控制的话，你应该开始跟踪这些文件并提交。你可通过git add命令来实现对指定文件的跟踪，然后执行git commit提交： $ git add *.py$ git add LICENSE$ git commit -m &#39;initial project version&#39; 稍后会逐一解释每一条指令的意思。现在，你已经得到了一个实际维护（或者说是跟踪）着若干个文件的Git仓库。 克隆现有的仓库如果你想获得一份已经存在了的Git仓库的拷贝，比如说，你想为某个开源项目贡献自己的一份力，这时就要用到git clone命令。 克隆仓库的命令格式是git clone [url]。比如，要克隆一个叫“TensorFlow-Examples”的库，可以用下面的命令： $ git clone https:&#x2F;&#x2F;github.com&#x2F;aymericdamien&#x2F;TensorFlow-Examples 这会在当前目录下创建一个名为“TensorFlow-Examples”的目录，并在这个目录下初始化一个.git文件夹，并从远程仓库拉取下所有数据放入.git文件夹，然后从中读取最新版本的文件的拷贝。如果你进入到这个新建的“TensorFlow-Examples”文件夹，你会发现所有的项目文件已经在里面了，准备就绪等待后续的开发和使用。 如果你想在克隆远程仓库的时候，自定义本地仓库的名字，你可以使用如下命令： $ git clone https:&#x2F;&#x2F;github.com&#x2F;aymericdamien&#x2F;TensorFlow-Examples my-tf-examples 这将执行与上一个命令相同的操作，不过在本地创建的仓库名字变为“my-tf-examples”。 Git支持多种数据传输协议。上面的例子使用的是https://协议，不过你也可以使用git://协议或者使用例如user@server:path/to/repo.git的SSH传输协议。 注：Git克隆的是该Git仓库服务器上的几乎所有数据，而不是仅仅复制完成你的工作所需要文件。当你执行git clone命令的时候，默认配置下远程Git仓库中的每一个文件的每一个版本都将被拉取下来。 记录每次更新到仓库现在我们手上有了一个真实项目的Git仓库，并从这个仓库中取出了所有文件的工作拷贝。接下来，对这些文件做些修改，在完成了一个阶段的目标之后，提交本次更新到仓库。 工作目录下的每一个文件都不外乎这两种状态：已跟踪或未跟踪。已跟踪的文件是指那些被纳入了版本控制的文件，在上一次快照中有它们的记录，在工作一段时间后，它们的状态可能处于未修改，已修改或已放入暂存区。工作目录中除已跟踪文件以外的所有其它文件都属于未跟踪文件，它们既不存在于上次快照的记录中，也没有放入暂存区。初次克隆某个仓库的时候，工作目录中的所有文件都属于已跟踪文件，并处于未修改状态。 图1：文件的状态变化周期 检查当前文件状态要查看哪些文件处于什么状态，可以用git status命令。如果在克隆仓库后立即使用此命令，会看到类似这样的输出： $ git statusOn branch masterYour branch is up to date with &#39;origin&#x2F;master&#39;.nothing to commit, working tree clean 这说明你现在的工作目录相当干净。换句话说，所有已跟踪文件在上次提交后都未被更改过。此外，上面的信息还表明，当前目录下没有出现任何处于未跟踪状态的新文件，否则Git会在这里列出来。最后，该命令还显示了当前所在分支，并告诉你这个分支同远程服务器上对应的分支没有偏离。现在，分支名是“master”，这是默认的分支名。在这里不用担心不明白分支是什么，这一部分会在 Git的杀手锏：分支模型（上） 单独讲解。 如果在项目中创建一个新的README文件，如果之前并不存在这个文件，使用git status命令，你将看到一个新的未跟踪文件： $ echo &#39;My Project&#39; &gt; README$ git statusOn branch masterYour branch is up to date with &#39;origin&#x2F;master&#39;.Untracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) READMEnothing added to commit but untracked files present (use &quot;git add&quot; to track) 在状态报告中可以看到新建的README文件出现在“Untracked files”下面。未跟踪的文件意味着Git在之前的快照（提交）中没有这些文件；Git不会自动将之纳入跟踪范围，除非你明明白白地告诉它“我需要跟踪该文件”，这样的处理让你不必担心将生成的二进制文件或其它不想被跟踪的文件包含进来。不过现在的例子中，我们确实想要跟踪管理README这个文件。 跟踪新文件使用命令git add开始跟踪一个文件。所以如果要跟踪README文件，运行： $ git add README 此时再运行git status命令，会看到README文件已被跟踪，并处于暂存状态： $ git statusOn branch masterYour branch is up to date with &#39;origin&#x2F;master&#39;.Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) new file: README 只要在“Changes to be committed”这行下面的，就说明是已暂存状态。如果此时提交，那么该文件此时此刻的版本将被留存在历史记录中。你可能会想起之前我们使用git init后就运行了git add &lt;files&gt;命令，开始跟踪当前目录下的文件。git add命令使用文件或目录的路径作为参数；如果参数是目录的路径，该命令将递归地跟踪该目录下的所有文件。 暂存已修改文件现在我们来修改一个已被跟踪的文件。如果你修改了一个名为TINY.md的已被跟踪的文件，然后运行git status命令，会看到下面内容： $ git statusOn branch masterYour branch is up to date with &#39;origin&#x2F;master&#39;.Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) new file: READMEChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: TINY.md 文件TINY.md出现在“Changes not staged for commit”这行下面，说明已跟踪文件的内容发生了变化，但还没有放到暂存区。要暂存这次更新，需要运行git add命令。git add是个多功能命令：可以用它开始跟踪新文件，或者把已跟踪的文件放到暂存区，还能用于合并时把有冲突的文件标记为已解决状态等。将这个命令理解为“添加内容到下一次提交中”而不是“将一个文件添加到项目中”要更加合适。 现在让我们运行git add将TINY.md放到暂存区，然后再看看git status的输出： $ git add TINY.md$ git statusOn branch masterYour branch is up to date with &#39;origin&#x2F;master&#39;.Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) new file: README modified: TINY.md 现在两个文件都已暂存，下次提交时就会一并记录到仓库。 这里再追加一个小问题，假设此时，你的TINY.md文件存在一些小瑕疵，于是你重新编辑该文件并存盘了，此时运行git status会发生什么呢？ $ vim TINY.md$ git statusOn branch masterYour branch is up to date with &#39;origin&#x2F;master&#39;.Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) new file: README modified: TINY.mdChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: TINY.md 怎么回事？现在TINY.md文件同时出现在暂存区和非暂存区。这怎么可能呢？实际上Git只不过暂存了你运行git add命令时的版本，如果你现在提交，TINY.md的版本是你最后一次运行git add命令时的那个版本，而不是你运行git commit时，在工作目录中的当前版本。 所以，运行了git add之后又作了修订的文件，需要重新运行git add把最新版本重新暂存起来： $ git add TINY.md$ git statusOn branch masterYour branch is up to date with &#39;origin&#x2F;master&#39;.Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) new file: README modified: TINY.md 状态简览git status命令的输出十分详细，但其用语有些繁琐。如果你使用git status -s命令或git status --short命令，你将得到一种更为紧凑的格式输出。运行git status -s，状态报告输出如下： $ git status -sA READMEM TINY.md 新添加的未跟踪文件前面有??标记，新添加到暂存区中的文件前面有A标记，修改过的文件前面有M标记。其中M有两个可以出现的位置，出现在右边的M表示该文件被修改了但是还没放入暂存区，出现在靠左边的M表示该文件被修改了并放入了暂存区。 忽略文件一般我们总会有些文件无需纳入Git的管理，也不希望它们总出现在未跟踪文件列表。通常都是些自动生成的文件，比如日志文件，或者编译过程中创建的临时文件等。在这种情况下，我们可以创建一个名为.gitignore的文件，列出要忽略的文件模式。来看一个实际的例子： $ cat .gitignore*.[oa]*~ 第一行告诉Git忽略所有以“.o”或“.a”结尾的文件。一般这类对象文件和存档文件都是编译过程中出现的。第二行告诉Git忽略所有以波浪符（~）结尾的文件，许多文本编辑软件（比如Emacs）都用这样的文件名保存副本。此外，你可能还需要忽略log，tmp或者pid目录，以及自动生成的文档等等。要养成一开始就设置好.gitignore文件的习惯，以免将来误提交这类无用的文件。 文件.gitignore的格式规范如下： 所有空行或者以#开头的行都会被Git忽略。 可以使用标准的glob模式匹配，并且会在整个工作目录中递归的应用。 匹配模式可以以（/）开头防止递归。 匹配模式可以以（/）结尾指定目录。 要忽略指定模式以外的文件或目录，可以在模式前加上感叹号（!）取反。 所谓的glob模式是指所使用的简化了的正则表达式。星号（*）匹配零个或多个任意字符；[abc]匹配任何一个列在方括号中的字符（这个例子要么匹配一个a，要么匹配一个b，要么匹配一个c）；问号（?）只匹配一个任意字符；如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配（比如[0-9]表示匹配所有0到9的数字）。使用两个星号表示匹配任意中间目录，比如a/**/z可以匹配a/z,a/b/z或a/b/c/z等。 我们再看一个.gitignore文件的例子： # no .a files *.a # but do track lib.a, even though you&apos;re ignoring .a files above !lib.a # only ignore the TODO file in the current directory, not subdir/TODO /TODO # ignore all files in the build/ directory build/ # ignore doc/notes.txt, but not doc/server/arch.txt doc/*.txt # ignore all .pdf files in the doc/ directory doc/**/*.pdf GitHub有一个十分详细的针对数十种项目及语言的.gitignore文件列表，你可以在 https://github.com/github/gitignore 找到它。 查看已暂存和未暂存的修改如果git status命令的输出对于你来说过于模糊，你想知道具体修改了什么地方，可以用git diff命令。 你可能通常会用git diff命令来回答这两个问题：当前做的哪些更新还没有暂存？有哪些更新已经暂存起来准备好了下次提交？尽管git status已经通过在相应栏下列出文件名的方式回答了这个问题，git diff将通过文件补丁的格式显示具体哪些行发生了改变。 假如此时你再次编辑TINY.md后不暂存，运行git status会看到： $ git statusYour branch is up to date with &#39;origin&#x2F;master&#39;.Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) new file: README modified: TINY.mdChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: TINY.md 要查看尚未暂存的文件更新了哪些部分，不加参数直接输入git diff，此命令比较的是工作目录中当前文件和暂存区域快照之间的差异，也就是修改之后还没有暂存起来的变化内容： $ git diffdiff --git a&#x2F;TINY.md b&#x2F;TINY.mdindex 5818529..f0bfc69 100644--- a&#x2F;TINY.md+++ b&#x2F;TINY.md@@ -1,3 +1,3 @@ tiny data modify this file, we add one new line-fix something+add one line for test 若要查看已暂存的将要添加到下次提交里的内容，可以用git diff --cached命令（Git 1.6.1及更高版本还允许使用git diff --staged，效果是相同的，但更好记些），此命令是将目前已暂存的更改和上次提交的内容进行比较： $ git diff --stageddiff --git a&#x2F;README b&#x2F;READMEnew file mode 100644index 0000000..e69de29diff --git a&#x2F;TINY.md b&#x2F;TINY.mdindex f8f5ae1..5818529 100644--- a&#x2F;TINY.md+++ b&#x2F;TINY.md@@ -1 +1,3 @@ tiny data+modify this file, we add one new line+fix something 请注意，git diff本身只显示尚未暂存的改动，而不是自上次提交以来所做的所有改动。所以有时候你一下子暂存了所有更新过的文件后，运行git diff后却什么也没有，就是这个原因。 提交更新当你的暂存区域已经准备妥当便可以提交了。在此之前，请一定要确认还有什么修改过的或新建的文件还没有git add过，否则提交的时候不会记录这些还没暂存起来的变化。这些修改过的文件只保留在本地磁盘。 所以，每次准备提交前，先用git status看下，是不是都已暂存起来了，然后再运行提交命令git commit： $ git commit 这种方式会启动默认的文本编辑器以便输入本次提交的说明，需要注意的是，如果输入的提交说明为空你的本次提交将会被中止。 更常用的方式是在commit命令后添加-m选项，将提交信息与命令放在同一行，如下所示： $ git commit -m &#39;add README file and fix some bugs in TINY.md&#39;[master 10114de] add README file and fix some bugs in TINY.md 2 files changed, 2 insertions(+) create mode 100644 README 现在你已经创建了第一个提交！可以看到，提交后它会告诉你，当前是在哪个分支（master）提交的，本次提交的完整SHA-1校验和是什么（10114de），以及在本次提交中，有多少文件修订过，多少行添加和删改过。 请记住，提交时记录的是放在暂存区域的快照。任何还未暂存的仍然保持已修改状态，可以在下次提交时纳入版本管理。每一次运行提交操作，都是对你项目作一次快照，以后可以回到这个状态，或者进行比较。 跳过使用暂存区域尽管使用暂存区域的方式可以精心准备要提交的细节，但有时候这么做略显繁琐。Git提供了一个跳过使用暂存区域的方式，只要在提交的时候，给git commit加上-a选项，Git就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过git add步骤： $ git statusOn branch masterYour branch is up to date with &#39;origin&#x2F;master&#39;.Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: TINY.mdno changes added to commit (use &quot;git add&quot; and&#x2F;or &quot;git commit -a&quot;)$ git commit -a -m &#39;test the commit -a option&#39;[master 45f06e1] test the commit -a option 1 file changed, 1 insertion(+), 1 deletion(-) 可以看到，提交之前不再需要git add文件TINY.md了。 移除文件要从Git中移除某个文件，就必须要从已跟踪文件清单中移除（确切地说，是从暂存区域移除），然后提交。可以用git rm命令完成此项工作，并连带从工作目录中删除指定的文件，这样以后就不会出现在未跟踪文件清单中了。 如果只是简单地从工作目录中手工删除文件，运行git status时就会在“Changes not staged for commit”部分（也就是未暂存清单）看到： $ rm TINY.md$ git statusOn branch masterYour branch is up to date with &#39;origin&#x2F;master&#39;.Changes not staged for commit: (use &quot;git add&#x2F;rm &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) deleted: TINY.mdno changes added to commit (use &quot;git add&quot; and&#x2F;or &quot;git commit -a&quot;) 然后再运行git rm记录此次移除文件的操作： $ git rm TINY.md$ git statusOn branch masterYour branch is up to date with &#39;origin&#x2F;master&#39;.Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) deleted: TINY.md 下一次提交时，该文件就不再纳入版本管理了。如果删除之前修改过并且已经放到暂存区域的话，则必须要用强制删除选项-f（即force的首字母）。这是一种安全特性，用于防止误删还没有添加到快照的数据，这样的数据不能被Git恢复。 另外一种情况是，我们想把文件从暂存区域移除，但仍然希望保留在当前工作目录中。换句话说，你想让文件保留在磁盘，但是并不想让Git继续跟踪。当你忘记添加.gitignore文件，不小心把一个很大的日志文件或一堆.a这样的编译生成文件添加到暂存区时，这一做法尤其有用。为达到这一目的，使用--cached选项： $ git rm --cached README git rm命令后面可以列出文件或者目录的名字，也可以使用glob模式。例如： $ git rm log&#x2F;\\*.log 注意到星号*之前的反斜杠\\，这是非常有必要的，因为除了拥有shell的文件模式扩展匹配方式，Git还有它自己的文件模式扩展匹配方式。 此命令会删除log/目录及其子目录下扩展名为.log的所有文件。 移动文件不像其它的VCS系统，Git并不显式跟踪文件移动操作。如果在Git中重命名了某个文件，仓库中存储的元数据并不会体现出这是一次改名操作。不过Git非常聪明，它会推断出究竟发生了什么，至于具体是如何做到的，可以放到以后再详细了解。 要在Git中对文件改名，可以这么做： $ git mv file_from file_to 此时查看状态信息，可以明白无误地看到关于重命名操作的说明： $ git mv TINY.md MY-TOY.md$ git statusOn branch masterYour branch is up to date with &#39;origin&#x2F;master&#39;.Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) renamed: TINY.md -&gt; MY-TOY.md 其实，运行git mv就相当于运行了下面三条命令： $ mv TINY.md MY-TOY.md$ git rm TINY.md$ git add MY-TOY.md 如此分开操作，Git也会意识到这是一次改名，所以不管何种方式结果都一样，但是git mv一个命令代替了第二种方式的三个命令，更加的轻便。更进一步，你可以使用任何你熟悉的工具来重命名文件，然后只需要记得在提交前，删除老的文件名并添加新的文件名。 查看提交历史在提交了若干更新，又或者克隆了某个项目之后，你也许想回顾下提交历史。完成这个任务最简单而又有效的工具是git log命令。 首先运行下面的命令获取用于演示的simplegit项目的源代码： $ git clone https:&#x2F;&#x2F;github.com&#x2F;schacon&#x2F;simplegit-progit 然后在此项目中运行git log，应该会看到下面的输出： $ git logcommit ca82a6dff817ec66f44342007202690a93763949 (HEAD -&gt; master, origin&#x2F;master, origin&#x2F;HEAD)Author: Scott Chacon &lt;schacon@gmail.com&gt;Date: Mon Mar 17 21:52:11 2008 -0700 changed the verison numbercommit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7Author: Scott Chacon &lt;schacon@gmail.com&gt;Date: Sat Mar 15 16:40:33 2008 -0700 removed unnecessary test codecommit a11bef06a3f659402fe7563abf99ad00de2209e6Author: Scott Chacon &lt;schacon@gmail.com&gt;Date: Sat Mar 15 10:31:28 2008 -0700 first commit 默认不用任何参数的话，git log会按提交时间列出所有的更新，最近的更新排在最上面。正如你所看到的，这个命令会列出每个提交的SHA-1校验和、作者的名字和电子邮件地址、提交时间以及提交说明。 git log有许多选项可以帮助你搜寻你所要找的提交， 接下来我们先来了解一些最常用的选项。 一个常用的选项是-p或者说是--patch，用来显示每次提交的内容差异（the patch output）。你也可以限制展示的提交条目数，例如可以加上-2来仅显示最近两次提交： $ git log -p -2commit ca82a6dff817ec66f44342007202690a93763949 (HEAD -&gt; master, origin&#x2F;master, origin&#x2F;HEAD)Author: Scott Chacon &lt;schacon@gmail.com&gt;Date: Mon Mar 17 21:52:11 2008 -0700 changed the verison numberdiff --git a&#x2F;Rakefile b&#x2F;Rakefileindex a874b73..8f94139 100644--- a&#x2F;Rakefile+++ b&#x2F;Rakefile@@ -5,7 +5,7 @@ require &#39;rake&#x2F;gempackagetask&#39; spec &#x3D; Gem::Specification.new do |s| s.platform &#x3D; Gem::Platform::RUBY s.name &#x3D; &quot;simplegit&quot;- s.version &#x3D; &quot;0.1.0&quot;+ s.version &#x3D; &quot;0.1.1&quot; s.author &#x3D; &quot;Scott Chacon&quot; s.email &#x3D; &quot;schacon@gmail.com&quot; s.summary &#x3D; &quot;A simple gem for using Git in Ruby code.&quot;commit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7Author: Scott Chacon &lt;schacon@gmail.com&gt;Date: Sat Mar 15 16:40:33 2008 -0700 removed unnecessary test codediff --git a&#x2F;lib&#x2F;simplegit.rb b&#x2F;lib&#x2F;simplegit.rbindex a0a60ae..47c6340 100644--- a&#x2F;lib&#x2F;simplegit.rb+++ b&#x2F;lib&#x2F;simplegit.rb@@ -18,8 +18,3 @@ class SimpleGit end end--if $0 &#x3D;&#x3D; __FILE__- git &#x3D; SimpleGit.new- puts git.show-end\\ No newline at end of file 该选项除了显示基本信息之外，还附带了每次commit的变化。当进行代码审查，或者快速浏览某个搭档提交的commit所带来的变化的时候，这个参数就非常有用了。你也可以为git log附带一系列的总结性选项。比如说，如果你想看到每次提交的简略的统计信息，你可以使用--stat选项： $ git log --statcommit ca82a6dff817ec66f44342007202690a93763949 (HEAD -&gt; master, origin&#x2F;master, origin&#x2F;HEAD)Author: Scott Chacon &lt;schacon@gmail.com&gt;Date: Mon Mar 17 21:52:11 2008 -0700 changed the verison number Rakefile | 2 +- 1 file changed, 1 insertion(+), 1 deletion(-)commit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7Author: Scott Chacon &lt;schacon@gmail.com&gt;Date: Sat Mar 15 16:40:33 2008 -0700 removed unnecessary test code lib&#x2F;simplegit.rb | 5 ----- 1 file changed, 5 deletions(-)commit a11bef06a3f659402fe7563abf99ad00de2209e6Author: Scott Chacon &lt;schacon@gmail.com&gt;Date: Sat Mar 15 10:31:28 2008 -0700 first commit README | 6 ++++++ Rakefile | 23 +++++++++++++++++++++++ lib&#x2F;simplegit.rb | 25 +++++++++++++++++++++++++ 3 files changed, 54 insertions(+) 正如你所看到的，--stat选项在每次提交的下面列出所有被修改过的文件、有多少文件被修改了以及被修改过的文件的哪些行被移除或是添加了。在每次提交的最后还有一个总结。 另外一个常用的选项是--pretty。这个选项可以指定使用不同于默认格式的方式展示提交历史。这个选项有一些内建的子选项供你使用。比如用oneline将每个提交放在一行显示，查看的提交数很大时非常有用。 git log还有许多非常实用的限制输出长度的选项，也就是只输出部分提交信息。例如之前看到的-2其实是-&lt;n&gt;选项的写法，其中的n可以是任何整数，表示仅显示最近的若干条提交；另外还有按照时间作限制的选项，比如--since和--until也很有用，还可以给出若干搜索条件，列出符合的提交… git log的选项非常多，在这里不再一一介绍。总之，git log是一个功能非常齐全、可定制化程度非常高的、能满足你几乎所有查询方式的查看提交历史的命令。 撤消操作在任何一个阶段，你都有可能想要撤消某些操作。这里，我们将会学习几个撤消你所做修改的基本工具。注意，有些撤消操作是不可逆的，这是在使用Git的过程中，会因为操作失误而导致之前的工作丢失的少有的几个地方之一。 有时候我们提交完了才发现漏掉了几个文件没有添加，或者提交信息写错了。此时，可以运行带有--amend选项的提交命令尝试重新提交： $ git commit --amend 这个命令会将暂存区中的文件提交。如果自上次提交以来你还未做任何修改（例如，在上次提交后马上执行了此命令），那么快照会保持不变，而你所修改的只是提交信息。 运行该命令后会启动文本编辑器，可以看到之前的提交信息。编辑后保存会覆盖原来的提交信息。 例如，你提交后发现忘记了暂存某些需要的修改，可以像下面这样操作： $ git commit -m &#39;initial commit&#39;$ git add forgotten_file$ git commit --amend 最终你只会有一个提交——第二次提交将代替第一次提交的结果。 接下来再来学习一下如何操作暂存区域与工作目录中已修改的文件。这些命令在修改文件状态的同时，也会提示如何撤消操作。 取消暂存的文件如果你已经修改了两个文件并且想要将它们作为两次独立的修改提交，但是却意外地输入了git add *暂存了它们两个。如何只取消暂存两个中的一个呢？git status命令提示了你： $ git add *$ git statusOn branch masterYour branch is up to date with &#39;origin&#x2F;master&#39;.Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) modified: README modified: TINY.md 在“Changes to be committed”文字正下方，提示使用git reset HEAD &lt;file&gt;...来取消暂存。所以，我们可以这样来取消暂存TINY.md文件： $ git reset HEAD TINY.mdOn branch masterYour branch is up to date with &#39;origin&#x2F;master&#39;.Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) modified: READMEChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: TINY.md 可以看到TINY.md文件已经是修改未暂存的状态了。 撤消对文件的修改如果你并不想保留对TINY.md文件的修改怎么办？你该如何方便地撤消修改——将它还原成上次提交时的样子（或者刚克隆完的样子，或者刚把它放入工作目录时的样子）？幸运的是，git status也告诉了你应该如何做。在最后一个例子中，未暂存区域是这样： Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: TINY.md 它非常清楚地告诉了你如何撤消之前所做的修改。让我们来按照提示执行： $ git checkout -- TINY.md$ git statusOn branch masterYour branch is up to date with &#39;origin&#x2F;master&#39;.Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) modified: README 可以看到关于TINY.md的修改已经被撤消了。 请记住，在Git中任何已提交的东西几乎总是可以恢复的。甚至那些被删除的分支中的提交或使用--amend选项覆盖的提交也可以恢复。然而，任何你未提交的东西丢失后很可能再也找不到了。 远程仓库的使用为了能在任意Git项目上协作，你需要知道如何管理自己的远程仓库。远程仓库是指托管在因特网或其他网络中的你的项目的版本库。 你可以有好几个远程仓库，通常有些仓库对你只读，有些则可以读写。与他人协作涉及管理远程仓库以及根据需要推送或拉取数据。管理远程仓库包括了解如何添加远程仓库、移除无效的远程仓库、管理不同的远程分支并定义它们是否被跟踪等等。在本章节中，将介绍一部分基础的远程管理技能。 查看远程仓库如果想查看你已经配置的远程仓库服务器，可以运行git remote命令。它会列出你指定的每一个远程服务器的简写。如果你已经克隆了自己的仓库，那么至少应该能看到origin，这是Git给你克隆的仓库服务器的默认名字 $ git clone https:&#x2F;&#x2F;github.com&#x2F;schacon&#x2F;ticgitCloning into &#39;ticgit&#39;...remote: Enumerating objects: 1857, done.remote: Total 1857 (delta 0), reused 0 (delta 0), pack-reused 1857Receiving objects: 100% (1857&#x2F;1857), 334.04 KiB | 36.00 KiB&#x2F;s, done.Resolving deltas: 100% (837&#x2F;837), done.$ cd ticgit$ git remoteorigin 你也可以指定选项-v，会显示需要读写远程仓库使用的Git保存的简写与其对应的URL。 $ git remote -vorigin https:&#x2F;&#x2F;github.com&#x2F;schacon&#x2F;ticgit (fetch)origin https:&#x2F;&#x2F;github.com&#x2F;schacon&#x2F;ticgit (push) 如果你的远程仓库不止一个，该命令会将它们全部列出。例如，与几个协作者合作的，拥有多个远程仓库的仓库看起来像下面这样： $ cd grit$ git remote -vbakkdoor https:&#x2F;&#x2F;github.com&#x2F;bakkdoor&#x2F;grit (fetch)bakkdoor https:&#x2F;&#x2F;github.com&#x2F;bakkdoor&#x2F;grit (push)cho45 https:&#x2F;&#x2F;github.com&#x2F;cho45&#x2F;grit (fetch)cho45 https:&#x2F;&#x2F;github.com&#x2F;cho45&#x2F;grit (push)defunkt https:&#x2F;&#x2F;github.com&#x2F;defunkt&#x2F;grit (fetch)defunkt https:&#x2F;&#x2F;github.com&#x2F;defunkt&#x2F;grit (push)koke git:&#x2F;&#x2F;github.com&#x2F;koke&#x2F;grit.git (fetch)koke git:&#x2F;&#x2F;github.com&#x2F;koke&#x2F;grit.git (push)origin git@github.com:mojombo&#x2F;grit.git (fetch)origin git@github.com:mojombo&#x2F;grit.git (push) 这样我们可以轻松拉取其中任何一个用户的贡献。此外，我们有可能还会有某些远程仓库的推送权限，不过暂时不在此介绍。 添加远程仓库运行git remote add &lt;shortname&gt; &lt;url&gt;添加一个新的远程Git仓库，同时指定一个你可以轻松引用的简写： $ git remoteorigin$ git remote add pb https:&#x2F;&#x2F;github.com&#x2F;paulboone&#x2F;ticgit$ git remote -vorigin https:&#x2F;&#x2F;github.com&#x2F;schacon&#x2F;ticgit (fetch)origin https:&#x2F;&#x2F;github.com&#x2F;schacon&#x2F;ticgit (push)pb https:&#x2F;&#x2F;github.com&#x2F;paulboone&#x2F;ticgit (fetch)pb https:&#x2F;&#x2F;github.com&#x2F;paulboone&#x2F;ticgit (push) 现在你可以在命令行中使用字符串pb来代替整个URL。例如，如果你想拉取Paul的仓库中有但你没有的信息，可以运行git fetch pb： $ git fetch pbremote: Enumerating objects: 22, done.remote: Counting objects: 100% (22&#x2F;22), done.remote: Total 43 (delta 22), reused 22 (delta 22), pack-reused 21Unpacking objects: 100% (43&#x2F;43), done.From https:&#x2F;&#x2F;github.com&#x2F;paulboone&#x2F;ticgit * [new branch] master -&gt; pb&#x2F;master * [new branch] ticgit -&gt; pb&#x2F;ticgit 现在Paul的master分支可以在本地通过pb/master访问到——你可以将它合并到自己的某个分支中，或者如果你想要查看它的话，可以检出一个指向该点的本地分支。 关于什么是分支以及如何使用分支将会在 Git的杀手锏：分支模型（上） 中详细介绍 从远程仓库中抓取与拉取就如刚才所见，从远程仓库中获得数据，可以执行： $ git fetch [remote-name] 这个命令会访问远程仓库，从中拉取所有你还没有的数据。执行完成后，你将会拥有那个远程仓库中所有分支的引用，可以随时合并或查看。 如果你使用clone命令克隆了一个仓库，命令会自动将其添加为远程仓库并默认以“origin”为简写。所以，git fetch origin会抓取克隆（或上一次抓取）后新推送的所有工作。必须注意git fetch命令会将数据拉取到你的本地仓库，但是它并不会自动合并或修改你当前的工作。 当准备好时你必须手动将其合并入你的工作。 如果你有一个分支设置为跟踪一个远程分支，可以使用git pull命令来自动的抓取然后合并远程分支到当前分支。这对你来说可能是一个更简单或更舒服的工作流程；默认情况下，git clone命令会自动设置本地master分支跟踪克隆的远程仓库的master分支（或不管是什么名字的默认分支）。运行git pull通常会从最初克隆的服务器上抓取数据并自动尝试合并到当前所在的分支。 推送到远程仓库当你想分享你的项目时，必须将其推送到上游。这个命令很简单：git push [remote-name] [branch-name]。当你想要将master分支推送到origin服务器时（再次说明，克隆时通常会自动帮你设置好那两个名字），那么运行这个命令就可以将你所做的备份到服务器： $ git push origin master 只有当你有所克隆服务器的写入权限，并且之前没有人推送过时，这条命令才能生效。当你和其他人在同一时间克隆，他们先推送到上游然后你再推送到上游，你的推送就会毫无疑问地被拒绝。你必须先将他们的工作拉取下来并将其合并进你的工作后才能推送。阅读 Git的杀手锏：分支模型（上） 了解如何推送到远程仓库服务器的详细信息。 查看某个远程仓库如果想要查看某一个远程仓库的更多信息，可以使用git remote show [remote-name]命令。如果想以一个特定的缩写名运行这个命令，例如 origin，会得到像下面类似的信息： $ git remote show origin* remote origin Fetch URL: https:&#x2F;&#x2F;github.com&#x2F;schacon&#x2F;ticgit Push URL: https:&#x2F;&#x2F;github.com&#x2F;schacon&#x2F;ticgit HEAD branch: master Remote branches: master tracked ticgit tracked Local branch configured for &#39;git pull&#39;: master merges with remote master Local ref configured for &#39;git push&#39;: master pushes to master (up to date) 它同样会列出远程仓库的URL与跟踪分支的信息。这些信息非常有用，它告诉你正处于master分支，并且如果运行git pull，就会抓取所有的远程引用，然后将远程master分支合并到本地master分支。它也会列出拉取到的所有远程引用。 这是一个经常遇到的简单例子。如果你是Git的重度使用者，那么还可以通过git remote show看到更多的信息： $ git remote show origin* remote origin URL: https:&#x2F;&#x2F;github.com&#x2F;my-org&#x2F;complex-project Fetch URL: https:&#x2F;&#x2F;github.com&#x2F;my-org&#x2F;complex-project Push URL: https:&#x2F;&#x2F;github.com&#x2F;my-org&#x2F;complex-project HEAD branch: master Remote branches: master tracked dev-branch tracked markdown-strip tracked issue-43 new (next fetch will store in remotes&#x2F;origin) issue-45 new (next fetch will store in remotes&#x2F;origin) refs&#x2F;remotes&#x2F;origin&#x2F;issue-11 stale (use &#39;git remote prune&#39; to remove) Local branches configured for &#39;git pull&#39;: dev-branch merges with remote dev-branch master merges with remote master Local refs configured for &#39;git push&#39;: dev-branch pushes to dev-branch (up to date) markdown-strip pushes to markdown-strip (up to date) master pushes to master (up to date) 这个命令列出了当你在特定的分支上执行git push会自动地推送到哪一个远程分支。它也同样地列出了哪些远程分支不在你的本地，哪些远程分支已经从服务器上移除了，还有当你执行git pull时哪些分支会自动合并。 远程仓库的移除与重命名如果想要重命名引用的名字可以运行git remote rename去修改一个远程仓库的简写名。例如，想要将pb重命名为paul，可以用git remote rename这样做： $ git remote rename pb paul$ git remoteoriginpaul 值得注意的是这同样也会修改你的远程分支名字。那些过去引用pb/master的现在会引用paul/master。 如果因为一些原因想要移除一个远程仓库——你已经从服务器上搬走了或不再想使用某一个特定的镜像了，又或者某一个贡献者不再贡献了——可以使用git remote rm： $ git remote rm paul$ git remoteorigin 打标签像其他版本控制系统（VCS）一样，Git可以给历史中的某一个提交打上标签，以示重要。比较有代表性的是人们会使用这个功能来标记发布结点（v1.0 等等）。在本章节中，你将会学习如何列出已有的标签、如何创建新标签、以及不同类型的标签分别是什么。 列出标签在Git中列出已有的标签是非常简单直观的。只需要输入git tag： $ git tagv0.1v1.3 这个命令以字母顺序列出标签；但是它们出现的顺序并不重要。 你也可以使用特定的模式查找标签。例如，Git自身的源代码仓库包含标签的数量超过500个。如果只对1.8.5系列感兴趣，可以运行： $ git tag -l &#39;v1.8.5*&#39;v1.8.5v1.8.5-rc0v1.8.5-rc1v1.8.5-rc2v1.8.5-rc3v1.8.5.1v1.8.5.2v1.8.5.3v1.8.5.4v1.8.5.5 注：如果你只是想要列出全部的标签，-l或者--list选项可以省略；但是如果是希望用特定的模式查找标签，-l或--list选项则是必须的 创建标签Git使用两种主要类型的标签：轻量标签（lightweight）与附注标签（annotated）。 一个轻量标签很像一个不会改变的分支——它只是一个特定提交的引用。 然而，附注标签是存储在Git数据库中的一个完整对象。它们是可以被校验的：其中包含打标签者的名字、电子邮件地址、日期时间，还有一个标签信息，并且可以使用GNU Privacy Guard （GPG）签名与验证。通常建议创建附注标签，这样你可以拥有以上所有信息；但是如果你只是想用一个临时的标签，或者因为某些原因不想要保存那些信息，轻量标签也是可用的。 附注标签在Git中创建一个附注标签是很简单的。最简单的方式是当你在运行tag命令时指定-a选项： $ git tag -a v1.4 -m &quot;my version 1.4&quot;$ git tagv0.1v1.3v1.4 -m选项指定了一条将会存储在标签中的信息。如果没有为附注标签指定一条信息，Git会运行编辑器要求你输入信息。 通过使用git show命令可以看到标签信息与对应的提交信息： $ git show v1.4tag v1.4Tagger: Ben Straub &lt;ben@straub.cc&gt;Date: Sat May 3 20:19:12 2014 -0700my version 1.4commit ca82a6dff817ec66f44342007202690a93763949Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Mon Mar 17 21:52:11 2008 -0700 changed the version number 输出显示了打标签者的信息、打标签的日期时间、附注信息，然后显示具体的提交信息。 轻量标签另一种给提交打标签的方式是使用轻量标签。轻量标签本质上是将提交校验和存储到一个文件中——没有保存任何其他信息。 创建轻量标签，不需要使用-a、-s或-m选项，只需要提供标签名字： $ git tag v1.4-lw$ git tagv0.1v1.3v1.4v1.4-lwv1.5 这时，如果在标签上运行git show，你不会看到额外的标签信息。命令只会显示出提交信息： $ git show v1.4-lwcommit ca82a6dff817ec66f44342007202690a93763949Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Mon Mar 17 21:52:11 2008 -0700 changed the version number 后期打标签你也可以对过去的提交打标签。假设提交历史是这样的： $ git log --pretty&#x3D;oneline15027957951b64cf874c3557a0f3547bd83b3ff6 Merge branch &#39;experiment&#39;a6b4c97498bd301d84096da251c98a07c7723e65 beginning write support0d52aaab4479697da7686c15f77a3d64d9165190 one more thing6d52a271eda8725415634dd79daabbc4d9b6008e Merge branch &#39;experiment&#39;0b7434d86859cc7b8c3d5e1dddfed66ff742fcbc added a commit function4682c3261057305bdd616e23b64b0857d832627b added a todo file166ae0c4d3f420721acbb115cc33848dfcc2121a started write support9fceb02d0ae598e95dc970b74767f19372d61af8 updated rakefile964f16d36dfccde844893cac5b347e7b3d44abbc commit the todo8a5cbc430f1a9c3d00faaeffd07798508422908a updated readme 现在，假设在v1.2时你忘记给项目打标签，也就是在“updated rakefile”提交。你可以在之后补上标签。要在那个提交上打标签，你需要在命令的末尾指定提交的校验和（或部分校验和）： $ git tag -a v1.2 9fceb02 共享标签默认情况下，git push命令并不会传送标签到远程仓库服务器上。在创建完标签后你必须显式地推送标签到共享服务器上。这个过程就像共享远程分支一样——你可以运行git push origin [tagname]。 $ git push origin v1.5Counting objects: 14, done.Delta compression using up to 8 threads.Compressing objects: 100% (12&#x2F;12), done.Writing objects: 100% (14&#x2F;14), 2.05 KiB | 0 bytes&#x2F;s, done.Total 14 (delta 3), reused 0 (delta 0)To git@github.com:schacon&#x2F;simplegit.git * [new tag] v1.5 -&gt; v1.5 如果想要一次性推送很多标签，也可以使用带有--tags选项的git push命令。这将会把所有不在远程仓库服务器上的标签全部传送到那里。 $ git push origin --tagsCounting objects: 1, done.Writing objects: 100% (1&#x2F;1), 160 bytes | 0 bytes&#x2F;s, done.Total 1 (delta 0), reused 0 (delta 0)To git@github.com:schacon&#x2F;simplegit.git * [new tag] v1.4 -&gt; v1.4 * [new tag] v1.4-lw -&gt; v1.4-lw 现在，当其他人从仓库中克隆或拉取，他们也能得到你的那些标签。 删除标签要删除掉你本地仓库上的标签，可以使用命令git tag -d &lt;tagname&gt;。例如，可以使用下面的命令删除掉一个轻量级标签： $ git tag -d v1.4-lwDeleted tag &#39;v1.4-lw&#39; (was e7d5add) 应该注意的是上述命令并不会从任何远程仓库中移除这个标签，有两种方式可以从远程仓库中删除标签。 第一种方式是使用git push &lt;remote&gt; :refs/tags/&lt;tagname&gt;来更新你的远程仓库： $ git push origin :refs&#x2F;tags&#x2F;v1.4-lwTo &#x2F;git@github.com:schacon&#x2F;simplegit.git - [deleted] v1.4-lw 第二种删除远程仓库标签的方式更加直观： $ git push origin --delete &lt;tagname&gt; 检出标签如果你想查看某个标签所指向的文件版本，可以使用git checkout命令。但是这会使你的仓库处于“分离头指针（detacthed HEAD）”状态，这个状态有些不好的副作用：如果你做了某些更改然后提交它们，标签不会发生变化，但你的新提交将不属于任何分支，并且将无法访问，除非确切的提交哈希。 因此，如果你需要进行更改，比如说你正在修复旧版本的错误，这通常需要创建一个新分支来进行操作。 Git别名有一个小技巧可以使你的Git体验更简单、容易、熟悉：别名 Git并不会在你输入部分命令时自动推断出你想要的命令。如果不想每次都输入完整的Git命令，可以通过git config来轻松地为每一个命令设置一个别名。 这里有一些例子你可以试试： $ git config --global alias.co checkout$ git config --global alias.br branch$ git config --global alias.ci commit$ git config --global alias.st status 这意味着，当要输入git commit时，只需要输入git ci。随着你继续不断地使用Git，可能也会经常使用其他命令，此时不要犹豫，为它创建一个别名吧。 在创建你认为应该存在的命令时这个技术也会很有用。例如，为了解决取消暂存文件的易用性问题，可以向Git中添加你自己的取消暂存别名： $ git config --global alias.unstage &#39;reset HEAD --&#39; 这会使下面的两个命令等价： $ git unstage fileA$ git reset HEAD -- fileA 这样看起来更清楚一些。通常也会添加一个last命令，像这样： $ git config --global alias.last &#39;log -1 HEAD&#39; 这样，可以轻松地看到最后一次提交： $ git lastcommit 66938dae3329c7aebe598c2246a8e6af90d04646Author: Josh Goebel &lt;dreamer3@example.com&gt;Date: Tue Aug 26 19:48:51 2008 +0800 test for current head Signed-off-by: Scott Chacon &lt;schacon@example.com&gt; 可以看出，Git只是简单地将别名替换为对应的命令。然而，你可能想要执行外部命令，而不是一个Git子命令。如果是那样的话，可以在命令前面加入!符号。如果你自己要写一些与Git仓库协作的工具的话，那会很有用。例如将git visual定义为gitk的别名： $ git config --global alias.visual &#39;!gitk&#39; 总结本文介绍的Git的内容的确不少，但是这些都是你学习Git务必要掌握的基础。随着你对Git的使用越来越多，你会发现Git的每一个命令都是精简而高效的，他们使得你对文件版本控制变得异常方便快捷。 Just do it! 参考文献 Pro Git Book Git Documents","categories":[],"tags":[{"name":"Git","slug":"Git","permalink":"http://www.yingzq.com/tags/Git/"}]},{"title":"What is Git","slug":"what-is-git","date":"2019-09-18T14:19:30.000Z","updated":"2019-12-01T10:55:46.127Z","comments":true,"path":"2019/09/18/what-is-git/","link":"","permalink":"http://www.yingzq.com/2019/09/18/what-is-git/","excerpt":"相信写过代码的程序猿们都听说过Git的大名，哪怕是刚刚入门的小白，也在GitHub等代码托管网站git clone过他人的代码。那么什么是Git呢？本文将为你讲述Git的前世往生，看完本文，你应该会对版本控制、Git的诞生、Git基本概念和Git的基本特性有了一个初步的认识。","text":"相信写过代码的程序猿们都听说过Git的大名，哪怕是刚刚入门的小白，也在GitHub等代码托管网站git clone过他人的代码。那么什么是Git呢？本文将为你讲述Git的前世往生，看完本文，你应该会对版本控制、Git的诞生、Git基本概念和Git的基本特性有了一个初步的认识。 注：本文只包含了Git的相关概念，不包含任何命令行代码。 关于版本控制首先来了解一下什么是“版本控制”：版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统。一般情况下是针对程序的源代码文件进行版本控制，但实际上你可以对任何类型的文件进行版本控制。 本地版本控制系统许多人习惯用复制整个项目目录的方式来保存不同的版本，或许还会改名加上备份时间以示区别。这么做唯一的好处就是简单。不过坏处也不少：有时候会混淆所在的工作目录，一旦弄错文件丢了数据就没法撤销恢复。 为了解决这个问题，人们很久以前就开发了许多种本地版本控制系统，大多都是采用某种简单的数据库来记录文件的历次更新差异（见图1）。 图1：本地版本控制系统 其中最流行的一种叫做RCS（Revision Control System），现今许多计算机系统上都还看得到它的踪影。甚至在流行的Mac OS X系统上安装了开发者工具包之后，也可以使用rcs命令。它的工作原理是在硬盘上保存补丁集（补丁是指文件修订前后的变化）；通过应用所有的补丁，便可以重新计算出各个版本的文件内容。 集中化的版本控制系统接下来人们又遇到一个问题，如何让在不同系统上的开发者协同工作呢？于是，集中化的版本控制系统（Centralized Version Control Systems，简称CVCS）应运而生。 这类系统，诸如CVS、Subversion以及Perforce等，都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。多年以来，这已成为版本控制系统的标准做法（见图2）。 图2：集中化的版本控制系统 这种做法带来了许多好处，特别是相较于老式的本地VCS来说。现在，每个人都可以在一定程度上看到项目中的其他人正在做些什么。而管理员也可以轻松掌控每个开发者的权限，并且管理一个CVCS要远比在各个客户端上维护本地数据库来得轻松容易。 事分两面，有好有坏。这么做最显而易见的缺点是中央服务器的单点故障。如果宕机一小时，那么在这一小时内，谁都无法提交更新，也就无法协同工作。如果中心数据库所在的磁盘发生损坏，又没有做恰当备份，毫无疑问你将丢失所有数据——包括项目的整个变更历史，只剩下人们在各自机器上保留的单独快照。本地版本控制系统也存在类似问题，只要整个项目的历史记录被保存在单一位置，就有丢失所有历史更新记录的风险。 分布式版本控制系统为了解决上述版本控制系统存在的问题，分布式版本控制系统（Distributed Version Control System，简称DVCS）诞生了。 在这类系统中，像Git、Mercurial、Bazaar以及Darcs等，客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来。这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。因为每一次的克隆操作，实际上都是一次对代码仓库的完整备份（见图3）。 图3：分布式版本控制系统 更进一步，许多这类系统都可以指定和若干不同的远端代码仓库进行交互。籍此，你就可以在同一个项目中，分别和不同工作小组的人相互协作。你可以根据需要设定不同的协作流程，比如层次模型式的工作流，而这在以前的集中式系统中是无法实现的。 小结：可以看出分布式版本控制系统相比于本地版本控制和集中化的版本控制有着非常明显的优点。Git就是一个典型的分布式版本控制系统（DVCS）。 Git的诞生同生活中的许多伟大事物一样，Git诞生于一个极富纷争大举创新的年代。 1991年，22岁芬兰程序员Linus Torvalds（后文简称Linus）开源了Linux，从此，Linux系统不断发展，已经成为最大的服务器系统软件了。 图4：Linus Torvalds Linus虽然创建了Linux，但Linux的壮大是靠全世界热心的志愿者参与的，这么多人在世界各地为Linux编写代码，那Linux的代码是如何管理的呢？事实是，在2002年以前，世界各地的志愿者把源代码文件通过diff的方式发给Linus，然后由Linus本人通过手工方式合并代码！ 你也许会想，为什么Linus不把Linux代码放到版本控制系统里呢？不是有CVS、SVN这些免费的版本控制系统吗？因为Linus坚定地反对CVS和SVN，这些集中式的版本控制系统不但速度慢，而且必须联网才能使用。有一些商用的版本控制系统，虽然比CVS、SVN好用，但那是付费的，和Linux的开源精神不符。 不过，到了2002年，Linux系统已经发展了十年了，代码库之大让Linus很难继续通过手工方式管理了，社区的弟兄们也对这种方式表达了强烈不满，于是Linus选择了一个商业的版本控制系统BitKeeper，BitKeeper的东家BitMover公司出于人道主义精神，授权Linux社区免费使用这个版本控制系统。 安定团结的大好局面在2005年就被打破了，原因是Linux社区牛人聚集，不免沾染了一些梁山好汉的江湖习气。开发Samba的Andrew试图破解BitKeeper的协议（这么干的其实也不只他一个），被BitMover公司发现了（监控工作做得不错！），于是BitMover公司怒了，要收回Linux社区的免费使用权。 Linus可以向BitMover公司道个歉，保证以后严格管教弟兄们，嗯，这是不可能的。实际情况是这样的：Linus花了两周时间自己用C写了一个分布式版本控制系统，这就是Git！一个月之内，Linux系统的源码已经由Git管理了！牛是怎么定义的呢？大家可以体会一下。 Git迅速成为最流行的分布式版本控制系统，尤其是2008年，GitHub网站上线了，它为开源项目免费提供Git存储，无数开源项目开始迁移至GitHub，包括jQuery，PHP，Ruby等等。 历史就是这么偶然，如果不是当年BitMover公司威胁Linux社区，可能现在我们就没有免费而超级好用的Git了。 小节：Linux的诞生是因为当时还是大学生的Linus觉得好玩编写并开源的；而Git的诞生则是因为Linux社区刚好需要，所以Linus花了两周简单写了一下并开源的，可能这就是大佬吧… Git的基本特性我们已经知道Git是一种分布式版本控制系统，那么Git与其他的版本控制系统的差异是什么？自身的的特性又是什么呢？ Git直接记录快照，而非差异比较Git和其它版本控制系统（包括Subversion和近似工具）的主要差别在于Git对待数据的方法。概念上来区分，其它大部分系统以文件变更列表的方式存储信息。这类系统（CVS、Subversion、Perforce、Bazaar等等）将它们保存的信息看作是一组基本文件和每个文件随时间逐步累积的差异（见图5）。 图5：存储每个文件与初始版本的差异 Git不按照以上方式对待或保存数据。反之，Git更像是把数据看作是对小型文件系统的一组快照（snapshot）。每次你提交更新，或在Git中保存项目状态时，它主要对当时的全部文件制作一个快照并保存这个快照的索引。为了高效，如果文件没有修改，Git不再重新存储该文件，而是只保留一个链接指向之前存储的文件（见图6）。Git对待数据更像是一个快照流。 图6：存储项目随时间改变的快照 这是Git与几乎所有其它版本控制系统的重要区别。Git更像是一个小型的文件系统，提供了许多以此为基础构建的超强工具，而不只是一个简单的VCS。如果你刚刚接触VCS和Git，可能对这句话感触并不深，但是随着你对Git的了解越来越深入，例如在研究Git分支的管理时，会发现这种方式对待数据所能获得的巨大益处。 Git近乎所有操作都是本地执行在Git中的绝大多数操作都只需要访问本地文件和资源，一般不需要来自网络上其它计算机的信息。如果你习惯于所有操作都有网络延时开销的集中式版本控制系统，Git在这方面会让你感到速度之神赐给了Git超凡的能量。因为你在本地磁盘上就有项目的完整历史，所以大部分操作看起来瞬间完成。 举个例子，要浏览项目的历史，Git不需外连到服务器去获取历史，然后再显示出来——它只需直接从本地数据库中读取。你能立即看到项目历史。如果你想查看当前版本与一个月前的版本之间引入的修改，Git会查找到一个月前的文件做一次本地的差异计算，而不是由远程服务器处理或从远程服务器拉回旧版本文件再来本地处理。 这也意味着你离线或者没有VPN时，几乎可以进行任何操作。如你在飞机或火车上想做些工作，你能愉快地提交，直到有网络连接时再上传。如你回家后VPN客户端不正常，你仍能工作。使用其它系统，做到如此是不可能或很费力的。比如，用Perforce，你没有连接服务器时几乎不能做什么事；用Subversion和CVS，你能修改文件，但不能向数据库提交修改（因为你的本地数据库离线了）。这看起来不是大问题，但是你可能会惊喜地发现它带来的巨大的不同。 Git保证完整性Git中所有数据在存储前都计算校验和，然后以校验和来引用。这意味着不可能在Git不知情时更改任何文件内容或目录内容。这个功能建构在Git底层，是构成Git哲学不可或缺的部分。若你在传送过程中丢失信息或损坏文件，Git就能发现。 Git用以计算校验和的机制叫做SHA-1散列。这是一个由40个十六进制字符（0-9和a-f）组成的字符串，基于Git中文件的内容或目录结构计算出来。SHA-1哈希看起来是这样： 24b9da6552252987aa493b52f8696cd6d3b00373Git一般只添加数据你执行的Git操作，几乎只往Git数据库中增加数据。很难让Git执行任何不可逆操作，或者让它以任何方式清除数据。同别的VCS一样，未提交更新时有可能丢失或弄乱修改的内容；但是一旦你提交快照到Git中，就难以再丢失数据，特别是如果你定期的推送数据库到其它仓库的话。 Git文件有三种状态请注意，如果你希望后面的学习过程更加顺利，这里将是关于Git你最需要记住的一点——对于任何一个文件，在Git内都只有三种状态：已修改（modified），已暂存（staged）和已提交（committed）。 已修改表示修改了文件，但还没保存到数据库中 已暂存表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中 已提交表示数据已经安全的保存在本地数据库中 由此引入Git项目的三个工作区域的概念：工作目录、暂存区域以及Git仓库。 图7：工作目录、暂存区域以及Git仓库 工作目录是对项目的某个版本独立提取出来的内容。这些从Git仓库的压缩数据库中提取出来的文件，放在磁盘上供你使用或修改。 暂存区域是一个文件，保存了下次将提交的文件列表信息，一般在Git仓库目录中。有时候也被称作“索引”，不过一般说法还是叫暂存区域。 Git仓库目录是Git用来保存项目的元数据和对象数据库的地方。这是Git中最重要的部分，从其它计算机克隆仓库时，拷贝的就是这里的数据。 基本的Git工作流程如下： 在工作目录中修改文件 暂存文件，将文件的快照放入暂存区域 提交更新，找到暂存区域的文件，将快照永久性存储到Git仓库目录 如果Git目录中保存着特定版本的文件，就属于已提交状态。如果作了修改并已放入暂存区域，就属于已暂存状态。如果自上次取出后，作了修改但还没有放到暂存区域，就是已修改状态。 总结你应该已经对Git是什么、Git与你可能正在使用的集中式版本控制系统有何区别等问题有了基本的了解。对于Git的特性，本文也只是简单提及，它的分支管理、代码合并等功能更让人欲罢不能。如今Git已经成为了最流行的开源分布式版本控制系统，它值得你去深入探索研究! 参考文献 Pro Git Book 廖雪峰 Git教程","categories":[],"tags":[{"name":"Git","slug":"Git","permalink":"http://www.yingzq.com/tags/Git/"}]}]}