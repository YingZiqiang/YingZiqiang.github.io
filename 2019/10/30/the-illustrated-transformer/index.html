<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>Transformer图解 | yingzq&#39;s Blog | 练习bug时长两年半的实习生</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="NLP,Deep Learning,Transformer,BERT">
    <meta name="description" content="Transformer模型来自论文 Attention Is All You Need 。这个模型最初是为了提高机器翻译的效率，它的Self-Attention机制和Position Encoding可以替代RNN。因为RNN是顺序执行的，t时刻没有完成就不能处理t+1时刻，因此很难并行。但是后来发现Self-Attention效果很好，在很多其它的地方也可以使用Transformer模型。这包括">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer图解">
<meta property="og:url" content="http://www.yingzq.com/2019/10/30/the-illustrated-transformer/index.html">
<meta property="og:site_name" content="yingzq&#39;s Blog">
<meta property="og:description" content="Transformer模型来自论文 Attention Is All You Need 。这个模型最初是为了提高机器翻译的效率，它的Self-Attention机制和Position Encoding可以替代RNN。因为RNN是顺序执行的，t时刻没有完成就不能处理t+1时刻，因此很难并行。但是后来发现Self-Attention效果很好，在很多其它的地方也可以使用Transformer模型。这包括">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://image.yingzq.com/img/20191103151039.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191103151740.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191103153855.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191103154622.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191103155935.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191103161116.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191103163213.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191103235047.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191104092221.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191104100653.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191104101640.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191104102956.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191104104331.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191104112848.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191104113256.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191104113551.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191104114652.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191104114937.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191104115314.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191104115506.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191104115858.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191104195401.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191104201012.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191104203857.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191104204732.png">
<meta property="og:image" content="http://image.yingzq.com/img/20191104210604.png">
<meta property="article:published_time" content="2019-10-29T16:23:45.000Z">
<meta property="article:modified_time" content="2020-02-19T17:43:21.206Z">
<meta property="article:author" content="应子强">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="BERT">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://image.yingzq.com/img/20191103151039.png">
    
        <link rel="alternate" type="application/atom+xml" title="yingzq&#39;s Blog" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/avatar.jpg">
    <link rel="stylesheet" href="/css/style.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">应子强</h5>
          <a href="mailto:yingzq0116@163.com" title="yingzq0116@163.com" class="mail">yingzq0116@163.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                标签云
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                分类
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-info-circle"></i>
                关于
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/YingZiqiang" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">Transformer图解</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">Transformer图解</h1>
        <h5 class="subtitle">
            
                <time datetime="2019-10-29T16:23:45.000Z" itemprop="datePublished" class="page-time">
  2019-10-30
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/NLP/">NLP</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#模型概述"><span class="post-toc-number">1.</span> <span class="post-toc-text">模型概述</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#画出Tensor进行分析"><span class="post-toc-number">2.</span> <span class="post-toc-text">画出Tensor进行分析</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#开始编码"><span class="post-toc-number">3.</span> <span class="post-toc-text">开始编码</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Self-Attention概述"><span class="post-toc-number">4.</span> <span class="post-toc-text">Self-Attention概述</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Self-Attention详细介绍"><span class="post-toc-number">5.</span> <span class="post-toc-text">Self-Attention详细介绍</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Self-Attention的矩阵计算"><span class="post-toc-number">6.</span> <span class="post-toc-text">Self-Attention的矩阵计算</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Multi-Head-Attention"><span class="post-toc-number">7.</span> <span class="post-toc-text">Multi-Head Attention</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#位置编码-Positional-Encoding"><span class="post-toc-number">8.</span> <span class="post-toc-text">位置编码(Positional Encoding)</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#残差连接"><span class="post-toc-number">9.</span> <span class="post-toc-text">残差连接</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#省略部分"><span class="post-toc-number">10.</span> <span class="post-toc-text">省略部分</span></a></li></ol>
        </nav>
    </aside>


<article id="post-the-illustrated-transformer"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">Transformer图解</h1>
        <div class="post-meta">
            <time class="post-time" title="2019-10-30 00:23:45" datetime="2019-10-29T16:23:45.000Z"  itemprop="datePublished">2019-10-30</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/NLP/">NLP</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>Transformer模型来自论文 <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a> 。这个模型最初是为了提高机器翻译的效率，它的Self-Attention机制和Position Encoding可以替代RNN。因为RNN是顺序执行的，t时刻没有完成就不能处理t+1时刻，因此很难并行。但是后来发现Self-Attention效果很好，在很多其它的地方也可以使用Transformer模型。这包括著名的GPT和BERT模型，都是以Transformer为基础的。</p>
<p>本文我们将通过图解的方式来直观的理解Transformer模型的基本原理，内容主要参考了文章 <a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">The Illustrated Transformer</a> 。</p>
<a id="more"></a>

<h2 id="模型概述"><a href="#模型概述" class="headerlink" title="模型概述"></a>模型概述</h2><p>我们首先把模型看成一个黑盒子，如下图所示，对于机器翻译来说，它的输入是源语言(法语)的句子，输出是目标语言(英语)的句子。</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191103151039.png" width="700" alt="Transformer的输入和输出" />

<p>图：Transformer的输入和输出</p>
</div>

<p>把黑盒子稍微打开一点，我们可以看到Transformer(或者任何的NMT系统)由2个部分组成：Encoders和Decoders，如下图所示：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191103151740.png" width="600" alt="典型的Encoder-Decoder结构" />

<p>图：典型的Encoder-Decoder结构</p>
</div>

<blockquote>
<p>注：这里Encoder/Decoder使用复数形式是为了强调编码/解码阶段可能由多个组件堆叠而成，下文会讲到</p>
</blockquote>
<p>其中Encoders是由多个(论文中是6个)Encoder堆叠而成，Decoders也同样是由多个(论文中是6个)Decoder堆叠而成，如下图所示：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191103153855.png" width="600" alt="Stacked Encoder and Decoder" />

<p>图：Stacked Encoder and Decoder</p>
</div>

<p>对于Encoders中的每一个Encoder，它们结构都是相同的，<strong>但是并不会共享权值</strong>。每个Encoder由两个子网络层组成，分别是一个Self-Attention层和一个Feed-Forward层(全连接层)，如下图所示：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191103154622.png" width="600" alt="Transformer的一个Encoder层" />

<p>图：Transformer的一个Encoder层</p>
</div>

<p>对于Decoders中的每一个Decoder，它们的结构也都是相同的并且权值不共享。<strong>相比于Encoder，每一个Decoder除了Self-Attention层和全连接层之外还多了一个普通的Attention层，这个Attention层使得Decoder在解码时会考虑最后一层Encoder所有时刻的输出。</strong> 它的结构如下图所示：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191103155935.png" width="700" alt="Transformer的一个Decoder层" />

<p>图：Transformer的一个Decoder层</p>
</div>

<h2 id="画出Tensor进行分析"><a href="#画出Tensor进行分析" class="headerlink" title="画出Tensor进行分析"></a>画出Tensor进行分析</h2><p>现在我们已经了解了模型的主要组件，让我们开始研究对应的Tensor如何在这些组件之间流动。</p>
<p>由于输入的句子是一个词的序列，利用NLP的常规做法，我们先通过Embedding把它变成一个连续稠密的向量，如下图所示：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191103161116.png" width="600" alt="Emebdding层" />

<p>图：Emebdding层</p>
</div>

<blockquote>
<p>注：每一个词都编码成了512维度的向量，为了便于展示我们将用上面这些简单的小方块来表示这些向量</p>
</blockquote>
<p>虽然Emebdding层只发生在最底层的Encoder中，但是对于所有的Encoder层，你都可以按照<strong>同一种</strong>思维模式来理解，那就是<strong>它们都接收一个512维度的向量列表作为输入，只不过最底层的Encoder接收的是单词嵌入，其它Encoder则接收的是前一层Encoder的输出</strong>。</p>
<p>Embedding之后的序列会输入到最底层的Encoder，首先经过Self-Attention层然后再经过全连接层，如下图所示：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191103163213.png" width="600" alt="Transformer Encoder层" />

<p>图：Transformer Encoder层</p>
</div>

<h2 id="开始编码"><a href="#开始编码" class="headerlink" title="开始编码"></a>开始编码</h2><p>接下来，我们将以一个较短的句子为例，看看在编码器中每个子层中具体都发生了什么，首先最底层的Encoder结构如下图所示：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191103235047.png" width="600" alt="Transformer Encoder层" />

<p>图：Transformer Encoder层</p>
</div>

<p>可以看出这张图与上一张图内容基本相同，但是其实本张图展示了更多的细节：图中Self-Attention层是<strong>一个大的方框</strong>，表示它的输入是所有的$x_1,x_2,\ldots,x_n$，输出是$z_1,z_2,\ldots,z_n$，也就是说在计算$z_i$时需要依赖所有时刻的输入$x_1,x_2,\ldots,x_n$，不过我们可以用矩阵运算一下子把所有的$z_i$计算出来(后面介绍)；而全连接层是<strong>每个时刻是一个方框</strong>(但不同时刻的参数是共享的)，表示计算$r_i$只需要$z_i$，因此全连接网络的计算是完全是独立的，很容易并行计算；此外，前一层的输出$r_1,r_2,\ldots,r_n$直接输入到下一层。</p>
<h2 id="Self-Attention概述"><a href="#Self-Attention概述" class="headerlink" title="Self-Attention概述"></a>Self-Attention概述</h2><p>再把黑盒子打开一点，我们来了解一下Tensor如何在Self-Attention层中流动。</p>
<p>Self-Attention是Trandformer中最重要的组件，我们先来提炼一下它是如何工作的。假如我们想要翻译下面的句子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;The animal didn&#39;t cross the street because it was too tired&quot;</span><br></pre></td></tr></table></figure>

<p>句子的意思大概是“这个动物无法穿越马路，因为它太累了”，这里的it到底指代什么呢，是animal还是street？要知道具体的指代，我们需要在理解it的时候同时关注所有的单词，重点是animal、street和tired，然后根据知识(常识)我们知道只有animal才能tired，而street是不能tired的。<strong>Self-Attention用Encoder在编码一个词的时候会考虑句子中所有其它的词，从而确定怎么编码当前词。</strong> 但是如果把tired换成narrow，那么it就指代的是street了。</p>
<p><strong>而LSTM(即使是双向的)是无法实现上面的逻辑的。</strong> 为什么呢？比如前向的LSTM，我们在编码it的时候根本没有看到后面是tired还是narrow，所有它无法把it编码成哪个词。而后向的LSTM呢？当然它看到了tired，但是到it的时候它还没有看到animal和street这两个单词，当然就更无法编码it的内容了。</p>
<p>当然多层的LSTM理论上是可以编码这个语义的，它需要下层的LSTM同时编码了animal和street以及tired三个词的语义，然后由更高层的LSTM来把it编码成animal的语义。但是这样模型更加复杂。</p>
<p>下图是模型的最上一层(下标0是第一层，5是第六层)Encoder的Attention可视化图，这是 <a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb" target="_blank" rel="noopener">Tensor2Tensor notebook</a> 输出的内容：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191104092221.png" width="600" alt="Self-Attention的可视化" />

<p>图：Self-Attention的可视化</p>
</div>

<p>我们可以看到，在编码it这个单词的时候，Self-Attention让模型更多地关注到“The animal”，因此编码后的it有了Animal的语义。</p>
<h2 id="Self-Attention详细介绍"><a href="#Self-Attention详细介绍" class="headerlink" title="Self-Attention详细介绍"></a>Self-Attention详细介绍</h2><p>下面我们详细的介绍Self-Attention是怎么计算的，首先介绍向量的形式逐个时刻计算，这便于理解，接下来我们把它写出矩阵的形式一次计算所有时刻的结果。</p>
<p>对于输入的每一个向量(第一层是词的Embedding，其它层是前一层的输出)，我们首先需要生成3个新的向量Q、K和V，分别代表Query向量、Key向量和Value向量。<strong>Query向量表示为了编码当前词，需要去注意(attend to)其它(其实也包括它自己)的词；而Key向量可以认为是这个词用于被检索的关键信息；Value向量则是真正的内容。</strong></p>
<p>我们对比一下普通的Attention(Luong 2015)，使用内积计算energy的情况。如下图所示，在这里，Key和Value向量都是它本身，而Query向量是当前隐状态$h_t$。计算energy $e_{tj}$的时候我们计算Q($h_t$)和K($\overline{h}_j$)的内积，然后用softmax进行归一化，最后把所有的$\overline{h}_j$加权平均得到context向量。</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191104100653.png" width="600" alt="普通的Attention机制" />

<p>图：普通的Attention机制</p>
</div>

<p><strong>而Self-Attention里的Query不是隐状态，并且来自当前输入向量本身，因此叫作Self-Attention。</strong> 另外Key和Value都不是输入向量，而是输入向量做了一下线性变换，这样做的好处是模型可以根据数据从输入向量中提取最适合作为Key和Value的部分。类似的，Query也是对输入向量做一下线性变换，它让系统可以根据任务学习出最适合的Query，从而可以注意到特定的内容。</p>
<p>K、V和Q的具体的计算过程如下如所示：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191104101640.png" width="600" alt="K、V和Q的计算过程" />

<p>图：K、V和Q的计算过程</p>
</div>

<p>图中输入的是两个词“thinking”和“machines”，我们对它们进行Embedding(这是第一层，如果是后面的层，直接输入就是向量了)，得到向量$x_1$,$x_2$。接着我们用3个矩阵分别对它们进行变换，得到向量$q_1,k_1,v_1$和$q_2,k_2,v_2$。比如$q_1=x_1W^Q$，图中$x_1$的shape是1x4，$W^Q$是4x3，得到的$q_1$便是1x3。其它的计算也是类似的。</p>
<p>值得注意的是，为了能够使得Key和Query可以内积，我们要求$W^K$和$W^Q$的shape是一样的，但是并不要求$W^V$和它们一定一样(虽然实际论文实现是一样的)。</p>
<p>每个时刻t都计算出$q_t,k_t,v_t$后，我们就可以来计算Self-Attention了。以第一个时刻为例，我们首先计算$q_1$和$k_1,k_2$的内积，得到score，过程如下图所示：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191104102956.png" width="600" alt="Self-Attention的向量计算步骤一" />

<p>图：Self-Attention的向量计算步骤一</p>
</div>

<p>接下来使用softmax把score归一化，注意这里把score除以$\sqrt{d_k}$($d_k$表示Key向量的维度)之后再计算的softmax，根据论文的说法，这样计算梯度时会更加稳定(stable)。计算过程如下图所示：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191104104331.png" width="600" alt="Self-Attention的向量计算步骤二" />

<p>图：Self-Attention的向量计算步骤二</p>
</div>

<blockquote>
<p>简单阐述下为什么这里要除以$\sqrt{d_k}$。假设q、k向量中的每一个元素是独立的随机变量并且它们的均值是0方差是1，此时将q、k向量做点乘运算$q \cdot k=\sum_{i=1}^{d_k} q_i k_i$，点乘的结果是一个均值为0，方差为$d_k$的随机变量。</p>
</blockquote>
<p>接下来用softmax得到的归一化分数对所有时刻的V求加权平均，这样就可以认为得到的向量在Self-Attention的帮助下综合考虑了所有时刻的输入信息，计算过程如下图所示：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191104112848.png" width="600" alt="Self-Attention的向量计算步骤三" />

<p>图：Self-Attention的向量计算步骤三</p>
</div>

<p>这里只是演示了计算第一个时刻的过程，计算其它时刻的过程是完全一样的。</p>
<h2 id="Self-Attention的矩阵计算"><a href="#Self-Attention的矩阵计算" class="headerlink" title="Self-Attention的矩阵计算"></a>Self-Attention的矩阵计算</h2><p>前面介绍的方法需要一个循环遍历所有的时刻t计算得到$z_t$，我们可以把上面的向量计算变成矩阵的形式，从而一次计算出所有时刻的输出，这样的矩阵运算可以充分利用硬件资源(包括一些软件的优化)，从而效率更高。</p>
<p>首先还是计算Q、K和V，不过不是计算某个时刻的$q_t,k_t,v_t$了，而是一次计算所有时刻的Q、K和V。计算过程如下图所示。这里的输入是一个矩阵，矩阵的第i行表示第i个时刻的输入$x_i$。</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191104113256.png" width="400" alt="Self-Attention的矩阵计算步骤一" />

<p>图：Self-Attention的矩阵计算步骤一</p>
</div>

<p>接下来就是计算Q和K得到score，然后除以$\sqrt{d_k}$，然后再softmax，最后加权平均得到输出。全过程如下图所示：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191104113551.png" width="600" alt="Self-Attention的矩阵计算步骤二" />

<p>图：Self-Attention的矩阵计算步骤二</p>
</div>

<h2 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h2><p>这篇论文还提出了Multi-Head Attention的概念。其实很简单，前面定义的一组Q、K和V可以让一个词attend to相关的词，我们可以定义多组Q、K和V，它们分别可以关注不同的上下文。计算Q、K和V的过程还是一样，这不过现在变换矩阵从一组$(W_Q,W_K,W_V)$变成了多组$(W_0^Q,W_0^K,W_0^V)$ ，$(W_1^Q,W_1^K,W_1^V)$，…。如下图所示：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191104114652.png" width="600" alt="Multi-Head计算多组Q、K和V" />

<p>图：Multi-Head计算多组Q、K和V</p>
</div>

<p>论文中使用了8组不同的Q、K和V。对于输入矩阵X，每一组Q、K和V都可以得到一个输出矩阵Z，如下图所示：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191104114937.png" width="600" alt="Multi-Head计算输出多个Z" />

<p>图：Multi-Head计算输出多个Z</p>
</div>

<p>但是后面的全连接网络需要的输入是一个矩阵而不是多个矩阵，因此我们可以把每个head输出的$Z_i$拼接起来，然后再经过一个线性变换(矩阵$W^O$)得到最终的输出矩阵Z。这个过程如下图所示：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191104115314.png" width="600" alt="Multi-Head生成最终的矩阵Z" />

<p>图：Multi-Head生成最终的矩阵Z</p>
</div>

<p>上面的步骤涉及很多步骤和矩阵运算，我们用一张大图把整个过程表示出来，如下图所示：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191104115506.png" width="700" alt="Multi-Head计算完整过程" />

<p>图：Multi-Head计算完整过程</p>
</div>

<p>让我们回顾一下之前的例子，当我们在例句“The animal didn’t cross the street because it was too tired”中编码单词“it”时，不同的head把焦点放到了哪里：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191104115858.png" width="400" alt="Multi-Head Attention的焦点" />

<p>图：Multi-Head Attention的焦点</p>
</div>

<p>可以看到，有的head(橘黄色部分)计算的结果认为其与“the animal”关系比较密切，而另一个head(绿色部分)则认为和“tired”关系更近。换句话说，使用Multi-Head Attention对“it”进行编码时，可以同时注意到“animal”和“tired”，这使得对“it”的编码更加全面而准确。</p>
<h2 id="位置编码-Positional-Encoding"><a href="#位置编码-Positional-Encoding" class="headerlink" title="位置编码(Positional Encoding)"></a>位置编码(Positional Encoding)</h2><blockquote>
<p>注意：这是Transformer原始论文使用的位置编码方法，而在BERT模型里，使用的是简单的可以学习的Embedding，和Word Embedding一样，只不过输入是位置而不是词而已。</p>
</blockquote>
<p>我们的目的是用Self-Attention替代RNN，RNN能够记住过去的信息，这可以通过Self-Attention“实时”的注意相关的任何词来实现等价(甚至更好)的效果。RNN还有一个特点就是能考虑词的顺序(位置)关系，一个句子即使词完全是相同的但是语义可能完全不同，比如“北京到上海的机票”与“上海到北京的机票”，它们的语义就有很大的差别。我们上面的介绍的Self-Attention是不考虑词的顺序的，如果模型参数固定了，上面两个句子的北京都会被编码成相同的向量。但是实际上我们可以期望这两个北京编码的结果不同，前者可能需要编码出发城市的语义，而后者需要包含目的城市的语义。而RNN是可以(至少是可能)学到这一点的。当然RNN为了实现这一点的代价就是顺序处理，很难并行。</p>
<p>为了解决这个问题，我们需要引入位置编码，也就是t时刻的输入，除了Embedding之外(这是与位置无关的)，我们还引入一个向量，这个向量是与t有关的，我们把Embedding和位置编码向量加起来作为模型的输入。这样的话如果两个词在不同的位置出现了，虽然它们的Embedding是相同的，但是由于位置编码不同，最终得到的向量也是不同的。</p>
<p>位置编码有很多方法，其中需要考虑的一个重要因素就是需要它编码的是相对位置的关系。比如两个句子：“北京到上海的机票”和“你好，我们要一张北京到上海的机票”。显然加入位置编码之后，两个北京的向量是不同的了，两个上海的向量也是不同的了，但是我们期望$Q_{北京1} K_{上海1}=Q_{北京2} K_{上海2}$。具体的位置编码算法不是这里关注的重点，我们以后再介绍。位置编码加入后的模型如下图所示：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191104195401.png" width="700" alt="位置编码" />

<p>图：位置编码</p>
</div>

<p>举一个简单的例子，假设Embedding的维度是4，那么实际的位置编码应该是这样的：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191104201012.png" width="600" alt="位置编码例子" />

<p>图：位置编码例子</p>
</div>

<h2 id="残差连接"><a href="#残差连接" class="headerlink" title="残差连接"></a>残差连接</h2><p>在继续之前，我们需要说说编码器架构中的一个细节：每个Encoder中的每个子层(Self-Attention层和全连接层)在其周围都有一个残差连接，然后紧接着是一个 <a href="https://arxiv.org/abs/1607.06450" target="_blank" rel="noopener">Layer Normalization</a> 层。如下图所示：</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191104203857.png" width="600" alt="残差和Layer Normalization" />

<p>图：残差和Layer Normalization</p>
</div>

<p>下图则展示了更多细节：输入$x_1,x_2$经Self-Attention层之后变成$z_1,z_2$，然后和残差连接的输入$x_1,x_2$加起来，然后经过Layer Normalization层输出给全连接层；全连接层正如上文提到的也是有一个残差连接和一个Layer Normalization层，最后再输出给上一层。</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191104204732.png" width="600" alt="残差和Layer Normalization细节" />

<p>图：残差和Layer Normalization细节</p>
</div>

<p><strong>Decoder和Encoder是类似的，区别在于它多了一个Encoder-Decoder Attention层，这个层的输入除了来自Self-Attention之外还有Encoder最后一层的所有时刻的输出。Encoder-Decoder Attention层的Query来自下一层，而Key和Value则来自Encoder的输出。</strong></p>
<p>如果我们仅考虑一个由两个编码器和两个解码器组成的Transformer，它看起来是这样的:</p>
<div align=center>
<img src="http://image.yingzq.com/img/20191104210604.png" width="700" alt="两层Encoder、Decoder堆叠的Transformer" />

<p>图：两层Encoder、Decoder堆叠的Transformer</p>
</div>

<h2 id="省略部分"><a href="#省略部分" class="headerlink" title="省略部分"></a>省略部分</h2><p>到这里，Transformer的主体部分已经全部介绍完，关于机器翻译任务解码的细节、最后输出层的设计、损失函数的选取等并不是本文关注的重点，我们会在 <a href="/2019/11/18/the-annotated-transformer/" title="Transformer代码实现">Transformer代码实现</a> 中完完全全的实现一遍，在这里便不做多余的介绍。</p>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2020-02-19T17:43:21.206Z" itemprop="dateUpdated">2020-02-20 01:43:21</time>
</span><br>


        
        若有疑问或者本人有写的不对的地方, 欢迎留言反馈, 非常感谢您的阅读~
        
    </div>
    
    <footer>
        <a href="http://www.yingzq.com">
            <img src="/img/avatar.jpg" alt="应子强">
            应子强
        </a>
    </footer>
</blockquote>

        


        <div class="post-footer">
            
	<ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BERT/" rel="tag">BERT</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://www.yingzq.com/2019/10/30/the-illustrated-transformer/&title=《Transformer图解》 — yingzq's Blog&pic=http://www.yingzq.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://www.yingzq.com/2019/10/30/the-illustrated-transformer/&title=《Transformer图解》 — yingzq's Blog&source=Transformer模型来自论文 Attention Is All You Need 。这个模型最初是为了提高机器翻译的效率，它的Self-Attent..." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://www.yingzq.com/2019/10/30/the-illustrated-transformer/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Transformer图解》 — yingzq's Blog&url=http://www.yingzq.com/2019/10/30/the-illustrated-transformer/&via=http://www.yingzq.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://www.yingzq.com/2019/10/30/the-illustrated-transformer/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2019/11/18/the-annotated-transformer/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">Transformer代码实现</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2019/10/22/greedy-algorithm/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">贪心算法</h4>
      </a>
    </div>
  
</nav>



    











    <!-- Valine Comments -->
    <div class="comments vcomment" id="comments"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
    <!-- Valine Comments script -->
    <script>
        var GUEST_INFO = ['nick','mail','link'];
        var guest_info = 'nick,mail'.split(',').filter(function(item){
          return GUEST_INFO.indexOf(item) > -1
        });
        new Valine({
            el: '#comments',
            notify: 'false' == 'true',
            verify: 'true' == 'true',
            appId: "mGqkrgTscxf4JdcqkEHkFCfs-gzGzoHsz",
            appKey: "PwU9oOfhNnWOKaMaOTqQpiNW",
            avatar: "mm",
            placeholder: "要不要说点什么？",
            guest_info: guest_info.length == 0 ? GUEST_INFO : guest_info,
            pageSize: "10"
        })
    </script>
    <!-- Valine Comments end -->










</article>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>应子强 &copy; 2019 - 2020</span>
            <span>
                
                <a href="http://www.miitbeian.gov.cn/" target="_blank">京ICP备19041830号</a><br>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://www.yingzq.com/2019/10/30/the-illustrated-transformer/&title=《Transformer图解》 — yingzq's Blog&pic=http://www.yingzq.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://www.yingzq.com/2019/10/30/the-illustrated-transformer/&title=《Transformer图解》 — yingzq's Blog&source=Transformer模型来自论文 Attention Is All You Need 。这个模型最初是为了提高机器翻译的效率，它的Self-Attent..." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://www.yingzq.com/2019/10/30/the-illustrated-transformer/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Transformer图解》 — yingzq's Blog&url=http://www.yingzq.com/2019/10/30/the-illustrated-transformer/&via=http://www.yingzq.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://www.yingzq.com/2019/10/30/the-illustrated-transformer/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACL0lEQVR42u3aQY7DIBBEUd//0s7WUmSoqiaRaD6raOKxeV5UGprrksf9GM+/vH3+vv77buP/vX4xYMCAsS3jHo4xY/xI5Xr9uZMXBAMGjAMYeixmU8yepVwPAwYMGAqjUj66z4UBAwYM93bKWnLVi4ABAwYMZTGpbMPp5WO2ebdgLQ4DBowNGXqM/v/zT/obMGDA2Ipxm0NvErh3K80KBgwYrRl6wI23/pUlsb7EDecDAwaMpgx36ViZbqX9MIHBgAGjNUMv9eoNg2zbzmgtwIAB4wCGG7tuM0AHhA0AGDBgtGYosbiqWKzEqx3xMGDAaMFwA1E/yKWXj27L4XKnCAMGjM0Z2caWO1G3MVCKeBgwYBzAqMdx1uZ0g/X1ehgwYDRluIce1rY29RCfvFYYMGA0ZWRb9tl09aJT3/6DAQPGCYxK2C04OWsCJttwMGDAaMpwF5xZdOrlo3vsFQYMGCcwSkcczEnosR4uoWHAgNGaMQ5To8Y0G5yVkvT1tcKAAaMpQyn4KovYN7we0FITAgYMGAcwVkWqW/Dpr2DyLQwYMI5huMXcGKYfmKi3FmDAgNGVcZsj2xSrl4+TO8CAAaM1Qx9681JfBrvtz6yZCgMGjB6MLGSzbX09mu3AhQEDxgEMN/jcrfwMGZ4ZgQEDxvEMt4hUArfyRBgwYMCoLCz12HVnuOx3AwYMGFsxKoe0FpyfFQJ32XYbDBgwNmRU6q6sjNO34SptURgwYLRgfABPCqcVRdNs3wAAAABJRU5ErkJggg==" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: false };


</script>

<script src="/js/main.min.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.2" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '快回来练习bug!';
            clearTimeout(titleTime);
        } else {
            document.title = 'yingzq's Blog';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
